{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: c:\\Users\\n740789\\Documents\\clarity_data_quality_controls\n",
      "2025-04-25 16:56:43,412 - utils.get_date - INFO - Date format is valid. Date set to 202504.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get the parent directory of current notebook dir, which is the repo root\n",
    "repo_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Verify the path is correct:\n",
    "print(f\"Added to sys.path: {repo_root}\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.utils.dataloaders import (\n",
    "    load_clarity_data,\n",
    "    load_aladdin_data,\n",
    "    load_crossreference,\n",
    "    load_portfolios,\n",
    "    load_overrides,\n",
    "    save_excel\n",
    ")\n",
    "from scripts.utils.zombie_killer import main as zombie_killer\n",
    "\n",
    "# Import the centralized configuration\n",
    "from scripts.utils.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the common configuration for the Pre-OVR-Analysis script.\n",
    "config = get_config(\"pre-ovr-analysis\", interactive=False)\n",
    "logger = config[\"logger\"]\n",
    "DATE = config[\"DATE\"]\n",
    "YEAR = config[\"YEAR\"]\n",
    "DATE_PREV = config[\"DATE_PREV\"]\n",
    "REPO_DIR = config[\"REPO_DIR\"]\n",
    "DATAFEED_DIR = config[\"DATAFEED_DIR\"]\n",
    "SRI_DATA_DIR = config[\"SRI_DATA_DIR\"]\n",
    "paths = config[\"paths\"]\n",
    "\n",
    "# Use the paths from config\n",
    "df_1_path = paths[\"PRE_DF_WOVR_PATH\"]\n",
    "df_2_path = paths[\"CURRENT_DF_WOUTOVR_PATH\"]\n",
    "CROSSREFERENCE_PATH = paths[\"CROSSREFERENCE_PATH\"]\n",
    "BMK_PORTF_STR_PATH = paths[\"BMK_PORTF_STR_PATH\"]\n",
    "OVR_PATH = paths[\"OVR_PATH\"]\n",
    "COMMITTEE_PATH = paths[\"COMMITTEE_PATH\"]\n",
    "\n",
    "# Define the output directory and file based on the configuration.\n",
    "OUTPUT_DIR = config[\"OUTPUT_DIR\"]\n",
    "OUTPUT_FILE = OUTPUT_DIR / f\"{DATE}_pre_ovr_analysis.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore workbook warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the date constants are set correctly\n",
    "print(f\"{DATE} and {YEAR} and {DATE_PREV}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE CONSTANTS & TEST COLUMNS & DICTS\n",
    "# let's define necessary column lists\n",
    "\n",
    "id_name_cols = [\"permid\", \"isin\", \"issuer_name\"]\n",
    "id_name_issuers_cols = [\"aladdin_id\", \"permid\", \"issuer_name\"]\n",
    "clarity_test_col = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"art_8_basicos\",\n",
    "    \"str_006_sec\",\n",
    "    \"cs_001_sec\",\n",
    "    \"cs_002_ec\",\n",
    "]\n",
    "columns_to_read = id_name_cols + clarity_test_col\n",
    "delta_test_cols = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"str_006_sec\",\n",
    "    \"str_sfdr8_aec\",\n",
    "    \"scs_001_sec\",\n",
    "    \"scs_002_ec\",\n",
    "]\n",
    "\n",
    "brs_test_cols = [\"aladdin_id\"] + delta_test_cols\n",
    "rename_dict = {\n",
    "    \"cs_001_sec\": \"scs_001_sec\",\n",
    "    \"cs_002_ec\": \"scs_002_ec\",\n",
    "    \"art_8_basicos\": \"str_sfdr8_aec\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframes(\n",
    "    base_df: pd.DataFrame, new_df: pd.DataFrame, target_index:str = \"permid\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare DataFrames by setting the index and filtering for common indexes.\n",
    "    Logs info about common, new, and missing indexes.\n",
    "    \"\"\"\n",
    "    # Set index to 'permid' if it exists, otherwise assume it's already the index.\n",
    "    logger.info(f\"Setting index to {target_index}.\")\n",
    "    if target_index in base_df.columns:\n",
    "        base_df = base_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df1 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    if target_index in new_df.columns:\n",
    "        new_df = new_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df2 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    common_indexes = base_df.index.intersection(new_df.index)\n",
    "    new_indexes = new_df.index.difference(base_df.index)\n",
    "    missing_indexes = base_df.index.difference(new_df.index)\n",
    "\n",
    "    logger.info(f\"Number of common indexes: {len(common_indexes)}\")\n",
    "\n",
    "    return (\n",
    "        base_df.loc[common_indexes],\n",
    "        new_df.loc[common_indexes],\n",
    "        new_df.loc[new_indexes],\n",
    "        base_df.loc[missing_indexes],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(\n",
    "    df1: pd.DataFrame, df2: pd.DataFrame, test_col: List[str] = delta_test_cols\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare DataFrames and create a delta DataFrame.\"\"\"\n",
    "    delta = df2.copy()\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Comparing column: {col}\")\n",
    "            # Create a mask for differences between the two DataFrames\n",
    "            diff_mask = df1[col] != df2[col]\n",
    "            # Update the delta DataFrame with the differences\n",
    "            delta.loc[~diff_mask, col] = np.nan\n",
    "    return delta\n",
    "\n",
    "def get_exclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed to EXCLUDED.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] == \"EXCLUDED\" and df1.loc[row.name, col] != \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "def get_inclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed from EXCLUDED to any other value.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] != \"EXCLUDED\" and df1.loc[row.name, col] == \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "def check_new_exclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new exclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_exclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new exclusions in column: {col}\")\n",
    "            mask = (df1[col] != \"EXCLUDED\") & (df2[col] == \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_exclusion\"] = True\n",
    "            logger.info(f\"Number of new exclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"exclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_exclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "def check_new_inclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new inclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_inclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new inclusions in column: {col}\")\n",
    "            mask = (df1[col] == \"EXCLUDED\") & (df2[col] != \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_inclusion\"] = True\n",
    "            logger.info(f\"Number of new inclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"inclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_inclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "def finalize_delta(\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    target_index: str = \"permid\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Finalize the delta DataFrame by removing unchanged rows and resetting the index.\"\"\"\n",
    "    delta = delta.dropna(subset=test_col, how=\"all\")\n",
    "    delta.reset_index(inplace=True)\n",
    "    delta[target_index] = delta[target_index].astype(str)\n",
    "    logger.info(f\"Final delta shape: {delta.shape}\")\n",
    "    return delta\n",
    "\n",
    "def create_override_dict(\n",
    "    df: pd.DataFrame = None,\n",
    "    id_col: str = \"aladdin_id\",\n",
    "    str_col: str = \"ovr_target\",\n",
    "    ovr_col: str = \"ovr_value\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts the overrides DataFrame to a dictionary.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the overrides.\n",
    "        id_col (str): Column name for the identifier.\n",
    "        str_col (str): Column name for the strategy.\n",
    "        ovr_col (str): Column name for the override value.\n",
    "    Returns:\n",
    "        dict: Dictionary of overrides.\n",
    "    \"\"\"\n",
    "    # 1. Groupd the df by issuer_id\n",
    "    grouped = df.groupby(id_col)\n",
    "\n",
    "    # 2. Initialise the dictionary\n",
    "    ovr_dict = {}\n",
    "\n",
    "    # 3. Iterate over each group (issuer id and its corresponding rows)\n",
    "    for id, group_data in grouped:\n",
    "        # 3.1. for each issuer id create a dict pairing the strategy and the override value\n",
    "        ovr_result = dict(zip(group_data[str_col], group_data[ovr_col]))\n",
    "        # 3.2. add the dict to the main dict\n",
    "        ovr_dict[id] = ovr_result\n",
    "\n",
    "    return ovr_dict\n",
    "\n",
    "def add_portfolio_benchmark_info_to_df(\n",
    "    portfolio_dict, delta_df, column_name=\"affected_portfolio_str\"\n",
    "):\n",
    "\n",
    "    # Initialize a defaultdict to accumulate (portfolio_id, strategy_name) pairs\n",
    "    aladdin_to_info = defaultdict(list)\n",
    "\n",
    "    for portfolio_id, data in portfolio_dict.items():\n",
    "        strategy = data.get(\"strategy_name\")\n",
    "        for a_id in data.get(\"aladdin_id\", []):\n",
    "            aladdin_to_info[a_id].append((portfolio_id, strategy))\n",
    "\n",
    "    # Map each aladdin_id in delta_df to a list of accumulated portfolio info\n",
    "    delta_df[column_name] = delta_df[\"aladdin_id\"].apply(\n",
    "        lambda x: list(chain.from_iterable(aladdin_to_info.get(x, [])))\n",
    "    )\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "def get_issuer_level_df(df: pd.DataFrame, idx_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicates based on idx_name, and drops rows where idx_name column contains\n",
    "    NaN, None, or strings like \"nan\", \"NaN\", \"none\", or empty strings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        idx_name (str): Column name used for duplicate removal and NaN filtering.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Drop duplicates\n",
    "    df_cleaned = df.drop_duplicates(subset=[idx_name])\n",
    "\n",
    "    # Drop rows where idx_name is NaN/None or has invalid strings\n",
    "    valid_rows = df_cleaned[idx_name].notnull() & (\n",
    "        ~df_cleaned[idx_name]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .isin([\"nan\", \"none\", \"\"])\n",
    "    )\n",
    "\n",
    "    return df_cleaned[valid_rows]\n",
    "\n",
    "def filter_non_empty_lists(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame filtered so that rows where the specified column contains\n",
    "    an empty list are removed. Keeps rows where the column has a list with at least one element.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame\n",
    "    - column (str): The name of the column to check\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    return df[df[column].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "def filter_rows_with_common_elements(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Return rows of df where the lists in col1 and col2 have at least one common element.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        col1 (str): The name of the first column containing lists.\n",
    "        col2 (str): The name of the second column containing lists.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame filtered to include only rows where col1 and col2 have a common element.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering rows with common elements in columns: {col1} and {col2}\")\n",
    "    mask = df.apply(lambda row: bool(set(row[col1]).intersection(row[col2])), axis=1)\n",
    "    return df[mask].copy()\n",
    "\n",
    "def reorder_columns(df:pd.DataFrame, keep_first:list[str], exclude:list[str]=None):\n",
    "    if exclude is None:\n",
    "        exclude = set()\n",
    "    return df[\n",
    "        keep_first\n",
    "        + [col for col in df.columns if col not in keep_first and col not in exclude]\n",
    "    ]\n",
    "\n",
    "def remove_matching_rows(df):\n",
    "    # Identify columns with specific suffixes\n",
    "    cols_old = [col for col in df.columns if col.endswith('_old')]\n",
    "    cols_brs = [col for col in df.columns if col.endswith('_brs')]\n",
    "    cols_ovr = [col for col in df.columns if col.endswith('_ovr')]\n",
    "\n",
    "    # Assuming there's only one set of each column type based on your example\n",
    "    if len(cols_old) != 1 or len(cols_brs) != 1 or len(cols_ovr) != 1:\n",
    "        raise ValueError(\"Expected exactly one column each for '_old', '_brs', '_ovr'\")\n",
    "\n",
    "    col_old, col_brs, col_ovr = cols_old[0], cols_brs[0], cols_ovr[0]\n",
    "\n",
    "    # Filter rows where all three column values match\n",
    "    df_filtered = df[\n",
    "        ~(df[col_old] == df[col_brs]) | ~(df[col_old] == df[col_ovr])\n",
    "    ].copy()\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def clean_inclusion_list(df):\n",
    "    \"\"\"\n",
    "    Processes each row of df:\n",
    "    1. For each element in 'inclusion_list_brs', if the element is a key in 'ovr_list' and\n",
    "       its value is 'EXCLUDED', remove the element.\n",
    "    2. Rows where 'inclusion_list_brs' is empty (or becomes empty after filtering) are dropped.\n",
    "    \"\"\"\n",
    "    def process_row(row):\n",
    "        inc_list = row.get('inclusion_list_brs', [])\n",
    "        ovr_list = row.get('ovr_list', {})\n",
    "\n",
    "        # If inc_list is a NumPy array, convert it to a list.\n",
    "        if isinstance(inc_list, np.ndarray):\n",
    "            inc_list = inc_list.tolist()\n",
    "\n",
    "        # If inc_list is not a list (or array), then treat it as empty.\n",
    "        if not isinstance(inc_list, list):\n",
    "            return []\n",
    "\n",
    "        # For ovr_list: if it's not a dict, then treat it as an empty dict.\n",
    "        if not isinstance(ovr_list, dict):\n",
    "            ovr_list = {}\n",
    "\n",
    "        # Filter out items that are in ovr_list and marked as 'EXCLUDED'\n",
    "        return [item for item in inc_list if item not in ovr_list or ovr_list[item] != 'EXCLUDED']\n",
    "\n",
    "    # Apply the row-wise processing.\n",
    "    df.loc[:,'inclusion_list_brs'] = df.apply(process_row, axis=1)\n",
    "\n",
    "    # Drop rows where 'inclusion_list_brs' is empty.\n",
    "    df = df[df['inclusion_list_brs'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def pair_elements(input_list):\n",
    "    \"\"\"\n",
    "    Pairs consecutive elements in a list into tuples.\n",
    "    Parameters: -> input_list : list\n",
    "    -----------\n",
    "    Returns: -> list_of_tuples : list. A list where consecutive elements are paired as tuples.\n",
    "    --------\n",
    "    Raises TypeError: If the input is not a list and ValueError: If the list does not have an even number of elements.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_list, list):\n",
    "        raise TypeError(\"Expected a list as input.\")\n",
    "    if len(input_list) % 2 != 0:\n",
    "        raise ValueError(\"The list must have an even number of elements.\")\n",
    "\n",
    "    return [(input_list[i], input_list[i + 1]) for i in range(0, len(input_list), 2)]\n",
    "\n",
    "def clean_portfolio_and_exclusion_list(row):\n",
    "    \"\"\"\n",
    "    First pairs elements in 'affected_portfolio_str' and filters them based\n",
    "    on 'exclusion_list_brs'. Then cleans 'exclusion_list_brs' based on the\n",
    "    filtered results.\n",
    "    \"\"\"\n",
    "    raw_list = row[\"affected_portfolio_str\"]\n",
    "    exclusion_list = row[\"exclusion_list_brs\"]\n",
    "\n",
    "    # Pair elements first\n",
    "    paired = pair_elements(raw_list)\n",
    "\n",
    "    # Filter tuples based on exclusion list\n",
    "    cleaned_paired = [tup for tup in paired if tup[1] in exclusion_list]\n",
    "\n",
    "    # Update affected_portfolio_str\n",
    "    row[\"affected_portfolio_str\"] = cleaned_paired\n",
    "\n",
    "    # Extract strategies from the cleaned paired tuples\n",
    "    affected_strategies = {strategy for _, strategy in cleaned_paired}\n",
    "\n",
    "    # Update exclusion_list_brs\n",
    "    row[\"exclusion_list_brs\"] = [\n",
    "        strategy for strategy in exclusion_list if strategy in affected_strategies\n",
    "    ]\n",
    "\n",
    "    return row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.    LOAD DATA\n",
    "# 1.1.  clarity data\n",
    "df_1 = load_clarity_data(df_1_path, columns_to_read)\n",
    "df_2 = load_clarity_data(df_2_path, columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns in df_1 and df_2 using the rename_dict\n",
    "df_1.rename(columns=rename_dict, inplace=True)\n",
    "df_2.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"previous clarity df's  rows: {df_1.shape[0]}, new clarity df's rows: {df_2.shape[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.  aladdin /brs data / perimetros\n",
    "brs_carteras = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_carteras\")    \n",
    "brs_benchmarks = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_benchmarks\")\n",
    "crosreference = load_crossreference(CROSSREFERENCE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add aladdin_id to df_1 and df_2\n",
    "logger.info(\"Adding aladdin_id to clarity dfs\")\n",
    "df_1 = df_1.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")\n",
    "df_2 = df_2.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_carteras_issuerlevel = get_issuer_level_df(brs_carteras, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_benchmarks_issuerlevel = get_issuer_level_df(brs_benchmarks, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 sri/ESG Team data\n",
    "overrides = load_overrides(OVR_PATH)\n",
    "# rename column brs_id to aladdin_id\n",
    "overrides.rename(columns={\"brs_id\": \"aladdin_id\"}, inplace=True)\n",
    "# rename value column \"ovr_target\" using rename_dict if value is string\n",
    "overrides[\"ovr_target\"] = overrides[\"ovr_target\"].apply(\n",
    "    lambda x: pd.NA if isinstance(x, str) and x.strip().lower() in [\"na\", \"nan\"]\n",
    "    else rename_dict[x] if isinstance(x, str) and x in rename_dict\n",
    "    else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_dict = create_override_dict(overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load portfolios & benchmarks dicts\n",
    "(\n",
    "    portfolio_dict,\n",
    "    benchmark_dict,\n",
    ") = load_portfolios(path_pb=BMK_PORTF_STR_PATH, path_committe=COMMITTEE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START PRE-OVR ANALISIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PREP DATA FOR ANALYSIS\n",
    "# make sure that the values of of the columns delta_test_cols are strings and all uppercase and strip\n",
    "for col in delta_test_cols:\n",
    "    df_1[col] = df_1[col].str.upper().str.strip()\n",
    "    df_2[col] = df_2[col].str.upper().str.strip()\n",
    "    brs_carteras_issuerlevel[col] = brs_carteras_issuerlevel[col].str.upper().str.strip()\n",
    "    brs_benchmarks_issuerlevel[col] = brs_benchmarks_issuerlevel[col].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA CLARITY LEVEL\n",
    "(\n",
    "    df_1, \n",
    "    df_2,\n",
    "    new_issuers_clarity,\n",
    "    out_issuer_clarity,\n",
    ") = prepare_dataframes(df_1, df_2)\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number of new issuers: {new_issuers_clarity.shape[0]}\")\n",
    "logger.info(f\"Number of missing issuers: {out_issuer_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index for new_issuers_clarity and out_issuer_clarity\n",
    "new_issuers_clarity.reset_index(inplace=True)\n",
    "out_issuer_clarity.reset_index(inplace=True)\n",
    "\n",
    "# drop isin from out_issuer_clarity and new_issuers_clarity\n",
    "out_issuer_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "new_issuers_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "\n",
    "# remember to remove empyt empyt aladin id\n",
    "new_issuers_clarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA CARTERAS BRS LEVEL\n",
    "(\n",
    "    brs_df, \n",
    "    clarity_df,\n",
    "    in_clarity_but_not_in_brs,\n",
    "    in_brs_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_carteras_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not Aladdin: {in_clarity_but_not_in_brs.shape[0]}\")\n",
    "logger.info(f\"Number issuers in Aladdin but not Clarity: {in_brs_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA BENCHMARK BRS LEVEL\n",
    "(\n",
    "    brs_df_benchmarks, \n",
    "    clarity_df_benchmarks,\n",
    "    in_clarity_but_not_in_brs_benchmarks,\n",
    "    in_brs_benchmark_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_benchmarks_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not benchmarks: {in_clarity_but_not_in_brs_benchmarks.shape[0]}\")\n",
    "logger.info(f\"Number issuers in benchmarks but not Clarity: {in_brs_benchmark_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARE DATA\n",
    "logger.info(\"comparing clarity dataframes\")\n",
    "delta_clarity = compare_dataframes(df_1, df_2)\n",
    "delta_clarity = check_new_exclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = check_new_inclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = finalize_delta(delta_clarity)\n",
    "logger.info(\"checking impact compared to BRS portfolio data\")\n",
    "delta_brs = compare_dataframes(brs_df, clarity_df)\n",
    "delta_brs = check_new_exclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = check_new_inclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = finalize_delta(delta_brs, target_index=\"aladdin_id\")\n",
    "logger.info(\"checking impact compared to BRS benchmarks data\")\n",
    "delta_benchmarks = compare_dataframes(brs_df_benchmarks, clarity_df_benchmarks)\n",
    "delta_benchmarks = check_new_exclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = check_new_inclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = finalize_delta(delta_benchmarks, target_index=\"aladdin_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.zombie_killer import main as zombie_killer\n",
    "#logger.info(\"Getting zombie analysis df\")\n",
    "zombie_df = zombie_killer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREP DELTAS BEFORE SAVING\n",
    "logger.info(\"Preparing deltas before saving\")\n",
    "# use crossreference to add permid to delta_brs\n",
    "delta_brs = delta_brs.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "delta_benchmarks = delta_benchmarks.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "# drop isin from deltas\n",
    "delta_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_brs.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_benchmarks.drop(columns=[\"isin\"], inplace=True)\n",
    "# add new column to delta_brs with ovr_dict value using aladdin_id\n",
    "delta_brs[\"ovr_list\"] = delta_brs[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_clarity[\"ovr_list\"] = delta_clarity[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_benchmarks[\"ovr_list\"] = delta_benchmarks[\"aladdin_id\"].map(ovr_dict)\n",
    "# let's add portfolio info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_clarity)\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_brs)\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_benchmarks)\n",
    "# let's add benchmark info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_clarity, \"affected_benchmark_str\")\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_brs, \"affected_benchmark_str\")\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_benchmarks, \"affected_benchmark_str\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_brs = filter_non_empty_lists(delta_brs, \"affected_portfolio_str\")\n",
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_benchmarks = filter_non_empty_lists(delta_benchmarks, \"affected_portfolio_str\")\n",
    "\n",
    "# ADD TEST FOR INCLUSIONS\n",
    "dlt_inc_brs = delta_brs.copy()\n",
    "dlt_inc_benchmarks = delta_benchmarks.copy()\n",
    "\n",
    "# pass filter_rows_with_common_elements for columns exclusion_list_brs and affected_portfolio_str\n",
    "delta_brs = filter_rows_with_common_elements(delta_brs, \"exclusion_list_brs\", \"affected_portfolio_str\")\n",
    "delta_benchmarks = filter_rows_with_common_elements(delta_benchmarks, \"exclusion_list_brs\", \"affected_portfolio_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows from delta_brs where len of the list of in the column affected_benchmark_str is bigger than zero\n",
    "#delta_brs[delta_brs[\"affected_benchmark_str\"].apply(lambda x: len(x) > 0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reset df1 index to permid\n",
    "df_1.reset_index(inplace=True)\n",
    "df_1[\"permid\"] = df_1[\"permid\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. GET STRATEGIES DFS\n",
    "str_dfs_dict = {}\n",
    "\n",
    "# Iterate over strategies to build DataFrames\n",
    "for strategy in delta_test_cols:\n",
    "    rows = []\n",
    "\n",
    "    for _, row in delta_brs.iterrows():\n",
    "        if strategy in row[\"exclusion_list_brs\"]:\n",
    "            rows.append({\n",
    "                \"aladdin_id\": row[\"aladdin_id\"],\n",
    "                \"permid\": row[\"permid\"],\n",
    "                \"issuer_name\": row[\"issuer_name\"],\n",
    "                strategy: row[strategy],\n",
    "                \"affected_portfolio_str\": row[\"affected_portfolio_str\"]\n",
    "            })\n",
    "\n",
    "    str_dfs_dict[strategy] = pd.DataFrame(rows)\n",
    "\n",
    "# Prepare lookups for efficient mapping\n",
    "permid_to_df1 = df_1.set_index(\"permid\")\n",
    "aladdin_to_brs = brs_carteras_issuerlevel.set_index(\"aladdin_id\")\n",
    "\n",
    "for strategy_name, df in str_dfs_dict.items():\n",
    "    # Initialize additional columns\n",
    "    df[f\"{strategy_name}_old\"] = None\n",
    "    df[f\"{strategy_name}_brs\"] = None\n",
    "    df[f\"{strategy_name}_ovr\"] = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        permid = row[\"permid\"]\n",
    "        aladdin_id = row[\"aladdin_id\"]\n",
    "\n",
    "        # Lookup from df_1\n",
    "        if permid in permid_to_df1.index:\n",
    "            df.at[i, f\"{strategy_name}_old\"] = permid_to_df1.at[permid, strategy_name]\n",
    "\n",
    "        # Lookup from brs_carteras_issuerlevel\n",
    "        if aladdin_id in aladdin_to_brs.index:\n",
    "            df.at[i, f\"{strategy_name}_brs\"] = aladdin_to_brs.at[aladdin_id, strategy_name]\n",
    "\n",
    "        # Lookup from overrides\n",
    "        match = overrides.loc[\n",
    "            (overrides[\"permid\"] == permid) & (overrides[\"ovr_target\"] == strategy_name),\n",
    "            \"ovr_value\"\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            df.at[i, f\"{strategy_name}_ovr\"] = match.values[0]\n",
    "\n",
    "    # Move \"affected_portfolio_str\" to the end\n",
    "    cols = [col for col in df.columns if col != \"affected_portfolio_str\"] + [\"affected_portfolio_str\"]\n",
    "    df = df[cols]\n",
    "    str_dfs_dict[strategy_name] = df\n",
    "\n",
    "# Reset index and ensure permid is a string for df_1\n",
    "df_1.reset_index(drop=True, inplace=True)\n",
    "df_1[\"permid\"] = df_1[\"permid\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set id_name_issuers_cols first and exclude delta_test_cols\n",
    "delta_brs = reorder_columns(delta_brs, id_name_issuers_cols, delta_test_cols)\n",
    "delta_clarity = reorder_columns(delta_clarity, id_name_issuers_cols, delta_test_cols)\n",
    "delta_benchmarks = reorder_columns(delta_benchmarks, id_name_issuers_cols, delta_test_cols)\n",
    "# set id_name_issuers_cols first\n",
    "new_issuers_clarity = reorder_columns(new_issuers_clarity, id_name_issuers_cols, id_name_cols)\n",
    "out_issuer_clarity = reorder_columns(out_issuer_clarity, id_name_issuers_cols, id_name_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TEST FOR INCLUSIONS\n",
    "dlt_inc_brs = reorder_columns(dlt_inc_brs, id_name_issuers_cols, delta_test_cols)\n",
    "dlt_inc_benchmarks = reorder_columns(dlt_inc_benchmarks, id_name_issuers_cols, delta_test_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt_inc_brs = clean_inclusion_list(dlt_inc_brs)\n",
    "dlt_inc_benchmarks = clean_inclusion_list(dlt_inc_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in str_dfs_dict.items():\n",
    "    str_dfs_dict[df_name] = remove_matching_rows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# define a function to save results in an Excel file\n",
    "def save_excel(df_dict: dict, output_dir: Path, file_name: str) -> Path:\n",
    "    \"\"\"\n",
    "    Writes multiple DataFrames to an Excel file with each DataFrame in a separate sheet.\n",
    "\n",
    "    Parameters:\n",
    "    - df_dict (dict): A dictionary where keys are sheet names and values are DataFrames.\n",
    "    - output_dir (Path): The directory where the Excel file will be saved.\n",
    "    - file_name (str): The base name for the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    - Path: The full path to the saved Excel file.\n",
    "    \"\"\"\n",
    "    # Create a date string in \"YYYYMMDD\" format\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    logger.info(\"Creating output directory: %s\", output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Construct the full output file path (e.g., file_name_YYYYMMDD.xlsx)\n",
    "    output_file = output_dir / f\"{date_str}_{file_name}.xlsx\"\n",
    "\n",
    "    # Write each DataFrame to its own sheet with index set to False\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        logger.info(\"Writing DataFrames to Excel file: %s\", output_file)\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            logger.info(\"Writing sheet: %s\", sheet_name)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    logger.info(\"Results saved to Excel file: %s\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to remove from delta_brs, delta_benchmarks before saving\n",
    "exclusion_cols_to_remove = [\"new_inclusion\", \"inclusion_list_brs\"]\n",
    "# columns to remove from dlt_inc_brs and dlt_inc_benchmarks before saving\n",
    "inclusion_cols_to_remove = [\"new_exclusion\", \"exclusion_list_brs\"]\n",
    "\n",
    "# Filter and clean exclusion data\n",
    "delta_brs = delta_brs.drop(columns=exclusion_cols_to_remove)\n",
    "delta_brs = delta_brs[delta_brs[\"new_exclusion\"] == True]\n",
    "\n",
    "delta_benchmarks = delta_benchmarks.drop(columns=exclusion_cols_to_remove)\n",
    "delta_benchmarks = delta_benchmarks[delta_benchmarks[\"new_exclusion\"] == True]\n",
    "\n",
    "# Filter and clean inclusion data\n",
    "dlt_inc_brs = dlt_inc_brs.drop(columns=inclusion_cols_to_remove)\n",
    "dlt_inc_brs = dlt_inc_brs[dlt_inc_brs[\"new_inclusion\"] == True]\n",
    "\n",
    "dlt_inc_benchmarks = dlt_inc_benchmarks.drop(columns=inclusion_cols_to_remove)\n",
    "dlt_inc_benchmarks = dlt_inc_benchmarks[dlt_inc_benchmarks[\"new_inclusion\"] == True]\n",
    "\n",
    "# Clean delta_clarity\n",
    "delta_clarity.drop(columns=[\"new_inclusion\", \"inclusion_list\"], inplace=True)\n",
    "delta_clarity = delta_clarity[delta_clarity[\"new_exclusion\"] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup: drop 'new_exclusion' and 'new_inclusion' if present\n",
    "for df in [delta_brs, delta_benchmarks, dlt_inc_brs, dlt_inc_benchmarks, delta_clarity]:\n",
    "    for col in [\"new_exclusion\", \"new_inclusion\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=col, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of df and df name\n",
    "dfs_dict = {\n",
    "    \"zombie_analysis\": zombie_df,\n",
    "    \"delta_carteras\": delta_brs,\n",
    "    \"delta_benchmarks\": delta_benchmarks,\n",
    "    \"delta_clarity\": delta_clarity,\n",
    "    \"incl_carteras\": dlt_inc_brs,\n",
    "    \"incl_benchmarks\": dlt_inc_benchmarks,\n",
    "    \"new_issuers_clarity\": new_issuers_clarity,\n",
    "    \"out_issuer_clarity\": out_issuer_clarity,\n",
    "}\n",
    "\n",
    "# add to dfs_dict the str_dfs_dict\n",
    "dfs_dict.update(str_dfs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel\n",
    "#save_excel(str_dfs_dict, OUTPUT_DIR, file_name=\"pre_ovr_simple_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel\n",
    "save_excel(dfs_dict, OUTPUT_DIR, file_name=\"pre_ovr_analysis_beta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_benchmarks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_benchmarks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dfs_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:07:24,259 - utils.get_date - INFO - Date format is valid. Date set to 202504.\n",
      "Output directory for script zombie-killer is set to: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\zombie_list\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.dataloaders import (\n",
    "    load_clarity_data,\n",
    "    load_aladdin_data,\n",
    "    load_crossreference,\n",
    "    load_portfolios,\n",
    "    load_overrides,\n",
    "    save_excel\n",
    ")\n",
    "from utils.zombie_killer import main as zombie_killer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:07:28,237 - utils.get_date - INFO - Date format is valid. Date set to 202504.\n",
      "Output directory for script pre-ovr-analysis is set to: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\n"
     ]
    }
   ],
   "source": [
    "# Import the centralized configuration\n",
    "from config import get_config\n",
    "\n",
    "# Get the common configuration for the Pre-OVR-Analysis script.\n",
    "config = get_config(\"pre-ovr-analysis\", interactive=False)\n",
    "logger = config[\"logger\"]\n",
    "DATE = config[\"DATE\"]\n",
    "YEAR = config[\"YEAR\"]\n",
    "DATE_PREV = config[\"DATE_PREV\"]\n",
    "REPO_DIR = config[\"REPO_DIR\"]\n",
    "DATAFEED_DIR = config[\"DATAFEED_DIR\"]\n",
    "SRI_DATA_DIR = config[\"SRI_DATA_DIR\"]\n",
    "paths = config[\"paths\"]\n",
    "\n",
    "# Use the paths from config\n",
    "df_1_path = paths[\"PRE_DF_WOVR_PATH\"]\n",
    "df_2_path = paths[\"CURRENT_DF_WOUTOVR_PATH\"]\n",
    "CROSSREFERENCE_PATH = paths[\"CROSSREFERENCE_PATH\"]\n",
    "BMK_PORTF_STR_PATH = paths[\"BMK_PORTF_STR_PATH\"]\n",
    "OVR_PATH = paths[\"OVR_PATH\"]\n",
    "COMMITTEE_PATH = paths[\"COMMITTEE_PATH\"]\n",
    "\n",
    "# Define the output directory and file based on the configuration.\n",
    "OUTPUT_DIR = config[\"OUTPUT_DIR\"]\n",
    "OUTPUT_FILE = OUTPUT_DIR / f\"{DATE}_pre_ovr_analysis.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore workbook warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202504 and 2025 and 202503.\n"
     ]
    }
   ],
   "source": [
    "# check that the date constants are set correctly\n",
    "print(f\"{DATE} and {YEAR} and {DATE_PREV}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TEST COLUMNS\n",
    "# let's define necessary column lists\n",
    "\n",
    "clarity_test_col = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"art_8_basicos\",\n",
    "    \"str_006_sec\",\n",
    "    \"cs_001_sec\",\n",
    "    \"cs_002_ec\",\n",
    "]\n",
    "columns_to_read = [\"permid\", \"isin\", \"issuer_name\"] + clarity_test_col\n",
    "brs_test_cols = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"str_006_sec\",\n",
    "    \"str_sfdr8_aec\",\n",
    "    \"scs_001_sec\",\n",
    "    \"scs_002_ec\",\n",
    "    \"aladdin_id\",\n",
    "]\n",
    "rename_dict = {\n",
    "    \"cs_001_sec\": \"scs_001_sec\",\n",
    "    \"cs_002_ec\": \"scs_002_ec\",\n",
    "    \"art_8_basicos\": \"str_sfdr8_aec\",\n",
    "}\n",
    "\n",
    "delta_test_cols = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"str_006_sec\",\n",
    "    \"str_sfdr8_aec\",\n",
    "    \"scs_001_sec\",\n",
    "    \"scs_002_ec\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframes(\n",
    "    base_df: pd.DataFrame, new_df: pd.DataFrame, target_index:str = \"permid\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare DataFrames by setting the index and filtering for common indexes.\n",
    "    Logs info about common, new, and missing indexes.\n",
    "    \"\"\"\n",
    "    # Set index to 'permid' if it exists, otherwise assume it's already the index.\n",
    "    logger.info(f\"Setting index to {target_index}.\")\n",
    "    if target_index in base_df.columns:\n",
    "        base_df = base_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df1 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    if target_index in new_df.columns:\n",
    "        new_df = new_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df2 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    common_indexes = base_df.index.intersection(new_df.index)\n",
    "    new_indexes = new_df.index.difference(base_df.index)\n",
    "    missing_indexes = base_df.index.difference(new_df.index)\n",
    "\n",
    "    logger.info(f\"Number of common indexes: {len(common_indexes)}\")\n",
    "\n",
    "    return (\n",
    "        base_df.loc[common_indexes],\n",
    "        new_df.loc[common_indexes],\n",
    "        new_df.loc[new_indexes],\n",
    "        base_df.loc[missing_indexes],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(\n",
    "    df1: pd.DataFrame, df2: pd.DataFrame, test_col: List[str] = delta_test_cols\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare DataFrames and create a delta DataFrame.\"\"\"\n",
    "    delta = df2.copy()\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Comparing column: {col}\")\n",
    "            # Create a mask for differences between the two DataFrames\n",
    "            diff_mask = df1[col] != df2[col]\n",
    "            # Update the delta DataFrame with the differences\n",
    "            delta.loc[~diff_mask, col] = np.nan\n",
    "    return delta\n",
    "\n",
    "\n",
    "def get_exclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed to EXCLUDED.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] == \"EXCLUDED\" and df1.loc[row.name, col] != \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_inclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed from EXCLUDED to any other value.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] != \"EXCLUDED\" and df1.loc[row.name, col] == \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def check_new_exclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new exclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_exclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new exclusions in column: {col}\")\n",
    "            mask = (df1[col] != \"EXCLUDED\") & (df2[col] == \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_exclusion\"] = True\n",
    "            logger.info(f\"Number of new exclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"exclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_exclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "\n",
    "def check_new_inclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new inclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_inclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new inclusions in column: {col}\")\n",
    "            mask = (df1[col] == \"EXCLUDED\") & (df2[col] != \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_inclusion\"] = True\n",
    "            logger.info(f\"Number of new inclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"inclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_inclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "\n",
    "def finalize_delta(\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    target_index: str = \"permid\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Finalize the delta DataFrame by removing unchanged rows and resetting the index.\"\"\"\n",
    "    delta = delta.dropna(subset=test_col, how=\"all\")\n",
    "    delta.reset_index(inplace=True)\n",
    "    delta[target_index] = delta[target_index].astype(str)\n",
    "    logger.info(f\"Final delta shape: {delta.shape}\")\n",
    "    return delta\n",
    "\n",
    "def override_dict(\n",
    "        df:pd.DataFrame=None,\n",
    "        id_col:str=\"aladdin_id\",\n",
    "        str_col:str=\"ovr_target\",\n",
    "        ovr_col:str=\"ovr_value\",\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Converts the overrides DataFrame to a dictionary.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the overrides.\n",
    "        id_col (str): Column name for the identifier.\n",
    "        str_col (str): Column name for the strategy.\n",
    "        ovr_col (str): Column name for the override value.\n",
    "    Returns:\n",
    "        dict: Dictionary of overrides.\n",
    "    \"\"\"\n",
    "    # 1. Groupd the df by issuer_id\n",
    "    grouped = df.groupby(id_col)\n",
    "\n",
    "    # 2. Initialise the dictionary\n",
    "    ovr_dict = {}\n",
    "\n",
    "    # 3. Iterate over each group (issuer id and its corresponding rows)\n",
    "    for id, group_data in grouped:\n",
    "        # 3.1. for each issuer id create a dict pairing the strategy and the override value\n",
    "        ovr_result = dict(zip(group_data[str_col], group_data[ovr_col]))\n",
    "        # 3.2. add the dict to the main dict\n",
    "        ovr_dict[id] = ovr_result\n",
    "    \n",
    "    return ovr_dict\n",
    "\n",
    "# define a function to add portfolio OR benchmark info to the delta_df\n",
    "def add_portfolio_benchmark_info_to_df(\n",
    "    portfolio_dict, delta_df, column_name=\"affected_portfolio_str\"\n",
    "):\n",
    "\n",
    "    # Initialize a defaultdict to accumulate (portfolio_id, strategy_name) pairs\n",
    "    aladdin_to_info = defaultdict(list)\n",
    "\n",
    "    for portfolio_id, data in portfolio_dict.items():\n",
    "        strategy = data.get(\"strategy_name\")\n",
    "        for a_id in data.get(\"aladdin_id\", []):\n",
    "            aladdin_to_info[a_id].append((portfolio_id, strategy))\n",
    "\n",
    "    # Map each aladdin_id in delta_df to a list of accumulated portfolio info\n",
    "    delta_df[column_name] = delta_df[\"aladdin_id\"].apply(\n",
    "        lambda x: list(chain.from_iterable(aladdin_to_info.get(x, [])))\n",
    "    )\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "\n",
    "def get_issuer_level_df(df: pd.DataFrame, idx_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicates based on idx_name, and drops rows where idx_name column contains\n",
    "    NaN, None, or strings like \"nan\", \"NaN\", \"none\", or empty strings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        idx_name (str): Column name used for duplicate removal and NaN filtering.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Drop duplicates\n",
    "    df_cleaned = df.drop_duplicates(subset=[idx_name])\n",
    "\n",
    "    # Drop rows where idx_name is NaN/None or has invalid strings\n",
    "    valid_rows = df_cleaned[idx_name].notnull() & (\n",
    "        ~df_cleaned[idx_name]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .isin([\"nan\", \"none\", \"\"])\n",
    "    )\n",
    "\n",
    "    return df_cleaned[valid_rows]\n",
    "\n",
    "\n",
    "def filter_non_empty_lists(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame filtered so that rows where the specified column contains\n",
    "    an empty list are removed. Keeps rows where the column has a list with at least one element.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame\n",
    "    - column (str): The name of the column to check\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    return df[df[column].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "def filter_rows_with_common_elements(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Return rows of df where the lists in col1 and col2 have at least one common element.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        col1 (str): The name of the first column containing lists.\n",
    "        col2 (str): The name of the second column containing lists.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame filtered to include only rows where col1 and col2 have a common element.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering rows with common elements in columns: {col1} and {col2}\")\n",
    "    mask = df.apply(lambda row: bool(set(row[col1]).intersection(row[col2])), axis=1)\n",
    "    return df[mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:07:28,328 - utils.dataloaders - INFO - Loading Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\datafeeds_with_ovr\\202503_df_issuer_level_with_ovr.csv\n",
      "2025-03-27 20:07:28,784 - utils.dataloaders - INFO - Successfully loaded Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\datafeeds_with_ovr\\202503_df_issuer_level_with_ovr.csv\n",
      "2025-03-27 20:07:28,786 - utils.dataloaders - INFO - Loading Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n",
      "2025-03-27 20:07:29,297 - utils.dataloaders - INFO - Successfully loaded Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "# clarity data\n",
    "df_1 = load_clarity_data(df_1_path, columns_to_read)\n",
    "df_2 = load_clarity_data(df_2_path, columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns in df_1 and df_2 using the rename_dict\n",
    "df_1.rename(columns=rename_dict, inplace=True)\n",
    "df_2.rename(columns=rename_dict, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:07:29,324 - utils.dataloaders - INFO - Loading portfolio_carteras data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-27 20:07:43,081 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_carteras\n",
      "2025-03-27 20:07:43,083 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-03-27 20:07:43,089 - utils.dataloaders - INFO - Converting column 'portfolio_id' to string.\n",
      "2025-03-27 20:07:43,091 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-27 20:07:43,092 - utils.dataloaders - INFO - Loading portfolio_benchmarks data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-27 20:07:55,028 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_benchmarks\n",
      "2025-03-27 20:07:55,030 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-03-27 20:07:55,034 - utils.dataloaders - INFO - Converting column 'benchmark_id' to string.\n",
      "2025-03-27 20:07:55,036 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-27 20:07:55,037 - utils.dataloaders - INFO - Loading crossreference data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n",
      "2025-03-27 20:07:55,246 - utils.dataloaders - INFO - Cleaning columns and renaming crossreference data\n",
      "2025-03-27 20:07:55,249 - utils.dataloaders - INFO - Successfully loaded crossreference from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n"
     ]
    }
   ],
   "source": [
    "# aladdin /brs data / perimetros\n",
    "brs_carteras = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_carteras\")    \n",
    "brs_benchmarks = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_benchmarks\")\n",
    "crosreference = load_crossreference(CROSSREFERENCE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:07:55,261 - pre-ovr-analysis - INFO - Adding aladdin_id to clarity dfs\n"
     ]
    }
   ],
   "source": [
    "# add aladdin_id to df_1 and df_2\n",
    "logger.info(\"Adding aladdin_id to clarity dfs\")\n",
    "df_1 = df_1.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")\n",
    "df_2 = df_2.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_carteras_issuerlevel = get_issuer_level_df(brs_carteras, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_benchmarks_issuerlevel = get_issuer_level_df(brs_benchmarks, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading overrides columns ['clarityid', 'permid', 'brs_id', 'ovr_target', 'ovr_value']\n",
      "2025-03-27 20:07:55,462 - utils.dataloaders - INFO - Loading overrides from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\overrides\\overrides_db.xlsx\n"
     ]
    }
   ],
   "source": [
    "# sri/ESG Team data\n",
    "overrides = load_overrides(OVR_PATH)\n",
    "# rename column brs_id to aladdin_id\n",
    "overrides.rename(columns={\"brs_id\": \"aladdin_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_dict = override_dict(overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:07:56,631 - utils.dataloaders - INFO - Loading portfolios from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-27 20:08:13,435 - utils.dataloaders - INFO - Loading benchmarks from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-27 20:08:23,900 - utils.dataloaders - INFO - Loading strategy data for portfolios from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\portfolios_committees\\portfolio_lists.xlsx\n",
      "2025-03-27 20:08:23,931 - utils.dataloaders - INFO - Loading strategy data for benchmarks from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\portfolios_committees\\portfolio_lists.xlsx\n",
      "2025-03-27 20:08:23,946 - utils.dataloaders - INFO - Benchmark ID 'FIFSANEUR' appears in multiple strategies\n",
      "2025-03-27 20:08:23,947 - utils.dataloaders - INFO - Benchmark ID 'MSNTCWEUR' appears in multiple strategies\n",
      "2025-03-27 20:08:23,949 - utils.dataloaders - INFO - Benchmark ID 'MSACWLDNET' appears in multiple strategies\n",
      "2025-03-27 20:08:23,950 - utils.dataloaders - INFO - Benchmark ID 'SNP500NR2' appears in multiple strategies\n",
      "2025-03-27 20:08:23,952 - utils.dataloaders - INFO - Benchmark ID 'MSCIEURNT' appears in multiple strategies\n",
      "2025-03-27 20:08:23,953 - utils.dataloaders - INFO - Benchmark ID 'IBEX35NR' appears in multiple strategies\n",
      "2025-03-27 20:08:23,955 - utils.dataloaders - INFO - Benchmark ID 'MLG5E0EUR' appears in multiple strategies\n",
      "2025-03-27 20:08:23,956 - utils.dataloaders - INFO - Benchmark ID 'MLEAASEUR' appears in multiple strategies\n",
      "2025-03-27 20:08:23,958 - utils.dataloaders - INFO - Benchmark ID 'STXEZ50' appears in multiple strategies\n",
      "2025-03-27 20:08:24,090 - utils.dataloaders - INFO - Filtering strategy entries\n",
      "2025-03-27 20:08:24,091 - utils.dataloaders - INFO - Dictonary cleaned of empty strategies\n",
      "2025-03-27 20:08:24,092 - utils.dataloaders - INFO - Filtering strategy entries\n",
      "2025-03-27 20:08:24,096 - utils.dataloaders - INFO - Dictonary cleaned of empty strategies\n"
     ]
    }
   ],
   "source": [
    "# Load portfolios & benchmarks dicts\n",
    "(\n",
    "    portfolio_dict,\n",
    "    benchmark_dict,\n",
    ") = load_portfolios(path_pb=BMK_PORTF_STR_PATH, path_committe=COMMITTEE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADM0PSA1 {'aladdin_id': ['J66430', 'R57958', 'R63005', 'R63005', 'F95505', 'F95505', 'B95488', 'I20488', 'I20488', 'I20488', 'R49188', '052591', 'R83199', '007699', 'R63609', 'H44516', '077779', '05564X', 'N15397', '128003', '128003', 'D43041', '191216', 'C96217', 'C70614', 'R50818', 'R57820', 'R60098', 'R60098', 'D34426', '36827E', '36827E', 'R51121', '24820T', 'F60954', 'C05702', 'C05702', 'D85398', 'FR0570', 'FR0570', 'FR0570', 'FR0570', 'R62757', 'R62757', 'R62757', 'K23944', '38142U', '38142U', 'F92008', 'R59316', 'C70514', 'R71927', 'C05671', 'R63519', 'R76481', 'R76481', 'R62699', 'R62699', '459200', '616880', 'R57939', 'R57939', 'R62038', 'R62038', 'R62038', 'C97848', 'R63600', 'R68028', '580135', '580135', 'G97316', 'D90986', 'R96795', 'MIZ00A', 'R55597', 'R65300', 'R68589', 'R57898', '128009', '128009', '713448', '713448', 'R57931', '742718', 'F13060', '760325', 'I40729', 'R65375', 'C74380', 'C74380', 'R21880', 'R21880', 'R57942', '830505', 'R49692', 'R49692', 'B89867', 'B89867', 'R65405', '846332', '846332', '846332', 'R64256', 'R58692', 'B89109', 'R28539', 'R57938', 'R56536', 'F50451', 'R59991', 'R59991', 'G88159', '911308', 'R60214', '949740', '949740', '949740'], 'strategy_name': 'scs_002_ec'}\n",
      "ADM0PSA2 {'aladdin_id': ['J66430', 'R57958', 'R63005', 'F95505', '052591', '007699', 'R63609', 'H44516', '077779', 'E81396', '128003', '128003', 'D43041', '172967', '191216', 'C96217', 'R60098', '36827E', '36827E', 'F60954', 'C05702', 'C05702', 'D85398', 'FR0570', 'FR0570', 'FR0570', 'FR0570', 'R62757', 'R62757', 'R62757', '38142U', 'R71927', 'R76481', 'R76481', 'R62699', '459200', '616880', 'R57939', 'R62038', 'R62038', 'R62038', 'R63600', 'R68028', '580135', 'G97316', 'R96795', 'R55597', 'R65300', 'R57898', '128009', '128009', '713448', '742718', 'I40729', 'R57476', 'R65375', 'C74380', 'R21880', 'R57942', '830505', 'R49692', 'R49692', 'B89867', 'R65405', '846332', '846332', '846332', 'R64256', 'R58692', 'B89109', 'R57938', 'R56536', 'R59991', 'G88159', '911308', 'R60214', '949740'], 'strategy_name': 'scs_002_ec'}\n",
      "CL19778 {'aladdin_id': ['C05702', 'C05702'], 'strategy_name': 'str_006_sec'}\n",
      "CPE00035 {'aladdin_id': ['I66408', 'E99416', '788350', '059456', '066050', '072730', 'J54377', 'R73574', 'D43041', 'D43041', '172967', '202597', '202597', '128005', 'G37009', 'R71411', 'R70538', '36827E', 'C05702', 'C05702', 'C05702', 'R44788', 'R66333', 'G32459', 'J95157', 'C05332', 'R74554', 'H92117', 'I03819', 'R57886', 'R94740', 'R93558', 'G88159', 'C05702', 'G68355'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00169 {'aladdin_id': ['I45020', 'G16611', 'E94490', 'K10973', 'C05702', 'C05702', 'F79320', 'G18629', 'G36494', 'G05261', 'I69133', 'G21293', '128009', 'R57882', 'H92117', 'I03819', 'G66335', 'G34089'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00264 {'aladdin_id': ['G06219', 'J10626', 'R59305', '059456', 'R49427', '05564X', 'C05702', 'R72674', 'H09269', 'C05702', 'R57928', 'C05702', 'R71411', 'F97381', 'R51302', 'C05702', 'C05702', 'F04261', 'FR0570', 'C05702', 'R63519', 'R71197', 'R65993', 'D17394', 'G18629', 'R60526', 'R57925', 'R57535', 'R54282', '128009', 'L77985', 'R74554', 'R55374', 'D88771', 'R21809', 'R77126', 'R93558', 'C05702', 'R60254', 'G29077'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00277 {'aladdin_id': ['G27169', '007699', 'D39873', 'D28969', 'E94490', 'R64339', '28503A', '28503A', '28503A', 'C05702', 'C05702', 'FR0570', 'FR0570', 'R62757', 'R62757', 'R63519', 'R60526', 'R64281', 'R57955', '846332', '846332', 'R64256', 'E97107', '92857L'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00363 {'aladdin_id': ['I42601', 'C85876', 'D80441', '059456', 'E94490', 'D43041', 'D43041', '202597', 'G37009', 'C05702', 'C05702', 'G22384', 'F38353', 'F32400', 'R54462', 'G21293', 'C05332', 'H92117', 'I03819', 'R77126', 'G66335'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00375 {'aladdin_id': ['J16059', 'E94490', 'C05702', 'C05702', 'G18629', 'I69133', 'F32400', 'F71548', 'G21293', 'G34089'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00448 {'aladdin_id': ['C85876', 'D39736', 'E81396', 'D43041', 'C05702', 'K10166', 'F71548', 'H92117', 'I03819', 'R77126', '912794', 'C05702'], 'strategy_name': 'str_003b_ec'}\n"
     ]
    }
   ],
   "source": [
    "for i, (k,v) in enumerate(portfolio_dict.items()):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = set(df_1.columns) & set(df_2.columns) & set(brs_carteras_issuerlevel.columns)\n",
    "common_cols = sorted(list(common_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START PRE-OVR ANALISIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:08:24,144 - pre-ovr-analysis - INFO - Setting index to permid.\n",
      "2025-03-27 20:08:24,265 - pre-ovr-analysis - INFO - Number of common indexes: 69222\n",
      "2025-03-27 20:08:24,326 - pre-ovr-analysis - INFO - Number of new issuers: 106\n",
      "2025-03-27 20:08:24,328 - pre-ovr-analysis - INFO - Number of missing issuers: 56\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA CLARITY LEVEL\n",
    "(\n",
    "    df_1, \n",
    "    df_2,\n",
    "    new_issuers_clarity,\n",
    "    out_issuer_clarity,\n",
    ") = prepare_dataframes(df_1, df_2)\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number of new issuers: {new_issuers_clarity.shape[0]}\")\n",
    "logger.info(f\"Number of missing issuers: {out_issuer_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permid</th>\n",
       "      <th>issuer_name</th>\n",
       "      <th>str_001_s</th>\n",
       "      <th>str_002_ec</th>\n",
       "      <th>str_003_ec</th>\n",
       "      <th>str_004_asec</th>\n",
       "      <th>str_005_ec</th>\n",
       "      <th>scs_001_sec</th>\n",
       "      <th>scs_002_ec</th>\n",
       "      <th>str_006_sec</th>\n",
       "      <th>str_sfdr8_aec</th>\n",
       "      <th>str_003b_ec</th>\n",
       "      <th>aladdin_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4295857675</td>\n",
       "      <td>Excelsior Capital Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>D73574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4295858270</td>\n",
       "      <td>Poseidon Nickel Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>D70698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4295858807</td>\n",
       "      <td>Silver Lake Deflector Pty Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4295859680</td>\n",
       "      <td>Azevedo &amp; Travassos SA</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>F80754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4295860216</td>\n",
       "      <td>Dimed SA Distribuidora de Medicamentos</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>F85102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       permid                             issuer_name str_001_s str_002_ec  \\\n",
       "0  4295857675                   Excelsior Capital Ltd        OK         OK   \n",
       "1  4295858270                     Poseidon Nickel Ltd        OK         OK   \n",
       "2  4295858807           Silver Lake Deflector Pty Ltd        OK         OK   \n",
       "3  4295859680                  Azevedo & Travassos SA        OK         OK   \n",
       "4  4295860216  Dimed SA Distribuidora de Medicamentos        OK         OK   \n",
       "\n",
       "  str_003_ec str_004_asec str_005_ec scs_001_sec scs_002_ec str_006_sec  \\\n",
       "0         OK     EXCLUDED         OK    EXCLUDED         OK    EXCLUDED   \n",
       "1         OK     EXCLUDED         OK    EXCLUDED         OK    EXCLUDED   \n",
       "2         OK     EXCLUDED         OK    EXCLUDED         OK          OK   \n",
       "3         OK     EXCLUDED         OK          OK         OK          OK   \n",
       "4         OK           OK         OK          OK         OK          OK   \n",
       "\n",
       "  str_sfdr8_aec str_003b_ec aladdin_id  \n",
       "0            OK          OK     D73574  \n",
       "1            OK          OK     D70698  \n",
       "2            OK          OK        NaN  \n",
       "3            OK          OK     F80754  \n",
       "4            OK          OK     F85102  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset index for new_issuers_clarity and out_issuer_clarity\n",
    "new_issuers_clarity.reset_index(inplace=True)\n",
    "out_issuer_clarity.reset_index(inplace=True)\n",
    "# drop isin from out_issuer_clarity and new_issuers_clarity\n",
    "out_issuer_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "new_issuers_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "\n",
    "# remember to remove empyt empyt aladin id\n",
    "new_issuers_clarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:08:24,367 - pre-ovr-analysis - INFO - Setting index to aladdin_id.\n",
      "2025-03-27 20:08:24,509 - pre-ovr-analysis - INFO - Number of common indexes: 2552\n",
      "2025-03-27 20:08:24,584 - pre-ovr-analysis - INFO - Number issuers in clarity but not Aladdin: 66670\n",
      "2025-03-27 20:08:24,586 - pre-ovr-analysis - INFO - Number issuers in Aladdin but not Clarity: 1303\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA CARTERAS BRS LEVEL\n",
    "(\n",
    "    brs_df, \n",
    "    clarity_df,\n",
    "    in_clarity_but_not_in_brs,\n",
    "    in_brs_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_carteras_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not Aladdin: {in_clarity_but_not_in_brs.shape[0]}\")\n",
    "logger.info(f\"Number issuers in Aladdin but not Clarity: {in_brs_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:08:24,598 - pre-ovr-analysis - INFO - Setting index to aladdin_id.\n",
      "2025-03-27 20:08:24,746 - pre-ovr-analysis - INFO - Number of common indexes: 2552\n",
      "2025-03-27 20:08:24,855 - pre-ovr-analysis - INFO - Number issuers in clarity but not benchmarks: 66670\n",
      "2025-03-27 20:08:24,857 - pre-ovr-analysis - INFO - Number issuers in benchmarks but not Clarity: 1303\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA BENCHMARK BRS LEVEL\n",
    "(\n",
    "    brs_df_benchmarks, \n",
    "    clarity_df_benchmarks,\n",
    "    in_clarity_but_not_in_brs_benchmarks,\n",
    "    in_brs_benchmark_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_benchmarks_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not benchmarks: {in_clarity_but_not_in_brs_benchmarks.shape[0]}\")\n",
    "logger.info(f\"Number issuers in benchmarks but not Clarity: {in_brs_benchmark_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:08:24,871 - pre-ovr-analysis - INFO - comparing clarity dataframes\n",
      "2025-03-27 20:08:24,901 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-03-27 20:08:24,920 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-03-27 20:08:24,940 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-03-27 20:08:24,955 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-03-27 20:08:24,972 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-03-27 20:08:24,986 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-03-27 20:08:24,999 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-03-27 20:08:25,018 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-03-27 20:08:25,031 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-03-27 20:08:25,043 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-03-27 20:08:25,054 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-03-27 20:08:25,069 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 498\n",
      "2025-03-27 20:08:25,071 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-03-27 20:08:25,090 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 262\n",
      "2025-03-27 20:08:25,093 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-03-27 20:08:25,114 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 211\n",
      "2025-03-27 20:08:25,117 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-03-27 20:08:25,136 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 165\n",
      "2025-03-27 20:08:25,139 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-03-27 20:08:25,155 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 838\n",
      "2025-03-27 20:08:25,156 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-03-27 20:08:25,173 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 206\n",
      "2025-03-27 20:08:25,175 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-03-27 20:08:25,198 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 546\n",
      "2025-03-27 20:08:25,201 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-03-27 20:08:25,213 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 50\n",
      "2025-03-27 20:08:25,216 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-03-27 20:08:25,239 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 610\n",
      "2025-03-27 20:08:25,241 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-03-27 20:08:25,257 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 444\n",
      "2025-03-27 20:08:26,701 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-03-27 20:08:26,714 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 485\n",
      "2025-03-27 20:08:26,717 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-03-27 20:08:26,727 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 604\n",
      "2025-03-27 20:08:26,730 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-03-27 20:08:26,742 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 244\n",
      "2025-03-27 20:08:26,744 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-03-27 20:08:26,756 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 151\n",
      "2025-03-27 20:08:26,758 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-03-27 20:08:26,770 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 686\n",
      "2025-03-27 20:08:26,772 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-03-27 20:08:26,784 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 1248\n",
      "2025-03-27 20:08:26,785 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-03-27 20:08:26,798 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 788\n",
      "2025-03-27 20:08:26,799 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-03-27 20:08:26,810 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 136\n",
      "2025-03-27 20:08:26,811 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-03-27 20:08:26,823 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 685\n",
      "2025-03-27 20:08:26,824 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-03-27 20:08:26,836 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 296\n",
      "2025-03-27 20:08:31,000 - pre-ovr-analysis - INFO - Final delta shape: (4184, 18)\n",
      "2025-03-27 20:08:31,008 - pre-ovr-analysis - INFO - checking impact compared to BRS portfolio data\n",
      "2025-03-27 20:08:31,010 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-03-27 20:08:31,014 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-03-27 20:08:31,016 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-03-27 20:08:31,017 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-03-27 20:08:31,019 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-03-27 20:08:31,021 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-03-27 20:08:31,024 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-03-27 20:08:31,029 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-03-27 20:08:31,033 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-03-27 20:08:31,036 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-03-27 20:08:31,039 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-03-27 20:08:31,042 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 204\n",
      "2025-03-27 20:08:31,044 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-03-27 20:08:31,048 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 226\n",
      "2025-03-27 20:08:31,048 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-03-27 20:08:31,051 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 125\n",
      "2025-03-27 20:08:31,052 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-03-27 20:08:31,055 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 109\n",
      "2025-03-27 20:08:31,056 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-03-27 20:08:31,058 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 632\n",
      "2025-03-27 20:08:31,060 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-03-27 20:08:31,063 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 320\n",
      "2025-03-27 20:08:31,064 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-03-27 20:08:31,066 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 436\n",
      "2025-03-27 20:08:31,067 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-03-27 20:08:31,068 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 19\n",
      "2025-03-27 20:08:31,069 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-03-27 20:08:31,071 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 474\n",
      "2025-03-27 20:08:31,072 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-03-27 20:08:31,075 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 347\n",
      "2025-03-27 20:08:31,118 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-03-27 20:08:31,120 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 0\n",
      "2025-03-27 20:08:31,121 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-03-27 20:08:31,123 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 0\n",
      "2025-03-27 20:08:31,124 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-03-27 20:08:31,125 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 0\n",
      "2025-03-27 20:08:31,128 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-03-27 20:08:31,131 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 0\n",
      "2025-03-27 20:08:31,132 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-03-27 20:08:31,133 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 0\n",
      "2025-03-27 20:08:31,135 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-03-27 20:08:31,136 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 0\n",
      "2025-03-27 20:08:31,138 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-03-27 20:08:31,141 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 0\n",
      "2025-03-27 20:08:31,144 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-03-27 20:08:31,149 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 0\n",
      "2025-03-27 20:08:31,150 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-03-27 20:08:31,153 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 0\n",
      "2025-03-27 20:08:31,154 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-03-27 20:08:31,157 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n740789\\AppData\\Local\\Temp\\ipykernel_37656\\2056266402.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  delta[target_index] = delta[target_index].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:08:31,278 - pre-ovr-analysis - INFO - Final delta shape: (2552, 17)\n",
      "2025-03-27 20:08:31,281 - pre-ovr-analysis - INFO - checking impact compared to BRS benchmarks data\n",
      "2025-03-27 20:08:31,283 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-03-27 20:08:31,285 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-03-27 20:08:31,287 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-03-27 20:08:31,289 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-03-27 20:08:31,291 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-03-27 20:08:31,292 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-03-27 20:08:31,297 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-03-27 20:08:31,300 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-03-27 20:08:31,305 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-03-27 20:08:31,308 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-03-27 20:08:31,314 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-03-27 20:08:31,317 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 204\n",
      "2025-03-27 20:08:31,318 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-03-27 20:08:31,320 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 226\n",
      "2025-03-27 20:08:31,322 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-03-27 20:08:31,324 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 125\n",
      "2025-03-27 20:08:31,325 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-03-27 20:08:31,328 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 109\n",
      "2025-03-27 20:08:31,329 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-03-27 20:08:31,332 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 632\n",
      "2025-03-27 20:08:31,334 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-03-27 20:08:31,337 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 320\n",
      "2025-03-27 20:08:31,339 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-03-27 20:08:31,341 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 436\n",
      "2025-03-27 20:08:31,344 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-03-27 20:08:31,349 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 19\n",
      "2025-03-27 20:08:31,350 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-03-27 20:08:31,352 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 474\n",
      "2025-03-27 20:08:31,354 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-03-27 20:08:31,355 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 347\n",
      "2025-03-27 20:08:31,397 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-03-27 20:08:31,399 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 0\n",
      "2025-03-27 20:08:31,400 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-03-27 20:08:31,403 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 0\n",
      "2025-03-27 20:08:31,404 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-03-27 20:08:31,407 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 0\n",
      "2025-03-27 20:08:31,408 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-03-27 20:08:31,409 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 0\n",
      "2025-03-27 20:08:31,411 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-03-27 20:08:31,413 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 0\n",
      "2025-03-27 20:08:31,415 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-03-27 20:08:31,418 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 0\n",
      "2025-03-27 20:08:31,419 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-03-27 20:08:31,422 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 0\n",
      "2025-03-27 20:08:31,423 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-03-27 20:08:31,426 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 0\n",
      "2025-03-27 20:08:31,427 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-03-27 20:08:31,430 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 0\n",
      "2025-03-27 20:08:31,432 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-03-27 20:08:31,435 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 0\n",
      "2025-03-27 20:08:31,562 - pre-ovr-analysis - INFO - Final delta shape: (2552, 17)\n"
     ]
    }
   ],
   "source": [
    "# COMPARE DATA\n",
    "logger.info(\"comparing clarity dataframes\")\n",
    "delta_clarity = compare_dataframes(df_1, df_2)\n",
    "delta_clarity = check_new_exclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = check_new_inclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = finalize_delta(delta_clarity)\n",
    "logger.info(\"checking impact compared to BRS portfolio data\")\n",
    "delta_brs = compare_dataframes(brs_df, clarity_df)\n",
    "delta_brs = check_new_exclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = check_new_inclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = finalize_delta(delta_brs, target_index=\"aladdin_id\")\n",
    "logger.info(\"checking impact compared to BRS benchmarks data\")\n",
    "delta_benchmarks = compare_dataframes(brs_df_benchmarks, clarity_df_benchmarks)\n",
    "delta_benchmarks = check_new_exclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = check_new_inclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = finalize_delta(delta_benchmarks, target_index=\"aladdin_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.zombie_killer import main as zombie_killer\n",
    "#logger.info(\"Getting zombie analysis df\")\n",
    "#zombie_df = zombie_killer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:10:59,942 - pre-ovr-analysis - INFO - Preparing deltas before saving\n"
     ]
    }
   ],
   "source": [
    "# PREP DELTAS BEFORE SAVING\n",
    "logger.info(\"Preparing deltas before saving\")\n",
    "# use crossreference to add permid to delta_brs\n",
    "delta_brs = delta_brs.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "delta_benchmarks = delta_benchmarks.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "# drop isin from deltas\n",
    "delta_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_brs.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_benchmarks.drop(columns=[\"isin\"], inplace=True)\n",
    "# add new column to delta_brs with ovr_dict value using aladdin_id\n",
    "delta_brs[\"ovr_list\"] = delta_brs[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_clarity[\"ovr_list\"] = delta_clarity[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_benchmarks[\"ovr_list\"] = delta_benchmarks[\"aladdin_id\"].map(ovr_dict)\n",
    "# let's add portfolio info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_clarity)\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_brs)\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_benchmarks)\n",
    "# let's add benchmark info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_clarity, \"affected_benchmark_str\")\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_brs, \"affected_benchmark_str\")\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_benchmarks, \"affected_benchmark_str\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:23:39,164 - pre-ovr-analysis - INFO - Filtering rows with common elements in columns: exclusion_list_brs and affected_portfolio_str\n",
      "2025-03-27 20:23:39,173 - pre-ovr-analysis - INFO - Filtering rows with common elements in columns: exclusion_list_brs and affected_portfolio_str\n"
     ]
    }
   ],
   "source": [
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_brs = filter_non_empty_lists(delta_brs, \"affected_portfolio_str\")\n",
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_benchmarks = filter_non_empty_lists(delta_benchmarks, \"affected_portfolio_str\")\n",
    "# pass filter_rows_with_common_elements for columns exclusion_list_brs and affected_portfolio_str\n",
    "delta_brs = filter_rows_with_common_elements(delta_brs, \"exclusion_list_brs\", \"affected_portfolio_str\")\n",
    "delta_benchmarks = filter_rows_with_common_elements(delta_benchmarks, \"exclusion_list_brs\", \"affected_portfolio_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2552 entries, 0 to 2551\n",
      "Data columns (total 20 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   aladdin_id              2552 non-null   object\n",
      " 1   issuer_name             2552 non-null   object\n",
      " 2   str_001_s               2552 non-null   object\n",
      " 3   str_002_ec              2552 non-null   object\n",
      " 4   str_003_ec              2552 non-null   object\n",
      " 5   str_004_asec            2552 non-null   object\n",
      " 6   str_005_ec              392 non-null    object\n",
      " 7   scs_001_sec             519 non-null    object\n",
      " 8   scs_002_ec              376 non-null    object\n",
      " 9   str_006_sec             500 non-null    object\n",
      " 10  str_sfdr8_aec           2552 non-null   object\n",
      " 11  str_003b_ec             2552 non-null   object\n",
      " 12  new_exclusion           2552 non-null   bool  \n",
      " 13  exclusion_list_brs      2552 non-null   object\n",
      " 14  new_inclusion           2552 non-null   bool  \n",
      " 15  inclusion_list_brs      2552 non-null   object\n",
      " 16  permid                  2552 non-null   object\n",
      " 17  ovr_list                238 non-null    object\n",
      " 18  affected_portfolio_str  2552 non-null   object\n",
      " 19  affected_benchmark_str  2552 non-null   object\n",
      "dtypes: bool(2), object(18)\n",
      "memory usage: 364.0+ KB\n"
     ]
    }
   ],
   "source": [
    "delta_benchmarks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# define a function to save results in an Excel file\n",
    "def save_excel(df_dict: dict, output_dir: Path, file_name: str) -> Path:\n",
    "    \"\"\"\n",
    "    Writes multiple DataFrames to an Excel file with each DataFrame in a separate sheet.\n",
    "\n",
    "    Parameters:\n",
    "    - df_dict (dict): A dictionary where keys are sheet names and values are DataFrames.\n",
    "    - output_dir (Path): The directory where the Excel file will be saved.\n",
    "    - file_name (str): The base name for the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    - Path: The full path to the saved Excel file.\n",
    "    \"\"\"\n",
    "    # Create a date string in \"YYYYMMDD\" format\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    logger.info(\"Creating output directory: %s\", output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Construct the full output file path (e.g., file_name_YYYYMMDD.xlsx)\n",
    "    output_file = output_dir / f\"{date_str}_{file_name}.xlsx\"\n",
    "\n",
    "    # Write each DataFrame to its own sheet with index set to False\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        logger.info(\"Writing DataFrames to Excel file: %s\", output_file)\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            logger.info(\"Writing sheet: %s\", sheet_name)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    logger.info(\"Results saved to Excel file: %s\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 20:23:52,380 - pre-ovr-analysis - INFO - Creating output directory: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\n",
      "2025-03-27 20:23:52,386 - pre-ovr-analysis - INFO - Writing DataFrames to Excel file: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\\20250327_pre_ovr_analysis_betathree.xlsx\n",
      "2025-03-27 20:23:52,390 - pre-ovr-analysis - INFO - Writing sheet: delta_carteras\n",
      "2025-03-27 20:23:52,433 - pre-ovr-analysis - INFO - Writing sheet: delta_benchmarks\n",
      "2025-03-27 20:23:52,494 - pre-ovr-analysis - INFO - Writing sheet: delta_clarity\n",
      "2025-03-27 20:23:54,480 - pre-ovr-analysis - INFO - Writing sheet: new_issuers_clarity\n",
      "2025-03-27 20:23:54,522 - pre-ovr-analysis - INFO - Writing sheet: out_issuer_clarity\n",
      "2025-03-27 20:23:54,942 - pre-ovr-analysis - INFO - Results saved to Excel file: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\\20250327_pre_ovr_analysis_betathree.xlsx\n"
     ]
    }
   ],
   "source": [
    "# MAKE SURE ISSUER HAS ALL THE PORTFOLIOS IN THEIR LIST NOT JUST THE LAST ONE\n",
    "\n",
    "# create dict of df and df name\n",
    "dfs_dict = {\n",
    "    #\"zombie_analysis\": zombie_df,\n",
    "    \"delta_carteras\": delta_brs,\n",
    "    \"delta_benchmarks\": delta_benchmarks,\n",
    "    \"delta_clarity\": delta_clarity,\n",
    "    \"new_issuers_clarity\": new_issuers_clarity,\n",
    "    \"out_issuer_clarity\": out_issuer_clarity,\n",
    "}\n",
    "# save to excel\n",
    "save_excel(dfs_dict, OUTPUT_DIR, file_name=\"pre_ovr_analysis_betathree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

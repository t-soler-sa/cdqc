{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:42:43,925 - utils.get_date - INFO - Date format is valid. Date set to 202504.\n",
      "Output directory for script zombie-killer is set to: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\zombie_list\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.dataloaders import (\n",
    "    load_clarity_data,\n",
    "    load_aladdin_data,\n",
    "    load_crossreference,\n",
    "    load_portfolios,\n",
    "    load_overrides,\n",
    "    save_excel\n",
    ")\n",
    "from utils.zombie_killer import main as zombie_killer\n",
    "\n",
    "# Import the centralized configuration\n",
    "from config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:42:47,171 - utils.get_date - INFO - Date format is valid. Date set to 202504.\n",
      "Output directory for script pre-ovr-analysis is set to: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\n"
     ]
    }
   ],
   "source": [
    "# Get the common configuration for the Pre-OVR-Analysis script.\n",
    "config = get_config(\"pre-ovr-analysis\", interactive=False)\n",
    "logger = config[\"logger\"]\n",
    "DATE = config[\"DATE\"]\n",
    "YEAR = config[\"YEAR\"]\n",
    "DATE_PREV = config[\"DATE_PREV\"]\n",
    "REPO_DIR = config[\"REPO_DIR\"]\n",
    "DATAFEED_DIR = config[\"DATAFEED_DIR\"]\n",
    "SRI_DATA_DIR = config[\"SRI_DATA_DIR\"]\n",
    "paths = config[\"paths\"]\n",
    "\n",
    "# Use the paths from config\n",
    "df_1_path = paths[\"PRE_DF_WOVR_PATH\"]\n",
    "df_2_path = paths[\"CURRENT_DF_WOUTOVR_PATH\"]\n",
    "CROSSREFERENCE_PATH = paths[\"CROSSREFERENCE_PATH\"]\n",
    "BMK_PORTF_STR_PATH = paths[\"BMK_PORTF_STR_PATH\"]\n",
    "OVR_PATH = paths[\"OVR_PATH\"]\n",
    "COMMITTEE_PATH = paths[\"COMMITTEE_PATH\"]\n",
    "\n",
    "# Define the output directory and file based on the configuration.\n",
    "OUTPUT_DIR = config[\"OUTPUT_DIR\"]\n",
    "OUTPUT_FILE = OUTPUT_DIR / f\"{DATE}_pre_ovr_analysis.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore workbook warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202504 and 2025 and 202503.\n"
     ]
    }
   ],
   "source": [
    "# check that the date constants are set correctly\n",
    "print(f\"{DATE} and {YEAR} and {DATE_PREV}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TEST COLUMNS & DICTS\n",
    "# let's define necessary column lists\n",
    "\n",
    "id_name_cols = [\"permid\", \"isin\", \"issuer_name\"]\n",
    "id_name_issuers_cols = [\"aladdin_id\", \"permid\", \"issuer_name\"]\n",
    "clarity_test_col = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"art_8_basicos\",\n",
    "    \"str_006_sec\",\n",
    "    \"cs_001_sec\",\n",
    "    \"cs_002_ec\",\n",
    "]\n",
    "columns_to_read = id_name_cols + clarity_test_col\n",
    "delta_test_cols = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"str_006_sec\",\n",
    "    \"str_sfdr8_aec\",\n",
    "    \"scs_001_sec\",\n",
    "    \"scs_002_ec\",\n",
    "]\n",
    "\n",
    "brs_test_cols = [\"aladdin_id\"] + delta_test_cols\n",
    "rename_dict = {\n",
    "    \"cs_001_sec\": \"scs_001_sec\",\n",
    "    \"cs_002_ec\": \"scs_002_ec\",\n",
    "    \"art_8_basicos\": \"str_sfdr8_aec\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframes(\n",
    "    base_df: pd.DataFrame, new_df: pd.DataFrame, target_index:str = \"permid\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare DataFrames by setting the index and filtering for common indexes.\n",
    "    Logs info about common, new, and missing indexes.\n",
    "    \"\"\"\n",
    "    # Set index to 'permid' if it exists, otherwise assume it's already the index.\n",
    "    logger.info(f\"Setting index to {target_index}.\")\n",
    "    if target_index in base_df.columns:\n",
    "        base_df = base_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df1 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    if target_index in new_df.columns:\n",
    "        new_df = new_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df2 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    common_indexes = base_df.index.intersection(new_df.index)\n",
    "    new_indexes = new_df.index.difference(base_df.index)\n",
    "    missing_indexes = base_df.index.difference(new_df.index)\n",
    "\n",
    "    logger.info(f\"Number of common indexes: {len(common_indexes)}\")\n",
    "\n",
    "    return (\n",
    "        base_df.loc[common_indexes],\n",
    "        new_df.loc[common_indexes],\n",
    "        new_df.loc[new_indexes],\n",
    "        base_df.loc[missing_indexes],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(\n",
    "    df1: pd.DataFrame, df2: pd.DataFrame, test_col: List[str] = delta_test_cols\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare DataFrames and create a delta DataFrame.\"\"\"\n",
    "    delta = df2.copy()\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Comparing column: {col}\")\n",
    "            # Create a mask for differences between the two DataFrames\n",
    "            diff_mask = df1[col] != df2[col]\n",
    "            # Update the delta DataFrame with the differences\n",
    "            delta.loc[~diff_mask, col] = np.nan\n",
    "    return delta\n",
    "\n",
    "def get_exclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed to EXCLUDED.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] == \"EXCLUDED\" and df1.loc[row.name, col] != \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "def get_inclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed from EXCLUDED to any other value.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] != \"EXCLUDED\" and df1.loc[row.name, col] == \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "def check_new_exclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new exclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_exclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new exclusions in column: {col}\")\n",
    "            mask = (df1[col] != \"EXCLUDED\") & (df2[col] == \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_exclusion\"] = True\n",
    "            logger.info(f\"Number of new exclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"exclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_exclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "def check_new_inclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new inclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_inclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new inclusions in column: {col}\")\n",
    "            mask = (df1[col] == \"EXCLUDED\") & (df2[col] != \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_inclusion\"] = True\n",
    "            logger.info(f\"Number of new inclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"inclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_inclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "def finalize_delta(\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    target_index: str = \"permid\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Finalize the delta DataFrame by removing unchanged rows and resetting the index.\"\"\"\n",
    "    delta = delta.dropna(subset=test_col, how=\"all\")\n",
    "    delta.reset_index(inplace=True)\n",
    "    delta[target_index] = delta[target_index].astype(str)\n",
    "    logger.info(f\"Final delta shape: {delta.shape}\")\n",
    "    return delta\n",
    "\n",
    "def create_override_dict(\n",
    "    df: pd.DataFrame = None,\n",
    "    id_col: str = \"aladdin_id\",\n",
    "    str_col: str = \"ovr_target\",\n",
    "    ovr_col: str = \"ovr_value\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts the overrides DataFrame to a dictionary.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the overrides.\n",
    "        id_col (str): Column name for the identifier.\n",
    "        str_col (str): Column name for the strategy.\n",
    "        ovr_col (str): Column name for the override value.\n",
    "    Returns:\n",
    "        dict: Dictionary of overrides.\n",
    "    \"\"\"\n",
    "    # 1. Groupd the df by issuer_id\n",
    "    grouped = df.groupby(id_col)\n",
    "\n",
    "    # 2. Initialise the dictionary\n",
    "    ovr_dict = {}\n",
    "\n",
    "    # 3. Iterate over each group (issuer id and its corresponding rows)\n",
    "    for id, group_data in grouped:\n",
    "        # 3.1. for each issuer id create a dict pairing the strategy and the override value\n",
    "        ovr_result = dict(zip(group_data[str_col], group_data[ovr_col]))\n",
    "        # 3.2. add the dict to the main dict\n",
    "        ovr_dict[id] = ovr_result\n",
    "\n",
    "    return ovr_dict\n",
    "\n",
    "def add_portfolio_benchmark_info_to_df(\n",
    "    portfolio_dict, delta_df, column_name=\"affected_portfolio_str\"\n",
    "):\n",
    "\n",
    "    # Initialize a defaultdict to accumulate (portfolio_id, strategy_name) pairs\n",
    "    aladdin_to_info = defaultdict(list)\n",
    "\n",
    "    for portfolio_id, data in portfolio_dict.items():\n",
    "        strategy = data.get(\"strategy_name\")\n",
    "        for a_id in data.get(\"aladdin_id\", []):\n",
    "            aladdin_to_info[a_id].append((portfolio_id, strategy))\n",
    "\n",
    "    # Map each aladdin_id in delta_df to a list of accumulated portfolio info\n",
    "    delta_df[column_name] = delta_df[\"aladdin_id\"].apply(\n",
    "        lambda x: list(chain.from_iterable(aladdin_to_info.get(x, [])))\n",
    "    )\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "def get_issuer_level_df(df: pd.DataFrame, idx_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicates based on idx_name, and drops rows where idx_name column contains\n",
    "    NaN, None, or strings like \"nan\", \"NaN\", \"none\", or empty strings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        idx_name (str): Column name used for duplicate removal and NaN filtering.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Drop duplicates\n",
    "    df_cleaned = df.drop_duplicates(subset=[idx_name])\n",
    "\n",
    "    # Drop rows where idx_name is NaN/None or has invalid strings\n",
    "    valid_rows = df_cleaned[idx_name].notnull() & (\n",
    "        ~df_cleaned[idx_name]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .isin([\"nan\", \"none\", \"\"])\n",
    "    )\n",
    "\n",
    "    return df_cleaned[valid_rows]\n",
    "\n",
    "def filter_non_empty_lists(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame filtered so that rows where the specified column contains\n",
    "    an empty list are removed. Keeps rows where the column has a list with at least one element.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame\n",
    "    - column (str): The name of the column to check\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    return df[df[column].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "def filter_rows_with_common_elements(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Return rows of df where the lists in col1 and col2 have at least one common element.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        col1 (str): The name of the first column containing lists.\n",
    "        col2 (str): The name of the second column containing lists.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame filtered to include only rows where col1 and col2 have a common element.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering rows with common elements in columns: {col1} and {col2}\")\n",
    "    mask = df.apply(lambda row: bool(set(row[col1]).intersection(row[col2])), axis=1)\n",
    "    return df[mask].copy()\n",
    "\n",
    "def reorder_columns(df:pd.DataFrame, keep_first:list[str], exclude:list[str]=None):\n",
    "    if exclude is None:\n",
    "        exclude = set()\n",
    "    return df[\n",
    "        keep_first\n",
    "        + [col for col in df.columns if col not in keep_first and col not in exclude]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:42:47,264 - utils.dataloaders - INFO - Loading Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\datafeeds_with_ovr\\202503_df_issuer_level_with_ovr.csv\n",
      "2025-04-02 19:42:47,773 - utils.dataloaders - INFO - Successfully loaded Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\datafeeds_with_ovr\\202503_df_issuer_level_with_ovr.csv\n",
      "2025-04-02 19:42:47,780 - utils.dataloaders - INFO - Loading Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n",
      "2025-04-02 19:42:48,217 - utils.dataloaders - INFO - Successfully loaded Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.    LOAD DATA\n",
    "# 1.1.  clarity data\n",
    "df_1 = load_clarity_data(df_1_path, columns_to_read)\n",
    "df_2 = load_clarity_data(df_2_path, columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns in df_1 and df_2 using the rename_dict\n",
    "df_1.rename(columns=rename_dict, inplace=True)\n",
    "df_2.rename(columns=rename_dict, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:42:48,252 - pre-ovr-analysis - INFO - previous clarity df's  rows: 69278, new clarity df's rows: 69328\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"previous clarity df's  rows: {df_1.shape[0]}, new clarity df's rows: {df_2.shape[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:42:48,265 - utils.dataloaders - INFO - Loading portfolio_carteras data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:43:03,255 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_carteras\n",
      "2025-04-02 19:43:03,255 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-04-02 19:43:03,271 - utils.dataloaders - INFO - Converting column 'portfolio_id' to string.\n",
      "2025-04-02 19:43:03,272 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:43:03,272 - utils.dataloaders - INFO - Loading portfolio_benchmarks data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:43:15,211 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_benchmarks\n",
      "2025-04-02 19:43:15,223 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-04-02 19:43:15,229 - utils.dataloaders - INFO - Converting column 'benchmark_id' to string.\n",
      "2025-04-02 19:43:15,229 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:43:15,229 - utils.dataloaders - INFO - Loading crossreference data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n",
      "2025-04-02 19:43:15,528 - utils.dataloaders - INFO - Cleaning columns and renaming crossreference data\n",
      "2025-04-02 19:43:15,528 - utils.dataloaders - INFO - Successfully loaded crossreference from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.2.  aladdin /brs data / perimetros\n",
    "brs_carteras = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_carteras\")    \n",
    "brs_benchmarks = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_benchmarks\")\n",
    "crosreference = load_crossreference(CROSSREFERENCE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:15,546 - pre-ovr-analysis - INFO - Adding aladdin_id to clarity dfs\n"
     ]
    }
   ],
   "source": [
    "# add aladdin_id to df_1 and df_2\n",
    "logger.info(\"Adding aladdin_id to clarity dfs\")\n",
    "df_1 = df_1.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")\n",
    "df_2 = df_2.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_carteras_issuerlevel = get_issuer_level_df(brs_carteras, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_benchmarks_issuerlevel = get_issuer_level_df(brs_benchmarks, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading overrides columns ['clarityid', 'permid', 'brs_id', 'ovr_target', 'ovr_value']\n",
      "2025-04-02 19:43:15,800 - utils.dataloaders - INFO - Loading overrides from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\overrides\\overrides_db.xlsx\n"
     ]
    }
   ],
   "source": [
    "# sri/ESG Team data\n",
    "overrides = load_overrides(OVR_PATH)\n",
    "# rename column brs_id to aladdin_id\n",
    "overrides.rename(columns={\"brs_id\": \"aladdin_id\"}, inplace=True)\n",
    "# rename value column \"ovr_target\" using rename_dict if value is string\n",
    "overrides[\"ovr_target\"] = overrides[\"ovr_target\"].apply(\n",
    "    lambda x: pd.NA if isinstance(x, str) and x.strip().lower() in [\"na\", \"nan\"]\n",
    "    else rename_dict[x] if isinstance(x, str) and x in rename_dict\n",
    "    else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_dict = create_override_dict(overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:16,710 - utils.dataloaders - INFO - Loading portfolios from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:43:35,457 - utils.dataloaders - INFO - Loading benchmarks from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:43:48,511 - utils.dataloaders - INFO - Loading strategy data for portfolios from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\portfolios_committees\\portfolio_lists.xlsx\n",
      "2025-04-02 19:43:48,544 - utils.dataloaders - INFO - Loading strategy data for benchmarks from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\portfolios_committees\\portfolio_lists.xlsx\n",
      "2025-04-02 19:43:48,560 - utils.dataloaders - INFO - Benchmark ID 'FIFSANEUR' appears in multiple strategies\n",
      "2025-04-02 19:43:48,560 - utils.dataloaders - INFO - Benchmark ID 'MSNTCWEUR' appears in multiple strategies\n",
      "2025-04-02 19:43:48,560 - utils.dataloaders - INFO - Benchmark ID 'MSACWLDNET' appears in multiple strategies\n",
      "2025-04-02 19:43:48,560 - utils.dataloaders - INFO - Benchmark ID 'SNP500NR2' appears in multiple strategies\n",
      "2025-04-02 19:43:48,560 - utils.dataloaders - INFO - Benchmark ID 'MSCIEURNT' appears in multiple strategies\n",
      "2025-04-02 19:43:48,560 - utils.dataloaders - INFO - Benchmark ID 'IBEX35NR' appears in multiple strategies\n",
      "2025-04-02 19:43:48,560 - utils.dataloaders - INFO - Benchmark ID 'MLG5E0EUR' appears in multiple strategies\n",
      "2025-04-02 19:43:48,560 - utils.dataloaders - INFO - Benchmark ID 'MLEAASEUR' appears in multiple strategies\n",
      "2025-04-02 19:43:48,576 - utils.dataloaders - INFO - Benchmark ID 'STXEZ50' appears in multiple strategies\n",
      "2025-04-02 19:43:48,660 - utils.dataloaders - INFO - Filtering strategy entries\n",
      "2025-04-02 19:43:48,660 - utils.dataloaders - INFO - Dictonary cleaned of empty strategies\n",
      "2025-04-02 19:43:48,660 - utils.dataloaders - INFO - Filtering strategy entries\n",
      "2025-04-02 19:43:48,660 - utils.dataloaders - INFO - Dictonary cleaned of empty strategies\n"
     ]
    }
   ],
   "source": [
    "# Load portfolios & benchmarks dicts\n",
    "(\n",
    "    portfolio_dict,\n",
    "    benchmark_dict,\n",
    ") = load_portfolios(path_pb=BMK_PORTF_STR_PATH, path_committe=COMMITTEE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START PRE-OVR ANALISIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PREP DATA FOR ANALYSIS\n",
    "# make sure that the values of of the columns delta_test_cols are strings and all uppercase and strip\n",
    "for col in delta_test_cols:\n",
    "    df_1[col] = df_1[col].str.upper().str.strip()\n",
    "    df_2[col] = df_2[col].str.upper().str.strip()\n",
    "    brs_carteras_issuerlevel[col] = brs_carteras_issuerlevel[col].str.upper().str.strip()\n",
    "    brs_benchmarks_issuerlevel[col] = brs_benchmarks_issuerlevel[col].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:49,096 - pre-ovr-analysis - INFO - Setting index to permid.\n",
      "2025-04-02 19:43:49,316 - pre-ovr-analysis - INFO - Number of common indexes: 69222\n",
      "2025-04-02 19:43:49,445 - pre-ovr-analysis - INFO - Number of new issuers: 106\n",
      "2025-04-02 19:43:49,445 - pre-ovr-analysis - INFO - Number of missing issuers: 56\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA CLARITY LEVEL\n",
    "(\n",
    "    df_1, \n",
    "    df_2,\n",
    "    new_issuers_clarity,\n",
    "    out_issuer_clarity,\n",
    ") = prepare_dataframes(df_1, df_2)\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number of new issuers: {new_issuers_clarity.shape[0]}\")\n",
    "logger.info(f\"Number of missing issuers: {out_issuer_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permid</th>\n",
       "      <th>issuer_name</th>\n",
       "      <th>str_001_s</th>\n",
       "      <th>str_002_ec</th>\n",
       "      <th>str_003_ec</th>\n",
       "      <th>str_004_asec</th>\n",
       "      <th>str_005_ec</th>\n",
       "      <th>scs_001_sec</th>\n",
       "      <th>scs_002_ec</th>\n",
       "      <th>str_006_sec</th>\n",
       "      <th>str_sfdr8_aec</th>\n",
       "      <th>str_003b_ec</th>\n",
       "      <th>aladdin_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4295857675</td>\n",
       "      <td>Excelsior Capital Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>D73574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4295858270</td>\n",
       "      <td>Poseidon Nickel Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>D70698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4295858807</td>\n",
       "      <td>Silver Lake Deflector Pty Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4295859680</td>\n",
       "      <td>Azevedo &amp; Travassos SA</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>F80754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4295860216</td>\n",
       "      <td>Dimed SA Distribuidora de Medicamentos</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>F85102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       permid                             issuer_name str_001_s str_002_ec  \\\n",
       "0  4295857675                   Excelsior Capital Ltd        OK         OK   \n",
       "1  4295858270                     Poseidon Nickel Ltd        OK         OK   \n",
       "2  4295858807           Silver Lake Deflector Pty Ltd        OK         OK   \n",
       "3  4295859680                  Azevedo & Travassos SA        OK         OK   \n",
       "4  4295860216  Dimed SA Distribuidora de Medicamentos        OK         OK   \n",
       "\n",
       "  str_003_ec str_004_asec str_005_ec scs_001_sec scs_002_ec str_006_sec  \\\n",
       "0         OK     EXCLUDED         OK    EXCLUDED         OK    EXCLUDED   \n",
       "1         OK     EXCLUDED         OK    EXCLUDED         OK    EXCLUDED   \n",
       "2         OK     EXCLUDED         OK    EXCLUDED         OK          OK   \n",
       "3         OK     EXCLUDED         OK          OK         OK          OK   \n",
       "4         OK           OK         OK          OK         OK          OK   \n",
       "\n",
       "  str_sfdr8_aec str_003b_ec aladdin_id  \n",
       "0            OK          OK     D73574  \n",
       "1            OK          OK     D70698  \n",
       "2            OK          OK        NaN  \n",
       "3            OK          OK     F80754  \n",
       "4            OK          OK     F85102  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset index for new_issuers_clarity and out_issuer_clarity\n",
    "new_issuers_clarity.reset_index(inplace=True)\n",
    "out_issuer_clarity.reset_index(inplace=True)\n",
    "\n",
    "# drop isin from out_issuer_clarity and new_issuers_clarity\n",
    "out_issuer_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "new_issuers_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "\n",
    "# remember to remove empyt empyt aladin id\n",
    "new_issuers_clarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:49,493 - pre-ovr-analysis - INFO - Setting index to aladdin_id.\n",
      "2025-04-02 19:43:49,763 - pre-ovr-analysis - INFO - Number of common indexes: 2552\n",
      "2025-04-02 19:43:49,878 - pre-ovr-analysis - INFO - Number issuers in clarity but not Aladdin: 66670\n",
      "2025-04-02 19:43:49,880 - pre-ovr-analysis - INFO - Number issuers in Aladdin but not Clarity: 1303\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA CARTERAS BRS LEVEL\n",
    "(\n",
    "    brs_df, \n",
    "    clarity_df,\n",
    "    in_clarity_but_not_in_brs,\n",
    "    in_brs_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_carteras_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not Aladdin: {in_clarity_but_not_in_brs.shape[0]}\")\n",
    "logger.info(f\"Number issuers in Aladdin but not Clarity: {in_brs_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:49,895 - pre-ovr-analysis - INFO - Setting index to aladdin_id.\n",
      "2025-04-02 19:43:50,051 - pre-ovr-analysis - INFO - Number of common indexes: 2552\n",
      "2025-04-02 19:43:50,181 - pre-ovr-analysis - INFO - Number issuers in clarity but not benchmarks: 66670\n",
      "2025-04-02 19:43:50,183 - pre-ovr-analysis - INFO - Number issuers in benchmarks but not Clarity: 1303\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA BENCHMARK BRS LEVEL\n",
    "(\n",
    "    brs_df_benchmarks, \n",
    "    clarity_df_benchmarks,\n",
    "    in_clarity_but_not_in_brs_benchmarks,\n",
    "    in_brs_benchmark_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_benchmarks_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not benchmarks: {in_clarity_but_not_in_brs_benchmarks.shape[0]}\")\n",
    "logger.info(f\"Number issuers in benchmarks but not Clarity: {in_brs_benchmark_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:50,196 - pre-ovr-analysis - INFO - comparing clarity dataframes\n",
      "2025-04-02 19:43:50,279 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-04-02 19:43:50,310 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-04-02 19:43:50,336 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-04-02 19:43:50,343 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-04-02 19:43:50,360 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-04-02 19:43:50,395 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-04-02 19:43:50,410 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-04-02 19:43:50,436 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-04-02 19:43:50,447 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-04-02 19:43:50,461 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-04-02 19:43:50,482 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-04-02 19:43:50,492 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 498\n",
      "2025-04-02 19:43:50,492 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-04-02 19:43:50,509 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 262\n",
      "2025-04-02 19:43:50,509 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-04-02 19:43:50,531 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 211\n",
      "2025-04-02 19:43:50,533 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-04-02 19:43:50,545 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 165\n",
      "2025-04-02 19:43:50,545 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-04-02 19:43:50,561 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 838\n",
      "2025-04-02 19:43:50,561 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-04-02 19:43:50,578 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 206\n",
      "2025-04-02 19:43:50,578 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-04-02 19:43:50,593 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 546\n",
      "2025-04-02 19:43:50,593 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-04-02 19:43:50,609 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 50\n",
      "2025-04-02 19:43:50,609 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-04-02 19:43:50,627 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 610\n",
      "2025-04-02 19:43:50,627 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-04-02 19:43:50,642 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 444\n",
      "2025-04-02 19:43:51,725 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-04-02 19:43:51,743 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 485\n",
      "2025-04-02 19:43:51,743 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-04-02 19:43:51,757 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 604\n",
      "2025-04-02 19:43:51,757 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-04-02 19:43:51,776 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 244\n",
      "2025-04-02 19:43:51,776 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-04-02 19:43:51,793 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 151\n",
      "2025-04-02 19:43:51,793 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-04-02 19:43:51,807 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 686\n",
      "2025-04-02 19:43:51,822 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-04-02 19:43:51,837 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 1248\n",
      "2025-04-02 19:43:51,842 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-04-02 19:43:51,858 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 788\n",
      "2025-04-02 19:43:51,858 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-04-02 19:43:51,875 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 136\n",
      "2025-04-02 19:43:51,875 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-04-02 19:43:51,908 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 685\n",
      "2025-04-02 19:43:51,908 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-04-02 19:43:51,928 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 296\n",
      "2025-04-02 19:43:57,343 - pre-ovr-analysis - INFO - Final delta shape: (4184, 18)\n",
      "2025-04-02 19:43:57,349 - pre-ovr-analysis - INFO - checking impact compared to BRS portfolio data\n",
      "2025-04-02 19:43:57,349 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-04-02 19:43:57,349 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-04-02 19:43:57,349 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-04-02 19:43:57,349 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-04-02 19:43:57,364 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-04-02 19:43:57,364 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-04-02 19:43:57,371 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-04-02 19:43:57,371 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-04-02 19:43:57,371 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-04-02 19:43:57,371 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-04-02 19:43:57,380 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-04-02 19:43:57,380 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 67\n",
      "2025-04-02 19:43:57,380 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-04-02 19:43:57,380 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 73\n",
      "2025-04-02 19:43:57,380 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-04-02 19:43:57,380 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 61\n",
      "2025-04-02 19:43:57,380 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-04-02 19:43:57,396 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 54\n",
      "2025-04-02 19:43:57,396 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-04-02 19:43:57,396 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 107\n",
      "2025-04-02 19:43:57,396 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-04-02 19:43:57,396 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 48\n",
      "2025-04-02 19:43:57,396 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-04-02 19:43:57,396 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 84\n",
      "2025-04-02 19:43:57,396 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-04-02 19:43:57,411 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 8\n",
      "2025-04-02 19:43:57,411 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-04-02 19:43:57,411 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 46\n",
      "2025-04-02 19:43:57,411 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-04-02 19:43:57,411 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 83\n",
      "2025-04-02 19:43:57,453 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-04-02 19:43:57,453 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 12\n",
      "2025-04-02 19:43:57,453 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-04-02 19:43:57,453 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 65\n",
      "2025-04-02 19:43:57,453 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-04-02 19:43:57,453 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 24\n",
      "2025-04-02 19:43:57,453 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-04-02 19:43:57,469 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 4\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 15\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 62\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 24\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 13\n",
      "2025-04-02 19:43:57,470 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-04-02 19:43:57,487 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 21\n",
      "2025-04-02 19:43:57,487 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-04-02 19:43:57,487 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n740789\\AppData\\Local\\Temp\\ipykernel_33356\\4223178409.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  delta[target_index] = delta[target_index].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:57,637 - pre-ovr-analysis - INFO - Final delta shape: (347, 17)\n",
      "2025-04-02 19:43:57,637 - pre-ovr-analysis - INFO - checking impact compared to BRS benchmarks data\n",
      "2025-04-02 19:43:57,643 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-04-02 19:43:57,643 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-04-02 19:43:57,643 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-04-02 19:43:57,643 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-04-02 19:43:57,652 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-04-02 19:43:57,655 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-04-02 19:43:57,655 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-04-02 19:43:57,655 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-04-02 19:43:57,655 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-04-02 19:43:57,655 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-04-02 19:43:57,655 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-04-02 19:43:57,655 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 67\n",
      "2025-04-02 19:43:57,655 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-04-02 19:43:57,671 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 73\n",
      "2025-04-02 19:43:57,671 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-04-02 19:43:57,671 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 61\n",
      "2025-04-02 19:43:57,671 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-04-02 19:43:57,671 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 54\n",
      "2025-04-02 19:43:57,671 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-04-02 19:43:57,671 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 107\n",
      "2025-04-02 19:43:57,671 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 48\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 84\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 8\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 46\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-04-02 19:43:57,687 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 83\n",
      "2025-04-02 19:43:57,735 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-04-02 19:43:57,736 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 12\n",
      "2025-04-02 19:43:57,736 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-04-02 19:43:57,736 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 65\n",
      "2025-04-02 19:43:57,743 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-04-02 19:43:57,743 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 24\n",
      "2025-04-02 19:43:57,743 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-04-02 19:43:57,749 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 4\n",
      "2025-04-02 19:43:57,749 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-04-02 19:43:57,749 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 15\n",
      "2025-04-02 19:43:57,749 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-04-02 19:43:57,749 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 62\n",
      "2025-04-02 19:43:57,749 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-04-02 19:43:57,749 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 24\n",
      "2025-04-02 19:43:57,749 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-04-02 19:43:57,765 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 13\n",
      "2025-04-02 19:43:57,765 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-04-02 19:43:57,765 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 21\n",
      "2025-04-02 19:43:57,765 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-04-02 19:43:57,765 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n740789\\AppData\\Local\\Temp\\ipykernel_33356\\4223178409.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  delta[target_index] = delta[target_index].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:57,920 - pre-ovr-analysis - INFO - Final delta shape: (347, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n740789\\AppData\\Local\\Temp\\ipykernel_33356\\4223178409.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  delta[target_index] = delta[target_index].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# COMPARE DATA\n",
    "logger.info(\"comparing clarity dataframes\")\n",
    "delta_clarity = compare_dataframes(df_1, df_2)\n",
    "delta_clarity = check_new_exclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = check_new_inclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = finalize_delta(delta_clarity)\n",
    "logger.info(\"checking impact compared to BRS portfolio data\")\n",
    "delta_brs = compare_dataframes(brs_df, clarity_df)\n",
    "delta_brs = check_new_exclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = check_new_inclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = finalize_delta(delta_brs, target_index=\"aladdin_id\")\n",
    "logger.info(\"checking impact compared to BRS benchmarks data\")\n",
    "delta_benchmarks = compare_dataframes(brs_df_benchmarks, clarity_df_benchmarks)\n",
    "delta_benchmarks = check_new_exclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = check_new_inclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = finalize_delta(delta_benchmarks, target_index=\"aladdin_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:43:57,925 - utils.dataloaders - INFO - Loading Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n",
      "2025-04-02 19:43:58,420 - utils.dataloaders - INFO - Successfully loaded Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n",
      "2025-04-02 19:43:58,429 - utils.dataloaders - INFO - Loading portfolio_carteras data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:44:14,655 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_carteras\n",
      "2025-04-02 19:44:14,655 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-04-02 19:44:14,659 - utils.dataloaders - INFO - Converting column 'portfolio_id' to string.\n",
      "2025-04-02 19:44:14,659 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:44:14,659 - utils.dataloaders - INFO - Loading portfolio_benchmarks data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:44:28,259 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_benchmarks\n",
      "2025-04-02 19:44:28,259 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-04-02 19:44:28,275 - utils.dataloaders - INFO - Converting column 'benchmark_id' to string.\n",
      "2025-04-02 19:44:28,275 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-04-02 19:44:28,275 - utils.dataloaders - INFO - Loading crossreference data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n",
      "2025-04-02 19:44:28,523 - utils.dataloaders - INFO - Cleaning columns and renaming crossreference data\n",
      "2025-04-02 19:44:28,525 - utils.dataloaders - INFO - Successfully loaded crossreference from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n"
     ]
    }
   ],
   "source": [
    "#from utils.zombie_killer import main as zombie_killer\n",
    "#logger.info(\"Getting zombie analysis df\")\n",
    "zombie_df = zombie_killer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:44:32,681 - pre-ovr-analysis - INFO - Preparing deltas before saving\n"
     ]
    }
   ],
   "source": [
    "# PREP DELTAS BEFORE SAVING\n",
    "logger.info(\"Preparing deltas before saving\")\n",
    "# use crossreference to add permid to delta_brs\n",
    "delta_brs = delta_brs.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "delta_benchmarks = delta_benchmarks.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "# drop isin from deltas\n",
    "delta_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_brs.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_benchmarks.drop(columns=[\"isin\"], inplace=True)\n",
    "# add new column to delta_brs with ovr_dict value using aladdin_id\n",
    "delta_brs[\"ovr_list\"] = delta_brs[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_clarity[\"ovr_list\"] = delta_clarity[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_benchmarks[\"ovr_list\"] = delta_benchmarks[\"aladdin_id\"].map(ovr_dict)\n",
    "# let's add portfolio info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_clarity)\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_brs)\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_benchmarks)\n",
    "# let's add benchmark info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_clarity, \"affected_benchmark_str\")\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_brs, \"affected_benchmark_str\")\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_benchmarks, \"affected_benchmark_str\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:44:32,864 - pre-ovr-analysis - INFO - Filtering rows with common elements in columns: exclusion_list_brs and affected_portfolio_str\n",
      "2025-04-02 19:44:32,876 - pre-ovr-analysis - INFO - Filtering rows with common elements in columns: exclusion_list_brs and affected_portfolio_str\n"
     ]
    }
   ],
   "source": [
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_brs = filter_non_empty_lists(delta_brs, \"affected_portfolio_str\")\n",
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_benchmarks = filter_non_empty_lists(delta_benchmarks, \"affected_portfolio_str\")\n",
    "\n",
    "# ADD TEST FOR INCLUSIONS\n",
    "dlt_inc_brs = delta_brs.copy()\n",
    "dlt_inc_benchmarks = delta_benchmarks.copy()\n",
    "\n",
    "\n",
    "# pass filter_rows_with_common_elements for columns exclusion_list_brs and affected_portfolio_str\n",
    "delta_brs = filter_rows_with_common_elements(delta_brs, \"exclusion_list_brs\", \"affected_portfolio_str\")\n",
    "delta_benchmarks = filter_rows_with_common_elements(delta_benchmarks, \"exclusion_list_brs\", \"affected_portfolio_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows from delta_brs where len of the list of in the column affected_benchmark_str is bigger than zero\n",
    "#delta_brs[delta_brs[\"affected_benchmark_str\"].apply(lambda x: len(x) > 0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reset df1 index to permid\n",
    "df_1.reset_index(inplace=True)\n",
    "df_1[\"permid\"] = df_1[\"permid\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. GET STRATEGIES DFS\n",
    "str_dfs_dict = {}\n",
    "\n",
    "# Iterate over strategies to build DataFrames\n",
    "for strategy in delta_test_cols:\n",
    "    rows = []\n",
    "\n",
    "    for _, row in delta_brs.iterrows():\n",
    "        if strategy in row[\"exclusion_list_brs\"]:\n",
    "            rows.append({\n",
    "                \"aladdin_id\": row[\"aladdin_id\"],\n",
    "                \"permid\": row[\"permid\"],\n",
    "                \"issuer_name\": row[\"issuer_name\"],\n",
    "                strategy: row[strategy],\n",
    "                \"affected_portfolio_str\": row[\"affected_portfolio_str\"]\n",
    "            })\n",
    "\n",
    "    str_dfs_dict[strategy] = pd.DataFrame(rows)\n",
    "\n",
    "# Prepare lookups for efficient mapping\n",
    "permid_to_df1 = df_1.set_index(\"permid\")\n",
    "aladdin_to_brs = brs_carteras_issuerlevel.set_index(\"aladdin_id\")\n",
    "\n",
    "for strategy_name, df in str_dfs_dict.items():\n",
    "    # Initialize additional columns\n",
    "    df[f\"{strategy_name}_old\"] = None\n",
    "    df[f\"{strategy_name}_brs\"] = None\n",
    "    df[f\"{strategy_name}_ovr\"] = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        permid = row[\"permid\"]\n",
    "        aladdin_id = row[\"aladdin_id\"]\n",
    "\n",
    "        # Lookup from df_1\n",
    "        if permid in permid_to_df1.index:\n",
    "            df.at[i, f\"{strategy_name}_old\"] = permid_to_df1.at[permid, strategy_name]\n",
    "\n",
    "        # Lookup from brs_carteras_issuerlevel\n",
    "        if aladdin_id in aladdin_to_brs.index:\n",
    "            df.at[i, f\"{strategy_name}_brs\"] = aladdin_to_brs.at[aladdin_id, strategy_name]\n",
    "\n",
    "        # Lookup from overrides\n",
    "        match = overrides.loc[\n",
    "            (overrides[\"permid\"] == permid) & (overrides[\"ovr_target\"] == strategy_name),\n",
    "            \"ovr_value\"\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            df.at[i, f\"{strategy_name}_ovr\"] = match.values[0]\n",
    "\n",
    "    # Move \"affected_portfolio_str\" to the end\n",
    "    cols = [col for col in df.columns if col != \"affected_portfolio_str\"] + [\"affected_portfolio_str\"]\n",
    "    df = df[cols]\n",
    "    str_dfs_dict[strategy_name] = df\n",
    "\n",
    "# Reset index and ensure permid is a string for df_1\n",
    "df_1.reset_index(drop=True, inplace=True)\n",
    "df_1[\"permid\"] = df_1[\"permid\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set id_name_issuers_cols first and exclude delta_test_cols\n",
    "delta_brs = reorder_columns(delta_brs, id_name_issuers_cols, delta_test_cols)\n",
    "delta_clarity = reorder_columns(delta_clarity, id_name_issuers_cols, delta_test_cols)\n",
    "delta_benchmarks = reorder_columns(delta_benchmarks, id_name_issuers_cols, delta_test_cols)\n",
    "# set id_name_issuers_cols first\n",
    "new_issuers_clarity = reorder_columns(new_issuers_clarity, id_name_issuers_cols, id_name_cols)\n",
    "out_issuer_clarity = reorder_columns(out_issuer_clarity, id_name_issuers_cols, id_name_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TEST FOR INCLUSIONS\n",
    "dlt_inc_brs = reorder_columns(dlt_inc_brs, id_name_issuers_cols, delta_test_cols)\n",
    "dlt_inc_benchmarks = reorder_columns(dlt_inc_benchmarks, id_name_issuers_cols, delta_test_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_inclusion_list(df):\n",
    "    \"\"\"\n",
    "    Processes each row of df:\n",
    "    1. For each element in 'inclusion_list_brs', if the element is a key in 'ovr_list' and\n",
    "       its value is 'EXCLUDED', remove the element.\n",
    "    2. Rows where 'inclusion_list_brs' is empty (or becomes empty after filtering) are dropped.\n",
    "    \"\"\"\n",
    "    def process_row(row):\n",
    "        inc_list = row.get('inclusion_list_brs', [])\n",
    "        ovr_list = row.get('ovr_list', {})\n",
    "\n",
    "        # If inc_list is a NumPy array, convert it to a list.\n",
    "        if isinstance(inc_list, np.ndarray):\n",
    "            inc_list = inc_list.tolist()\n",
    "\n",
    "        # If inc_list is not a list (or array), then treat it as empty.\n",
    "        if not isinstance(inc_list, list):\n",
    "            return []\n",
    "\n",
    "        # For ovr_list: if it's not a dict, then treat it as an empty dict.\n",
    "        if not isinstance(ovr_list, dict):\n",
    "            ovr_list = {}\n",
    "\n",
    "        # Filter out items that are in ovr_list and marked as 'EXCLUDED'\n",
    "        return [item for item in inc_list if item not in ovr_list or ovr_list[item] != 'EXCLUDED']\n",
    "\n",
    "    # Apply the row-wise processing.\n",
    "    df.loc[:,'inclusion_list_brs'] = df.apply(process_row, axis=1)\n",
    "\n",
    "    # Drop rows where 'inclusion_list_brs' is empty.\n",
    "    df = df[df['inclusion_list_brs'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt_inc_brs = clean_inclusion_list(dlt_inc_brs)\n",
    "dlt_inc_benchmarks = clean_inclusion_list(dlt_inc_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_matching_rows(df):\n",
    "    # Identify columns with specific suffixes\n",
    "    cols_old = [col for col in df.columns if col.endswith('_old')]\n",
    "    cols_brs = [col for col in df.columns if col.endswith('_brs')]\n",
    "    cols_ovr = [col for col in df.columns if col.endswith('_ovr')]\n",
    "\n",
    "    # Assuming there's only one set of each column type based on your example\n",
    "    if len(cols_old) != 1 or len(cols_brs) != 1 or len(cols_ovr) != 1:\n",
    "        raise ValueError(\"Expected exactly one column each for '_old', '_brs', '_ovr'\")\n",
    "\n",
    "    col_old, col_brs, col_ovr = cols_old[0], cols_brs[0], cols_ovr[0]\n",
    "\n",
    "    # Filter rows where all three column values match\n",
    "    df_filtered = df[\n",
    "        ~(df[col_old] == df[col_brs]) | ~(df[col_old] == df[col_ovr])\n",
    "    ].copy()\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in str_dfs_dict.items():\n",
    "    str_dfs_dict[df_name] = remove_matching_rows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# define a function to save results in an Excel file\n",
    "def save_excel(df_dict: dict, output_dir: Path, file_name: str) -> Path:\n",
    "    \"\"\"\n",
    "    Writes multiple DataFrames to an Excel file with each DataFrame in a separate sheet.\n",
    "\n",
    "    Parameters:\n",
    "    - df_dict (dict): A dictionary where keys are sheet names and values are DataFrames.\n",
    "    - output_dir (Path): The directory where the Excel file will be saved.\n",
    "    - file_name (str): The base name for the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    - Path: The full path to the saved Excel file.\n",
    "    \"\"\"\n",
    "    # Create a date string in \"YYYYMMDD\" format\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    logger.info(\"Creating output directory: %s\", output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Construct the full output file path (e.g., file_name_YYYYMMDD.xlsx)\n",
    "    output_file = output_dir / f\"{date_str}_{file_name}.xlsx\"\n",
    "\n",
    "    # Write each DataFrame to its own sheet with index set to False\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        logger.info(\"Writing DataFrames to Excel file: %s\", output_file)\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            logger.info(\"Writing sheet: %s\", sheet_name)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    logger.info(\"Results saved to Excel file: %s\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to remove from delta_brs, delta_benchmarks before saving\n",
    "exclusion_cols_to_remove = [\"new_inclusion\", \"inclusion_list_brs\"]\n",
    "# columns to remove from dlt_inc_brs and dlt_inc_benchmarks before saving\n",
    "inclusion_cols_to_remove = [\"new_exclusion\", \"exclusion_list_brs\"]\n",
    "\n",
    "# Filter and clean exclusion data\n",
    "delta_brs = delta_brs.drop(columns=exclusion_cols_to_remove)\n",
    "delta_brs = delta_brs[delta_brs[\"new_exclusion\"] == True]\n",
    "\n",
    "delta_benchmarks = delta_benchmarks.drop(columns=exclusion_cols_to_remove)\n",
    "delta_benchmarks = delta_benchmarks[delta_benchmarks[\"new_exclusion\"] == True]\n",
    "\n",
    "# Filter and clean inclusion data\n",
    "dlt_inc_brs = dlt_inc_brs.drop(columns=inclusion_cols_to_remove)\n",
    "dlt_inc_brs = dlt_inc_brs[dlt_inc_brs[\"new_inclusion\"] == True]\n",
    "\n",
    "dlt_inc_benchmarks = dlt_inc_benchmarks.drop(columns=inclusion_cols_to_remove)\n",
    "dlt_inc_benchmarks = dlt_inc_benchmarks[dlt_inc_benchmarks[\"new_inclusion\"] == True]\n",
    "\n",
    "# Clean delta_clarity\n",
    "delta_clarity.drop(columns=[\"new_inclusion\", \"inclusion_list\"], inplace=True)\n",
    "delta_clarity = delta_clarity[delta_clarity[\"new_exclusion\"] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup: drop 'new_exclusion' and 'new_inclusion' if present\n",
    "for df in [delta_brs, delta_benchmarks, dlt_inc_brs, dlt_inc_benchmarks, delta_clarity]:\n",
    "    for col in [\"new_exclusion\", \"new_inclusion\"]:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=col, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of df and df name\n",
    "dfs_dict = {\n",
    "    \"zombie_analysis\": zombie_df,\n",
    "    \"delta_carteras\": delta_brs,\n",
    "    \"delta_benchmarks\": delta_benchmarks,\n",
    "    \"delta_clarity\": delta_clarity,\n",
    "    \"incl_carteras\": dlt_inc_brs,\n",
    "    \"incl_benchmarks\": dlt_inc_benchmarks,\n",
    "    \"new_issuers_clarity\": new_issuers_clarity,\n",
    "    \"out_issuer_clarity\": out_issuer_clarity,\n",
    "}\n",
    "\n",
    "# add to dfs_dict the str_dfs_dict\n",
    "dfs_dict.update(str_dfs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel\n",
    "#save_excel(str_dfs_dict, OUTPUT_DIR, file_name=\"pre_ovr_simple_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:47:35,365 - pre-ovr-analysis - INFO - Creating output directory: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\n",
      "2025-04-02 19:47:35,451 - pre-ovr-analysis - INFO - Writing DataFrames to Excel file: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\\20250402_pre_ovr_analysis.xlsx\n",
      "2025-04-02 19:47:35,453 - pre-ovr-analysis - INFO - Writing sheet: zombie_analysis\n",
      "2025-04-02 19:47:35,470 - pre-ovr-analysis - INFO - Writing sheet: delta_carteras\n",
      "2025-04-02 19:47:35,475 - pre-ovr-analysis - INFO - Writing sheet: delta_benchmarks\n",
      "2025-04-02 19:47:35,480 - pre-ovr-analysis - INFO - Writing sheet: delta_clarity\n",
      "2025-04-02 19:47:35,550 - pre-ovr-analysis - INFO - Writing sheet: incl_carteras\n",
      "2025-04-02 19:47:35,555 - pre-ovr-analysis - INFO - Writing sheet: incl_benchmarks\n",
      "2025-04-02 19:47:35,558 - pre-ovr-analysis - INFO - Writing sheet: new_issuers_clarity\n",
      "2025-04-02 19:47:35,570 - pre-ovr-analysis - INFO - Writing sheet: out_issuer_clarity\n",
      "2025-04-02 19:47:35,577 - pre-ovr-analysis - INFO - Writing sheet: str_001_s\n",
      "2025-04-02 19:47:35,578 - pre-ovr-analysis - INFO - Writing sheet: str_002_ec\n",
      "2025-04-02 19:47:35,578 - pre-ovr-analysis - INFO - Writing sheet: str_003_ec\n",
      "2025-04-02 19:47:35,584 - pre-ovr-analysis - INFO - Writing sheet: str_003b_ec\n",
      "2025-04-02 19:47:35,588 - pre-ovr-analysis - INFO - Writing sheet: str_004_asec\n",
      "2025-04-02 19:47:35,591 - pre-ovr-analysis - INFO - Writing sheet: str_005_ec\n",
      "2025-04-02 19:47:35,595 - pre-ovr-analysis - INFO - Writing sheet: str_006_sec\n",
      "2025-04-02 19:47:35,600 - pre-ovr-analysis - INFO - Writing sheet: str_sfdr8_aec\n",
      "2025-04-02 19:47:35,606 - pre-ovr-analysis - INFO - Writing sheet: scs_001_sec\n",
      "2025-04-02 19:47:35,609 - pre-ovr-analysis - INFO - Writing sheet: scs_002_ec\n",
      "2025-04-02 19:47:35,743 - pre-ovr-analysis - INFO - Results saved to Excel file: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\\20250402_pre_ovr_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "# save to excel\n",
    "save_excel(dfs_dict, OUTPUT_DIR, file_name=\"pre_ovr_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aladdin_id', 'permid', 'issuer_name', 'new_exclusion',\n",
       "       'exclusion_list_brs', 'ovr_list', 'affected_portfolio_str',\n",
       "       'affected_benchmark_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_benchmarks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aladdin_id</th>\n",
       "      <th>permid</th>\n",
       "      <th>issuer_name</th>\n",
       "      <th>exclusion_list_brs</th>\n",
       "      <th>ovr_list</th>\n",
       "      <th>affected_portfolio_str</th>\n",
       "      <th>affected_benchmark_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E97445</td>\n",
       "      <td>5000757618</td>\n",
       "      <td>ABN Amro Bank NV</td>\n",
       "      <td>[str_004_asec]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[FIG00677, str_004_asec, FIG01998, str_sfdr8_a...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R63005</td>\n",
       "      <td>4295884955</td>\n",
       "      <td>Airbus SE</td>\n",
       "      <td>[str_001_s, str_004_asec]</td>\n",
       "      <td>{'str_001_s': 'OK', 'str_004_asec': 'OK', 'cs_...</td>\n",
       "      <td>[ADM0PSA1, scs_002_ec, ADM0PSA1, scs_002_ec, A...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>059456</td>\n",
       "      <td>4295889577</td>\n",
       "      <td>Banco Bilbao Vizcaya Argentaria SA</td>\n",
       "      <td>[str_001_s, str_002_ec, str_003_ec, str_003b_e...</td>\n",
       "      <td>{'str_001_s': 'OK', 'str_002_ec': 'OK', 'str_0...</td>\n",
       "      <td>[CPE00035, str_003b_ec, CPE00264, str_003b_ec,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05564X</td>\n",
       "      <td>8589934326</td>\n",
       "      <td>BNP Paribas SA</td>\n",
       "      <td>[str_001_s, str_002_ec, str_003_ec, str_003b_e...</td>\n",
       "      <td>{'str_001_s': 'OK', 'str_002_ec': 'OK', 'str_0...</td>\n",
       "      <td>[ADM0PSA1, scs_002_ec, CPE00264, str_003b_ec, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E81396</td>\n",
       "      <td>5000084509</td>\n",
       "      <td>Bpce SA</td>\n",
       "      <td>[str_001_s, str_002_ec, str_003_ec, str_003b_e...</td>\n",
       "      <td>{'str_001_s': 'OK', 'str_002_ec': 'OK', 'str_0...</td>\n",
       "      <td>[ADM0PSA2, scs_002_ec, CPE00448, str_003b_ec, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  aladdin_id      permid                         issuer_name  \\\n",
       "0     E97445  5000757618                    ABN Amro Bank NV   \n",
       "1     R63005  4295884955                           Airbus SE   \n",
       "2     059456  4295889577  Banco Bilbao Vizcaya Argentaria SA   \n",
       "3     05564X  8589934326                      BNP Paribas SA   \n",
       "4     E81396  5000084509                             Bpce SA   \n",
       "\n",
       "                                  exclusion_list_brs  \\\n",
       "0                                     [str_004_asec]   \n",
       "1                          [str_001_s, str_004_asec]   \n",
       "2  [str_001_s, str_002_ec, str_003_ec, str_003b_e...   \n",
       "3  [str_001_s, str_002_ec, str_003_ec, str_003b_e...   \n",
       "4  [str_001_s, str_002_ec, str_003_ec, str_003b_e...   \n",
       "\n",
       "                                            ovr_list  \\\n",
       "0                                                NaN   \n",
       "1  {'str_001_s': 'OK', 'str_004_asec': 'OK', 'cs_...   \n",
       "2  {'str_001_s': 'OK', 'str_002_ec': 'OK', 'str_0...   \n",
       "3  {'str_001_s': 'OK', 'str_002_ec': 'OK', 'str_0...   \n",
       "4  {'str_001_s': 'OK', 'str_002_ec': 'OK', 'str_0...   \n",
       "\n",
       "                              affected_portfolio_str affected_benchmark_str  \n",
       "0  [FIG00677, str_004_asec, FIG01998, str_sfdr8_a...                     []  \n",
       "1  [ADM0PSA1, scs_002_ec, ADM0PSA1, scs_002_ec, A...                     []  \n",
       "2  [CPE00035, str_003b_ec, CPE00264, str_003b_ec,...                     []  \n",
       "3  [ADM0PSA1, scs_002_ec, CPE00264, str_003b_ec, ...                     []  \n",
       "4  [ADM0PSA2, scs_002_ec, CPE00448, str_003b_ec, ...                     []  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_benchmarks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

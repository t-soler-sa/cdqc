{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:54:55,303 - utils.get_date - INFO - Date format is valid. Date set to 202504.\n",
      "Output directory for script zombie-killer is set to: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\zombie_list\n",
      "2025-03-28 16:54:58,533 - utils.get_date - INFO - Date format is valid. Date set to 202504.\n",
      "Output directory for script pre-ovr-analysis is set to: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.dataloaders import (\n",
    "    load_clarity_data,\n",
    "    load_aladdin_data,\n",
    "    load_crossreference,\n",
    "    load_portfolios,\n",
    "    load_overrides,\n",
    "    save_excel\n",
    ")\n",
    "from utils.zombie_killer import main as zombie_killer\n",
    "\n",
    "# Import the centralized configuration\n",
    "from config import get_config\n",
    "\n",
    "# Get the common configuration for the Pre-OVR-Analysis script.\n",
    "config = get_config(\"pre-ovr-analysis\", interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config[\"logger\"]\n",
    "DATE = config[\"DATE\"]\n",
    "YEAR = config[\"YEAR\"]\n",
    "DATE_PREV = config[\"DATE_PREV\"]\n",
    "REPO_DIR = config[\"REPO_DIR\"]\n",
    "DATAFEED_DIR = config[\"DATAFEED_DIR\"]\n",
    "SRI_DATA_DIR = config[\"SRI_DATA_DIR\"]\n",
    "paths = config[\"paths\"]\n",
    "\n",
    "# Use the paths from config\n",
    "df_1_path = paths[\"PRE_DF_WOVR_PATH\"]\n",
    "df_2_path = paths[\"CURRENT_DF_WOUTOVR_PATH\"]\n",
    "CROSSREFERENCE_PATH = paths[\"CROSSREFERENCE_PATH\"]\n",
    "BMK_PORTF_STR_PATH = paths[\"BMK_PORTF_STR_PATH\"]\n",
    "OVR_PATH = paths[\"OVR_PATH\"]\n",
    "COMMITTEE_PATH = paths[\"COMMITTEE_PATH\"]\n",
    "\n",
    "# Define the output directory and file based on the configuration.\n",
    "OUTPUT_DIR = config[\"OUTPUT_DIR\"]\n",
    "OUTPUT_FILE = OUTPUT_DIR / f\"{DATE}_pre_ovr_analysis.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore workbook warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202504 and 2025 and 202503.\n"
     ]
    }
   ],
   "source": [
    "# check that the date constants are set correctly\n",
    "print(f\"{DATE} and {YEAR} and {DATE_PREV}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TEST COLUMNS\n",
    "# let's define necessary column lists\n",
    "\n",
    "id_name_cols = [\"permid\", \"isin\", \"issuer_name\"]\n",
    "id_name_issuers_cols = [\"aladdin_id\", \"permid\", \"issuer_name\"]\n",
    "clarity_test_col = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"art_8_basicos\",\n",
    "    \"str_006_sec\",\n",
    "    \"cs_001_sec\",\n",
    "    \"cs_002_ec\",\n",
    "]\n",
    "columns_to_read = id_name_cols + clarity_test_col\n",
    "delta_test_cols = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"str_006_sec\",\n",
    "    \"str_sfdr8_aec\",\n",
    "    \"scs_001_sec\",\n",
    "    \"scs_002_ec\",\n",
    "]\n",
    "\n",
    "brs_test_cols = [\"aladdin_id\"] + delta_test_cols\n",
    "rename_dict = {\n",
    "    \"cs_001_sec\": \"scs_001_sec\",\n",
    "    \"cs_002_ec\": \"scs_002_ec\",\n",
    "    \"art_8_basicos\": \"str_sfdr8_aec\",\n",
    "}\n",
    "\n",
    "dfs_name_str_dict = {\n",
    "    \"str_001_s\" : \"str_001_s\",\n",
    "    \"str_002_ec\" : \"str_002_ec\",\n",
    "    \"str_003_ec\" : \"str_003_ec\",\n",
    "    \"str_003b_ec\" : \"str_003b_ec\",\n",
    "    \"str_004_asec\" : \"str_004_asec\",\n",
    "    \"str_005_ec\" : \"str_005_ec\",\n",
    "    \"str_006_sec\" : \"str_006_sec\",\n",
    "    \"str_sfdr8_aec\" : \"str_sfdr8_aec\",\n",
    "    \"scs_001_sec\" : \"scs_001_sec\",\n",
    "    \"scs_002_ec\" : \"scs_002_ec\",\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframes(\n",
    "    base_df: pd.DataFrame, new_df: pd.DataFrame, target_index:str = \"permid\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare DataFrames by setting the index and filtering for common indexes.\n",
    "    Logs info about common, new, and missing indexes.\n",
    "    \"\"\"\n",
    "    # Set index to 'permid' if it exists, otherwise assume it's already the index.\n",
    "    logger.info(f\"Setting index to {target_index}.\")\n",
    "    if target_index in base_df.columns:\n",
    "        base_df = base_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df1 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    if target_index in new_df.columns:\n",
    "        new_df = new_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df2 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    common_indexes = base_df.index.intersection(new_df.index)\n",
    "    new_indexes = new_df.index.difference(base_df.index)\n",
    "    missing_indexes = base_df.index.difference(new_df.index)\n",
    "\n",
    "    logger.info(f\"Number of common indexes: {len(common_indexes)}\")\n",
    "\n",
    "    return (\n",
    "        base_df.loc[common_indexes],\n",
    "        new_df.loc[common_indexes],\n",
    "        new_df.loc[new_indexes],\n",
    "        base_df.loc[missing_indexes],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(\n",
    "    df1: pd.DataFrame, df2: pd.DataFrame, test_col: List[str] = delta_test_cols\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare DataFrames and create a delta DataFrame.\"\"\"\n",
    "    delta = df2.copy()\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Comparing column: {col}\")\n",
    "            # Create a mask for differences between the two DataFrames\n",
    "            diff_mask = df1[col] != df2[col]\n",
    "            # Update the delta DataFrame with the differences\n",
    "            delta.loc[~diff_mask, col] = np.nan\n",
    "    return delta\n",
    "\n",
    "\n",
    "def get_exclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed to EXCLUDED.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] == \"EXCLUDED\" and df1.loc[row.name, col] != \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_inclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed from EXCLUDED to any other value.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] != \"EXCLUDED\" and df1.loc[row.name, col] == \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def check_new_exclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new exclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_exclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new exclusions in column: {col}\")\n",
    "            mask = (df1[col] != \"EXCLUDED\") & (df2[col] == \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_exclusion\"] = True\n",
    "            logger.info(f\"Number of new exclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"exclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_exclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "\n",
    "def check_new_inclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new inclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_inclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new inclusions in column: {col}\")\n",
    "            mask = (df1[col] == \"EXCLUDED\") & (df2[col] != \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_inclusion\"] = True\n",
    "            logger.info(f\"Number of new inclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"inclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_inclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "\n",
    "def finalize_delta(\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    target_index: str = \"permid\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Finalize the delta DataFrame by removing unchanged rows and resetting the index.\"\"\"\n",
    "    delta = delta.dropna(subset=test_col, how=\"all\")\n",
    "    delta.reset_index(inplace=True)\n",
    "    delta[target_index] = delta[target_index].astype(str)\n",
    "    logger.info(f\"Final delta shape: {delta.shape}\")\n",
    "    return delta\n",
    "\n",
    "def override_dict(\n",
    "        df:pd.DataFrame=None,\n",
    "        id_col:str=\"aladdin_id\",\n",
    "        str_col:str=\"ovr_target\",\n",
    "        ovr_col:str=\"ovr_value\",\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Converts the overrides DataFrame to a dictionary.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the overrides.\n",
    "        id_col (str): Column name for the identifier.\n",
    "        str_col (str): Column name for the strategy.\n",
    "        ovr_col (str): Column name for the override value.\n",
    "    Returns:\n",
    "        dict: Dictionary of overrides.\n",
    "    \"\"\"\n",
    "    # 1. Groupd the df by issuer_id\n",
    "    grouped = df.groupby(id_col)\n",
    "\n",
    "    # 2. Initialise the dictionary\n",
    "    ovr_dict = {}\n",
    "\n",
    "    # 3. Iterate over each group (issuer id and its corresponding rows)\n",
    "    for id, group_data in grouped:\n",
    "        # 3.1. for each issuer id create a dict pairing the strategy and the override value\n",
    "        ovr_result = dict(zip(group_data[str_col], group_data[ovr_col]))\n",
    "        # 3.2. add the dict to the main dict\n",
    "        ovr_dict[id] = ovr_result\n",
    "    \n",
    "    return ovr_dict\n",
    "\n",
    "# define a function to add portfolio OR benchmark info to the delta_df\n",
    "def add_portfolio_benchmark_info_to_df(\n",
    "    portfolio_dict, delta_df, column_name=\"affected_portfolio_str\"\n",
    "):\n",
    "\n",
    "    # Initialize a defaultdict to accumulate (portfolio_id, strategy_name) pairs\n",
    "    aladdin_to_info = defaultdict(list)\n",
    "\n",
    "    for portfolio_id, data in portfolio_dict.items():\n",
    "        strategy = data.get(\"strategy_name\")\n",
    "        for a_id in data.get(\"aladdin_id\", []):\n",
    "            aladdin_to_info[a_id].append((portfolio_id, strategy))\n",
    "\n",
    "    # Map each aladdin_id in delta_df to a list of accumulated portfolio info\n",
    "    delta_df[column_name] = delta_df[\"aladdin_id\"].apply(\n",
    "        lambda x: list(chain.from_iterable(aladdin_to_info.get(x, [])))\n",
    "    )\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "\n",
    "def get_issuer_level_df(df: pd.DataFrame, idx_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicates based on idx_name, and drops rows where idx_name column contains\n",
    "    NaN, None, or strings like \"nan\", \"NaN\", \"none\", or empty strings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        idx_name (str): Column name used for duplicate removal and NaN filtering.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Drop duplicates\n",
    "    df_cleaned = df.drop_duplicates(subset=[idx_name])\n",
    "\n",
    "    # Drop rows where idx_name is NaN/None or has invalid strings\n",
    "    valid_rows = df_cleaned[idx_name].notnull() & (\n",
    "        ~df_cleaned[idx_name]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .isin([\"nan\", \"none\", \"\"])\n",
    "    )\n",
    "\n",
    "    return df_cleaned[valid_rows]\n",
    "\n",
    "def filter_non_empty_lists(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame filtered so that rows where the specified column contains\n",
    "    an empty list are removed. Keeps rows where the column has a list with at least one element.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame\n",
    "    - column (str): The name of the column to check\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    return df[df[column].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "def filter_rows_with_common_elements(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Return rows of df where the lists in col1 and col2 have at least one common element.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        col1 (str): The name of the first column containing lists.\n",
    "        col2 (str): The name of the second column containing lists.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame filtered to include only rows where col1 and col2 have a common element.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering rows with common elements in columns: {col1} and {col2}\")\n",
    "    mask = df.apply(lambda row: bool(set(row[col1]).intersection(row[col2])), axis=1)\n",
    "    return df[mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:54:58,659 - utils.dataloaders - INFO - Loading Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\datafeeds_with_ovr\\202503_df_issuer_level_with_ovr.csv\n",
      "2025-03-28 16:54:59,221 - utils.dataloaders - INFO - Successfully loaded Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\datafeeds_with_ovr\\202503_df_issuer_level_with_ovr.csv\n",
      "2025-03-28 16:54:59,224 - utils.dataloaders - INFO - Loading Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n",
      "2025-03-28 16:54:59,800 - utils.dataloaders - INFO - Successfully loaded Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "# clarity data\n",
    "df_1 = load_clarity_data(df_1_path, columns_to_read)\n",
    "df_2 = load_clarity_data(df_2_path, columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns in df_1 and df_2 using the rename_dict\n",
    "df_1.rename(columns=rename_dict, inplace=True)\n",
    "df_2.rename(columns=rename_dict, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:54:59,831 - utils.dataloaders - INFO - Loading portfolio_carteras data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:55:16,551 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_carteras\n",
      "2025-03-28 16:55:16,553 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-03-28 16:55:16,557 - utils.dataloaders - INFO - Converting column 'portfolio_id' to string.\n",
      "2025-03-28 16:55:16,560 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:55:16,562 - utils.dataloaders - INFO - Loading portfolio_benchmarks data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:55:38,014 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_benchmarks\n",
      "2025-03-28 16:55:38,017 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-03-28 16:55:38,024 - utils.dataloaders - INFO - Converting column 'benchmark_id' to string.\n",
      "2025-03-28 16:55:38,028 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:55:38,030 - utils.dataloaders - INFO - Loading crossreference data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n",
      "2025-03-28 16:55:38,404 - utils.dataloaders - INFO - Cleaning columns and renaming crossreference data\n",
      "2025-03-28 16:55:38,407 - utils.dataloaders - INFO - Successfully loaded crossreference from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n"
     ]
    }
   ],
   "source": [
    "# aladdin /brs data / perimetros\n",
    "brs_carteras = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_carteras\")    \n",
    "brs_benchmarks = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_benchmarks\")\n",
    "crosreference = load_crossreference(CROSSREFERENCE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:55:38,429 - pre-ovr-analysis - INFO - Adding aladdin_id to clarity dfs\n"
     ]
    }
   ],
   "source": [
    "# add aladdin_id to df_1 and df_2\n",
    "logger.info(\"Adding aladdin_id to clarity dfs\")\n",
    "df_1 = df_1.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")\n",
    "df_2 = df_2.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_carteras_issuerlevel = get_issuer_level_df(brs_carteras, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_benchmarks_issuerlevel = get_issuer_level_df(brs_benchmarks, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading overrides columns ['clarityid', 'permid', 'brs_id', 'ovr_target', 'ovr_value']\n",
      "2025-03-28 16:55:38,808 - utils.dataloaders - INFO - Loading overrides from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\overrides\\overrides_db.xlsx\n"
     ]
    }
   ],
   "source": [
    "# sri/ESG Team data\n",
    "overrides = load_overrides(OVR_PATH)\n",
    "# rename column brs_id to aladdin_id\n",
    "overrides.rename(columns={\"brs_id\": \"aladdin_id\"}, inplace=True)\n",
    "# rename value column \"ovr_target\" using rename_dict if value is string\n",
    "overrides[\"ovr_target\"] = overrides[\"ovr_target\"].apply(\n",
    "    lambda x: pd.NA if isinstance(x, str) and x.strip().lower() in [\"na\", \"nan\"]\n",
    "    else rename_dict[x] if isinstance(x, str) and x in rename_dict\n",
    "    else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_dict = override_dict(overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:55:40,548 - utils.dataloaders - INFO - Loading portfolios from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:56:00,382 - utils.dataloaders - INFO - Loading benchmarks from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:56:13,880 - utils.dataloaders - INFO - Loading strategy data for portfolios from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\portfolios_committees\\portfolio_lists.xlsx\n",
      "2025-03-28 16:56:13,922 - utils.dataloaders - INFO - Loading strategy data for benchmarks from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\portfolios_committees\\portfolio_lists.xlsx\n",
      "2025-03-28 16:56:13,933 - utils.dataloaders - INFO - Benchmark ID 'FIFSANEUR' appears in multiple strategies\n",
      "2025-03-28 16:56:13,935 - utils.dataloaders - INFO - Benchmark ID 'MSNTCWEUR' appears in multiple strategies\n",
      "2025-03-28 16:56:13,937 - utils.dataloaders - INFO - Benchmark ID 'MSACWLDNET' appears in multiple strategies\n",
      "2025-03-28 16:56:13,938 - utils.dataloaders - INFO - Benchmark ID 'SNP500NR2' appears in multiple strategies\n",
      "2025-03-28 16:56:13,938 - utils.dataloaders - INFO - Benchmark ID 'MSCIEURNT' appears in multiple strategies\n",
      "2025-03-28 16:56:13,939 - utils.dataloaders - INFO - Benchmark ID 'IBEX35NR' appears in multiple strategies\n",
      "2025-03-28 16:56:13,940 - utils.dataloaders - INFO - Benchmark ID 'MLG5E0EUR' appears in multiple strategies\n",
      "2025-03-28 16:56:13,942 - utils.dataloaders - INFO - Benchmark ID 'MLEAASEUR' appears in multiple strategies\n",
      "2025-03-28 16:56:13,942 - utils.dataloaders - INFO - Benchmark ID 'STXEZ50' appears in multiple strategies\n",
      "2025-03-28 16:56:14,032 - utils.dataloaders - INFO - Filtering strategy entries\n",
      "2025-03-28 16:56:14,036 - utils.dataloaders - INFO - Dictonary cleaned of empty strategies\n",
      "2025-03-28 16:56:14,037 - utils.dataloaders - INFO - Filtering strategy entries\n",
      "2025-03-28 16:56:14,038 - utils.dataloaders - INFO - Dictonary cleaned of empty strategies\n"
     ]
    }
   ],
   "source": [
    "# Load portfolios & benchmarks dicts\n",
    "(\n",
    "    portfolio_dict,\n",
    "    benchmark_dict,\n",
    ") = load_portfolios(path_pb=BMK_PORTF_STR_PATH, path_committe=COMMITTEE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADM0PSA1 {'aladdin_id': ['J66430', 'R57958', 'R63005', 'R63005', 'F95505', 'F95505', 'B95488', 'I20488', 'I20488', 'I20488', 'R49188', '052591', 'R83199', '007699', 'R63609', 'H44516', '077779', '05564X', 'N15397', '128003', '128003', 'D43041', '191216', 'C96217', 'C70614', 'R50818', 'R57820', 'R60098', 'R60098', 'D34426', '36827E', '36827E', 'R51121', '24820T', 'F60954', 'C05702', 'C05702', 'D85398', 'FR0570', 'FR0570', 'FR0570', 'FR0570', 'R62757', 'R62757', 'R62757', 'K23944', '38142U', '38142U', 'F92008', 'R59316', 'C70514', 'R71927', 'C05671', 'R63519', 'R76481', 'R76481', 'R62699', 'R62699', '459200', '616880', 'R57939', 'R57939', 'R62038', 'R62038', 'R62038', 'C97848', 'R63600', 'R68028', '580135', '580135', 'G97316', 'D90986', 'R96795', 'MIZ00A', 'R55597', 'R65300', 'R68589', 'R57898', '128009', '128009', '713448', '713448', 'R57931', '742718', 'F13060', '760325', 'I40729', 'R65375', 'C74380', 'C74380', 'R21880', 'R21880', 'R57942', '830505', 'R49692', 'R49692', 'B89867', 'B89867', 'R65405', '846332', '846332', '846332', 'R64256', 'R58692', 'B89109', 'R28539', 'R57938', 'R56536', 'F50451', 'R59991', 'R59991', 'G88159', '911308', 'R60214', '949740', '949740', '949740'], 'strategy_name': 'scs_002_ec'}\n",
      "ADM0PSA2 {'aladdin_id': ['J66430', 'R57958', 'R63005', 'F95505', '052591', '007699', 'R63609', 'H44516', '077779', 'E81396', '128003', '128003', 'D43041', '172967', '191216', 'C96217', 'R60098', '36827E', '36827E', 'F60954', 'C05702', 'C05702', 'D85398', 'FR0570', 'FR0570', 'FR0570', 'FR0570', 'R62757', 'R62757', 'R62757', '38142U', 'R71927', 'R76481', 'R76481', 'R62699', '459200', '616880', 'R57939', 'R62038', 'R62038', 'R62038', 'R63600', 'R68028', '580135', 'G97316', 'R96795', 'R55597', 'R65300', 'R57898', '128009', '128009', '713448', '742718', 'I40729', 'R57476', 'R65375', 'C74380', 'R21880', 'R57942', '830505', 'R49692', 'R49692', 'B89867', 'R65405', '846332', '846332', '846332', 'R64256', 'R58692', 'B89109', 'R57938', 'R56536', 'R59991', 'G88159', '911308', 'R60214', '949740'], 'strategy_name': 'scs_002_ec'}\n",
      "CL19778 {'aladdin_id': ['C05702', 'C05702'], 'strategy_name': 'str_006_sec'}\n",
      "CPE00035 {'aladdin_id': ['I66408', 'E99416', '788350', '059456', '066050', '072730', 'J54377', 'R73574', 'D43041', 'D43041', '172967', '202597', '202597', '128005', 'G37009', 'R71411', 'R70538', '36827E', 'C05702', 'C05702', 'C05702', 'R44788', 'R66333', 'G32459', 'J95157', 'C05332', 'R74554', 'H92117', 'I03819', 'R57886', 'R94740', 'R93558', 'G88159', 'C05702', 'G68355'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00169 {'aladdin_id': ['I45020', 'G16611', 'E94490', 'K10973', 'C05702', 'C05702', 'F79320', 'G18629', 'G36494', 'G05261', 'I69133', 'G21293', '128009', 'R57882', 'H92117', 'I03819', 'G66335', 'G34089'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00264 {'aladdin_id': ['G06219', 'J10626', 'R59305', '059456', 'R49427', '05564X', 'C05702', 'R72674', 'H09269', 'C05702', 'R57928', 'C05702', 'R71411', 'F97381', 'R51302', 'C05702', 'C05702', 'F04261', 'FR0570', 'C05702', 'R63519', 'R71197', 'R65993', 'D17394', 'G18629', 'R60526', 'R57925', 'R57535', 'R54282', '128009', 'L77985', 'R74554', 'R55374', 'D88771', 'R21809', 'R77126', 'R93558', 'C05702', 'R60254', 'G29077'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00277 {'aladdin_id': ['G27169', '007699', 'D39873', 'D28969', 'E94490', 'R64339', '28503A', '28503A', '28503A', 'C05702', 'C05702', 'FR0570', 'FR0570', 'R62757', 'R62757', 'R63519', 'R60526', 'R64281', 'R57955', '846332', '846332', 'R64256', 'E97107', '92857L'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00363 {'aladdin_id': ['I42601', 'C85876', 'D80441', '059456', 'E94490', 'D43041', 'D43041', '202597', 'G37009', 'C05702', 'C05702', 'G22384', 'F38353', 'F32400', 'R54462', 'G21293', 'C05332', 'H92117', 'I03819', 'R77126', 'G66335'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00375 {'aladdin_id': ['J16059', 'E94490', 'C05702', 'C05702', 'G18629', 'I69133', 'F32400', 'F71548', 'G21293', 'G34089'], 'strategy_name': 'str_003b_ec'}\n",
      "CPE00448 {'aladdin_id': ['C85876', 'D39736', 'E81396', 'D43041', 'C05702', 'K10166', 'F71548', 'H92117', 'I03819', 'R77126', '912794', 'C05702'], 'strategy_name': 'str_003b_ec'}\n"
     ]
    }
   ],
   "source": [
    "for i, (k,v) in enumerate(portfolio_dict.items()):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = set(df_1.columns) & set(df_2.columns) & set(brs_carteras_issuerlevel.columns)\n",
    "common_cols = sorted(list(common_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START PRE-OVR ANALISIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the values of of the columns delta_test_cols are strings and all uppercase and strip\n",
    "for col in delta_test_cols:\n",
    "    df_1[col] = df_1[col].str.upper().str.strip()\n",
    "    df_2[col] = df_2[col].str.upper().str.strip()\n",
    "    brs_carteras_issuerlevel[col] = brs_carteras_issuerlevel[col].str.upper().str.strip()\n",
    "    brs_benchmarks_issuerlevel[col] = brs_benchmarks_issuerlevel[col].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:56:14,410 - pre-ovr-analysis - INFO - Setting index to permid.\n",
      "2025-03-28 16:56:14,554 - pre-ovr-analysis - INFO - Number of common indexes: 69222\n",
      "2025-03-28 16:56:14,650 - pre-ovr-analysis - INFO - Number of new issuers: 106\n",
      "2025-03-28 16:56:14,654 - pre-ovr-analysis - INFO - Number of missing issuers: 56\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA CLARITY LEVEL\n",
    "(\n",
    "    df_1, \n",
    "    df_2,\n",
    "    new_issuers_clarity,\n",
    "    out_issuer_clarity,\n",
    ") = prepare_dataframes(df_1, df_2)\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number of new issuers: {new_issuers_clarity.shape[0]}\")\n",
    "logger.info(f\"Number of missing issuers: {out_issuer_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permid</th>\n",
       "      <th>issuer_name</th>\n",
       "      <th>str_001_s</th>\n",
       "      <th>str_002_ec</th>\n",
       "      <th>str_003_ec</th>\n",
       "      <th>str_004_asec</th>\n",
       "      <th>str_005_ec</th>\n",
       "      <th>scs_001_sec</th>\n",
       "      <th>scs_002_ec</th>\n",
       "      <th>str_006_sec</th>\n",
       "      <th>str_sfdr8_aec</th>\n",
       "      <th>str_003b_ec</th>\n",
       "      <th>aladdin_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4295857675</td>\n",
       "      <td>Excelsior Capital Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>D73574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4295858270</td>\n",
       "      <td>Poseidon Nickel Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>D70698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4295858807</td>\n",
       "      <td>Silver Lake Deflector Pty Ltd</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4295859680</td>\n",
       "      <td>Azevedo &amp; Travassos SA</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>EXCLUDED</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>F80754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4295860216</td>\n",
       "      <td>Dimed SA Distribuidora de Medicamentos</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>OK</td>\n",
       "      <td>F85102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       permid                             issuer_name str_001_s str_002_ec  \\\n",
       "0  4295857675                   Excelsior Capital Ltd        OK         OK   \n",
       "1  4295858270                     Poseidon Nickel Ltd        OK         OK   \n",
       "2  4295858807           Silver Lake Deflector Pty Ltd        OK         OK   \n",
       "3  4295859680                  Azevedo & Travassos SA        OK         OK   \n",
       "4  4295860216  Dimed SA Distribuidora de Medicamentos        OK         OK   \n",
       "\n",
       "  str_003_ec str_004_asec str_005_ec scs_001_sec scs_002_ec str_006_sec  \\\n",
       "0         OK     EXCLUDED         OK    EXCLUDED         OK    EXCLUDED   \n",
       "1         OK     EXCLUDED         OK    EXCLUDED         OK    EXCLUDED   \n",
       "2         OK     EXCLUDED         OK    EXCLUDED         OK          OK   \n",
       "3         OK     EXCLUDED         OK          OK         OK          OK   \n",
       "4         OK           OK         OK          OK         OK          OK   \n",
       "\n",
       "  str_sfdr8_aec str_003b_ec aladdin_id  \n",
       "0            OK          OK     D73574  \n",
       "1            OK          OK     D70698  \n",
       "2            OK          OK        NaN  \n",
       "3            OK          OK     F80754  \n",
       "4            OK          OK     F85102  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset index for new_issuers_clarity and out_issuer_clarity\n",
    "new_issuers_clarity.reset_index(inplace=True)\n",
    "out_issuer_clarity.reset_index(inplace=True)\n",
    "\n",
    "# drop isin from out_issuer_clarity and new_issuers_clarity\n",
    "out_issuer_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "new_issuers_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "\n",
    "# remember to remove empyt empyt aladin id\n",
    "new_issuers_clarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:56:14,704 - pre-ovr-analysis - INFO - Setting index to aladdin_id.\n",
      "2025-03-28 16:56:14,863 - pre-ovr-analysis - INFO - Number of common indexes: 2552\n",
      "2025-03-28 16:56:14,968 - pre-ovr-analysis - INFO - Number issuers in clarity but not Aladdin: 66670\n",
      "2025-03-28 16:56:14,972 - pre-ovr-analysis - INFO - Number issuers in Aladdin but not Clarity: 1303\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA CARTERAS BRS LEVEL\n",
    "(\n",
    "    brs_df, \n",
    "    clarity_df,\n",
    "    in_clarity_but_not_in_brs,\n",
    "    in_brs_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_carteras_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not Aladdin: {in_clarity_but_not_in_brs.shape[0]}\")\n",
    "logger.info(f\"Number issuers in Aladdin but not Clarity: {in_brs_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:56:14,989 - pre-ovr-analysis - INFO - Setting index to aladdin_id.\n",
      "2025-03-28 16:56:15,191 - pre-ovr-analysis - INFO - Number of common indexes: 2552\n",
      "2025-03-28 16:56:15,304 - pre-ovr-analysis - INFO - Number issuers in clarity but not benchmarks: 66670\n",
      "2025-03-28 16:56:15,307 - pre-ovr-analysis - INFO - Number issuers in benchmarks but not Clarity: 1303\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA BENCHMARK BRS LEVEL\n",
    "(\n",
    "    brs_df_benchmarks, \n",
    "    clarity_df_benchmarks,\n",
    "    in_clarity_but_not_in_brs_benchmarks,\n",
    "    in_brs_benchmark_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_benchmarks_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not benchmarks: {in_clarity_but_not_in_brs_benchmarks.shape[0]}\")\n",
    "logger.info(f\"Number issuers in benchmarks but not Clarity: {in_brs_benchmark_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:56:15,326 - pre-ovr-analysis - INFO - comparing clarity dataframes\n",
      "2025-03-28 16:56:15,435 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-03-28 16:56:15,460 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-03-28 16:56:15,481 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-03-28 16:56:15,505 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-03-28 16:56:15,522 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-03-28 16:56:15,545 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-03-28 16:56:15,561 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-03-28 16:56:15,578 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-03-28 16:56:15,594 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-03-28 16:56:15,619 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-03-28 16:56:15,637 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-03-28 16:56:15,661 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 498\n",
      "2025-03-28 16:56:15,663 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-03-28 16:56:15,680 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 262\n",
      "2025-03-28 16:56:15,684 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-03-28 16:56:15,710 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 211\n",
      "2025-03-28 16:56:15,714 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-03-28 16:56:15,739 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 165\n",
      "2025-03-28 16:56:15,744 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-03-28 16:56:15,778 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 838\n",
      "2025-03-28 16:56:15,780 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-03-28 16:56:15,799 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 206\n",
      "2025-03-28 16:56:15,802 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-03-28 16:56:15,824 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 546\n",
      "2025-03-28 16:56:15,828 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-03-28 16:56:15,845 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 50\n",
      "2025-03-28 16:56:15,847 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-03-28 16:56:15,875 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 610\n",
      "2025-03-28 16:56:15,878 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-03-28 16:56:15,896 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 444\n",
      "2025-03-28 16:56:16,974 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-03-28 16:56:16,993 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 485\n",
      "2025-03-28 16:56:16,994 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-03-28 16:56:17,010 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 604\n",
      "2025-03-28 16:56:17,012 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-03-28 16:56:17,037 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 244\n",
      "2025-03-28 16:56:17,040 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-03-28 16:56:17,063 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 151\n",
      "2025-03-28 16:56:17,065 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-03-28 16:56:17,091 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 686\n",
      "2025-03-28 16:56:17,094 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-03-28 16:56:17,122 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 1248\n",
      "2025-03-28 16:56:17,126 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-03-28 16:56:17,155 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 788\n",
      "2025-03-28 16:56:17,157 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-03-28 16:56:17,190 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 136\n",
      "2025-03-28 16:56:17,193 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-03-28 16:56:17,235 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 685\n",
      "2025-03-28 16:56:17,238 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-03-28 16:56:17,272 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 296\n",
      "2025-03-28 16:56:21,663 - pre-ovr-analysis - INFO - Final delta shape: (4184, 18)\n",
      "2025-03-28 16:56:21,670 - pre-ovr-analysis - INFO - checking impact compared to BRS portfolio data\n",
      "2025-03-28 16:56:21,674 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-03-28 16:56:21,677 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-03-28 16:56:21,682 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-03-28 16:56:21,686 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-03-28 16:56:21,689 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-03-28 16:56:21,691 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-03-28 16:56:21,696 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-03-28 16:56:21,701 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-03-28 16:56:21,706 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-03-28 16:56:21,710 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-03-28 16:56:21,716 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-03-28 16:56:21,719 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 67\n",
      "2025-03-28 16:56:21,721 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-03-28 16:56:21,726 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 73\n",
      "2025-03-28 16:56:21,729 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-03-28 16:56:21,737 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 61\n",
      "2025-03-28 16:56:21,739 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-03-28 16:56:21,743 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 54\n",
      "2025-03-28 16:56:21,745 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-03-28 16:56:21,753 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 107\n",
      "2025-03-28 16:56:21,756 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-03-28 16:56:21,759 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 48\n",
      "2025-03-28 16:56:21,763 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-03-28 16:56:21,767 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 84\n",
      "2025-03-28 16:56:21,769 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-03-28 16:56:21,772 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 8\n",
      "2025-03-28 16:56:21,773 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-03-28 16:56:21,779 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 46\n",
      "2025-03-28 16:56:21,782 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-03-28 16:56:21,787 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 83\n",
      "2025-03-28 16:56:21,833 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-03-28 16:56:21,836 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 12\n",
      "2025-03-28 16:56:21,837 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-03-28 16:56:21,839 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 65\n",
      "2025-03-28 16:56:21,840 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-03-28 16:56:21,845 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n740789\\AppData\\Local\\Temp\\ipykernel_24160\\3880606627.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  delta[target_index] = delta[target_index].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:56:21,847 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-03-28 16:56:21,851 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 4\n",
      "2025-03-28 16:56:21,855 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-03-28 16:56:21,859 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 15\n",
      "2025-03-28 16:56:21,862 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-03-28 16:56:21,867 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 62\n",
      "2025-03-28 16:56:21,869 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-03-28 16:56:21,871 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 24\n",
      "2025-03-28 16:56:21,873 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-03-28 16:56:21,876 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 13\n",
      "2025-03-28 16:56:21,879 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-03-28 16:56:21,884 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 21\n",
      "2025-03-28 16:56:21,886 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-03-28 16:56:21,894 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 16\n",
      "2025-03-28 16:56:22,057 - pre-ovr-analysis - INFO - Final delta shape: (347, 17)\n",
      "2025-03-28 16:56:22,058 - pre-ovr-analysis - INFO - checking impact compared to BRS benchmarks data\n",
      "2025-03-28 16:56:22,063 - pre-ovr-analysis - INFO - Comparing column: str_001_s\n",
      "2025-03-28 16:56:22,066 - pre-ovr-analysis - INFO - Comparing column: str_002_ec\n",
      "2025-03-28 16:56:22,067 - pre-ovr-analysis - INFO - Comparing column: str_003_ec\n",
      "2025-03-28 16:56:22,069 - pre-ovr-analysis - INFO - Comparing column: str_003b_ec\n",
      "2025-03-28 16:56:22,072 - pre-ovr-analysis - INFO - Comparing column: str_004_asec\n",
      "2025-03-28 16:56:22,075 - pre-ovr-analysis - INFO - Comparing column: str_005_ec\n",
      "2025-03-28 16:56:22,082 - pre-ovr-analysis - INFO - Comparing column: str_006_sec\n",
      "2025-03-28 16:56:22,086 - pre-ovr-analysis - INFO - Comparing column: str_sfdr8_aec\n",
      "2025-03-28 16:56:22,089 - pre-ovr-analysis - INFO - Comparing column: scs_001_sec\n",
      "2025-03-28 16:56:22,095 - pre-ovr-analysis - INFO - Comparing column: scs_002_ec\n",
      "2025-03-28 16:56:22,102 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_001_s\n",
      "2025-03-28 16:56:22,108 - pre-ovr-analysis - INFO - Number of new exclusions in str_001_s: 67\n",
      "2025-03-28 16:56:22,111 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_002_ec\n",
      "2025-03-28 16:56:22,118 - pre-ovr-analysis - INFO - Number of new exclusions in str_002_ec: 73\n",
      "2025-03-28 16:56:22,121 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003_ec\n",
      "2025-03-28 16:56:22,128 - pre-ovr-analysis - INFO - Number of new exclusions in str_003_ec: 61\n",
      "2025-03-28 16:56:22,130 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_003b_ec\n",
      "2025-03-28 16:56:22,134 - pre-ovr-analysis - INFO - Number of new exclusions in str_003b_ec: 54\n",
      "2025-03-28 16:56:22,137 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_004_asec\n",
      "2025-03-28 16:56:22,145 - pre-ovr-analysis - INFO - Number of new exclusions in str_004_asec: 107\n",
      "2025-03-28 16:56:22,152 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_005_ec\n",
      "2025-03-28 16:56:22,162 - pre-ovr-analysis - INFO - Number of new exclusions in str_005_ec: 48\n",
      "2025-03-28 16:56:22,164 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_006_sec\n",
      "2025-03-28 16:56:22,170 - pre-ovr-analysis - INFO - Number of new exclusions in str_006_sec: 84\n",
      "2025-03-28 16:56:22,172 - pre-ovr-analysis - INFO - Checking for new exclusions in column: str_sfdr8_aec\n",
      "2025-03-28 16:56:22,178 - pre-ovr-analysis - INFO - Number of new exclusions in str_sfdr8_aec: 8\n",
      "2025-03-28 16:56:22,181 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_001_sec\n",
      "2025-03-28 16:56:22,185 - pre-ovr-analysis - INFO - Number of new exclusions in scs_001_sec: 46\n",
      "2025-03-28 16:56:22,187 - pre-ovr-analysis - INFO - Checking for new exclusions in column: scs_002_ec\n",
      "2025-03-28 16:56:22,189 - pre-ovr-analysis - INFO - Number of new exclusions in scs_002_ec: 83\n",
      "2025-03-28 16:56:22,235 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_001_s\n",
      "2025-03-28 16:56:22,236 - pre-ovr-analysis - INFO - Number of new inclusions in str_001_s: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n740789\\AppData\\Local\\Temp\\ipykernel_24160\\3880606627.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  delta[target_index] = delta[target_index].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:56:22,238 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_002_ec\n",
      "2025-03-28 16:56:22,241 - pre-ovr-analysis - INFO - Number of new inclusions in str_002_ec: 65\n",
      "2025-03-28 16:56:22,242 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003_ec\n",
      "2025-03-28 16:56:22,245 - pre-ovr-analysis - INFO - Number of new inclusions in str_003_ec: 24\n",
      "2025-03-28 16:56:22,247 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_003b_ec\n",
      "2025-03-28 16:56:22,250 - pre-ovr-analysis - INFO - Number of new inclusions in str_003b_ec: 4\n",
      "2025-03-28 16:56:22,252 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_004_asec\n",
      "2025-03-28 16:56:22,256 - pre-ovr-analysis - INFO - Number of new inclusions in str_004_asec: 15\n",
      "2025-03-28 16:56:22,258 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_005_ec\n",
      "2025-03-28 16:56:22,262 - pre-ovr-analysis - INFO - Number of new inclusions in str_005_ec: 62\n",
      "2025-03-28 16:56:22,264 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_006_sec\n",
      "2025-03-28 16:56:22,268 - pre-ovr-analysis - INFO - Number of new inclusions in str_006_sec: 24\n",
      "2025-03-28 16:56:22,270 - pre-ovr-analysis - INFO - Checking for new inclusions in column: str_sfdr8_aec\n",
      "2025-03-28 16:56:22,273 - pre-ovr-analysis - INFO - Number of new inclusions in str_sfdr8_aec: 13\n",
      "2025-03-28 16:56:22,275 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_001_sec\n",
      "2025-03-28 16:56:22,278 - pre-ovr-analysis - INFO - Number of new inclusions in scs_001_sec: 21\n",
      "2025-03-28 16:56:22,282 - pre-ovr-analysis - INFO - Checking for new inclusions in column: scs_002_ec\n",
      "2025-03-28 16:56:22,287 - pre-ovr-analysis - INFO - Number of new inclusions in scs_002_ec: 16\n",
      "2025-03-28 16:56:22,462 - pre-ovr-analysis - INFO - Final delta shape: (347, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n740789\\AppData\\Local\\Temp\\ipykernel_24160\\3880606627.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  delta[target_index] = delta[target_index].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# COMPARE DATA\n",
    "logger.info(\"comparing clarity dataframes\")\n",
    "delta_clarity = compare_dataframes(df_1, df_2)\n",
    "delta_clarity = check_new_exclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = check_new_inclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = finalize_delta(delta_clarity)\n",
    "logger.info(\"checking impact compared to BRS portfolio data\")\n",
    "delta_brs = compare_dataframes(brs_df, clarity_df)\n",
    "delta_brs = check_new_exclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = check_new_inclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = finalize_delta(delta_brs, target_index=\"aladdin_id\")\n",
    "logger.info(\"checking impact compared to BRS benchmarks data\")\n",
    "delta_benchmarks = compare_dataframes(brs_df_benchmarks, clarity_df_benchmarks)\n",
    "delta_benchmarks = check_new_exclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = check_new_inclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = finalize_delta(delta_benchmarks, target_index=\"aladdin_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:56:22,472 - pre-ovr-analysis - INFO - Getting zombie analysis df\n",
      "2025-03-28 16:56:22,473 - utils.dataloaders - INFO - Loading Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n",
      "2025-03-28 16:56:23,154 - utils.dataloaders - INFO - Successfully loaded Clarity data from: C:\\Users\\n740789\\Documents\\Projects_local\\DataSets\\DATAFEED\\ficheros_tratados\\2025\\20250401_Equities_feed_IssuerLevel_sinOVR.csv\n",
      "2025-03-28 16:56:23,157 - utils.dataloaders - INFO - Loading portfolio_carteras data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:56:47,407 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_carteras\n",
      "2025-03-28 16:56:47,410 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-03-28 16:56:47,414 - utils.dataloaders - INFO - Converting column 'portfolio_id' to string.\n",
      "2025-03-28 16:56:47,417 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:56:47,418 - utils.dataloaders - INFO - Loading portfolio_benchmarks data from C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:57:00,018 - utils.dataloaders - INFO - Cleaning columns and converting data types for portfolio_benchmarks\n",
      "2025-03-28 16:57:00,021 - utils.dataloaders - INFO - Converting column 'aladdin_id' to string.\n",
      "2025-03-28 16:57:00,036 - utils.dataloaders - INFO - Converting column 'benchmark_id' to string.\n",
      "2025-03-28 16:57:00,042 - utils.dataloaders - INFO - Successfully loaded Aladdin data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\bmk_portf_str\\202504_strategies_snt world_portf_bmks.xlsx\n",
      "2025-03-28 16:57:00,044 - utils.dataloaders - INFO - Loading crossreference data from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n",
      "2025-03-28 16:57:00,338 - utils.dataloaders - INFO - Cleaning columns and renaming crossreference data\n",
      "2025-03-28 16:57:00,341 - utils.dataloaders - INFO - Successfully loaded crossreference from: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\aladdin_data\\crossreference\\Aladdin_Clarity_Issuers_20250401.csv\n"
     ]
    }
   ],
   "source": [
    "#from utils.zombie_killer import main as zombie_killer\n",
    "logger.info(\"Getting zombie analysis df\")\n",
    "zombie_df = zombie_killer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:57:05,688 - pre-ovr-analysis - INFO - Preparing deltas before saving\n"
     ]
    }
   ],
   "source": [
    "# PREP DELTAS BEFORE SAVING\n",
    "logger.info(\"Preparing deltas before saving\")\n",
    "# use crossreference to add permid to delta_brs\n",
    "delta_brs = delta_brs.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "delta_benchmarks = delta_benchmarks.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "# drop isin from deltas\n",
    "delta_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_brs.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_benchmarks.drop(columns=[\"isin\"], inplace=True)\n",
    "# add new column to delta_brs with ovr_dict value using aladdin_id\n",
    "delta_brs[\"ovr_list\"] = delta_brs[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_clarity[\"ovr_list\"] = delta_clarity[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_benchmarks[\"ovr_list\"] = delta_benchmarks[\"aladdin_id\"].map(ovr_dict)\n",
    "# let's add portfolio info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_clarity)\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_brs)\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_benchmarks)\n",
    "# let's add benchmark info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_clarity, \"affected_benchmark_str\")\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_brs, \"affected_benchmark_str\")\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_benchmarks, \"affected_benchmark_str\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:57:05,800 - pre-ovr-analysis - INFO - Filtering rows with common elements in columns: exclusion_list_brs and affected_portfolio_str\n",
      "2025-03-28 16:57:05,803 - pre-ovr-analysis - INFO - Filtering rows with common elements in columns: exclusion_list_brs and affected_portfolio_str\n"
     ]
    }
   ],
   "source": [
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_brs = filter_non_empty_lists(delta_brs, \"affected_portfolio_str\")\n",
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_benchmarks = filter_non_empty_lists(delta_benchmarks, \"affected_portfolio_str\")\n",
    "# pass filter_rows_with_common_elements for columns exclusion_list_brs and affected_portfolio_str\n",
    "delta_brs = filter_rows_with_common_elements(delta_brs, \"exclusion_list_brs\", \"affected_portfolio_str\")\n",
    "delta_benchmarks = filter_rows_with_common_elements(delta_benchmarks, \"exclusion_list_brs\", \"affected_portfolio_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows from delta_brs where len of the list of in the column affected_benchmark_str is bigger than zero\n",
    "#delta_brs[delta_brs[\"affected_benchmark_str\"].apply(lambda x: len(x) > 0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reset df1 index to permid\n",
    "df_1.reset_index(inplace=True)\n",
    "df_1[\"permid\"] = df_1[\"permid\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dfs for the sheets of the strategies\n",
    "str_dfs_dict = {}\n",
    "# iterate through delta_test_cols\n",
    "for strategy in delta_test_cols:\n",
    "    df_name = strategy\n",
    "    # empty df\n",
    "    df_strategy = pd.DataFrame()\n",
    "    # iterate over the delta_df rows\n",
    "    for i, row in delta_brs.iterrows():\n",
    "        # if strategy is in the list inside exclusion_list_brs\n",
    "        if strategy in row[\"exclusion_list_brs\"]:\n",
    "            # get the aladdin_id\n",
    "            aladdin_id = row[\"aladdin_id\"]\n",
    "            # get the permid\n",
    "            permid = row[\"permid\"]\n",
    "            # get the issuer_name\n",
    "            issuer_name = row[\"issuer_name\"]\n",
    "            # get the strategy_value\n",
    "            strategy_value = row[strategy]\n",
    "            # get the affected_portfolio_str\n",
    "            affected_portfolio_str = row[\"affected_portfolio_str\"]\n",
    "            # get the exclusion_list_brs\n",
    "            exclusion_list_brs = row[\"exclusion_list_brs\"]\n",
    "            # get the new_exclusion\n",
    "            new_exclusion = row[\"new_exclusion\"]\n",
    "            # create new df with the columns\n",
    "            df = pd.DataFrame({\n",
    "                \"aladdin_id\": [aladdin_id],\n",
    "                \"permid\": [permid],\n",
    "                \"issuer_name\": [issuer_name],\n",
    "                \"affected_portfolio_str\": [affected_portfolio_str],\n",
    "                f\"{strategy}\": [strategy_value],\n",
    "            })\n",
    "            # add the df to the df_strategy\n",
    "            df_strategy = pd.concat([df_strategy, df])\n",
    "    # add the df_strategy to the dict\n",
    "    str_dfs_dict[df_name] = df_strategy\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "for strategy_name, df in str_dfs_dict.items():\n",
    "    #iterate over the rows of df\n",
    "    for i, row in df.iterrows():\n",
    "        # if permid in df_1\n",
    "        if row[\"permid\"] in df_1[\"permid\"].values:\n",
    "            # add the value column strategy_name in df_1 to the df\n",
    "            df.loc[i, f\"{strategy_name}_old\"] = df_1[df_1[\"permid\"] == row[\"permid\"]][strategy_name].values[0]\n",
    "        # if aladdin_id in brs_carteras_issuerlevel\n",
    "        if row[\"aladdin_id\"] in brs_carteras_issuerlevel[\"aladdin_id\"].values:\n",
    "            # add the value column strategy_name in brs_carteras_issuerlevel to the df\n",
    "            df.loc[i, f\"{strategy_name}_brs\"] = brs_carteras_issuerlevel[brs_carteras_issuerlevel[\"aladdin_id\"] == row[\"aladdin_id\"]][strategy_name].values[0]\n",
    "        # if permid in overrides & strategy_name in ovr_target\n",
    "        if (row[\"permid\"] in overrides[\"permid\"].values) and (strategy_name in overrides[\"ovr_target\"].values):\n",
    "            # Add the value column strategy_name in overrides to the df\n",
    "            match = overrides[(overrides[\"permid\"] == row[\"permid\"]) & (overrides[\"ovr_target\"] == strategy_name)]\n",
    "            if not match.empty:\n",
    "                df.loc[i, f\"{strategy_name}_ovr\"] = match[\"ovr_value\"].values[0]\n",
    "\n",
    "# for df in str_dfs_dict.values() move column named \"affected_portfolio_str\" to the end\n",
    "for df in str_dfs_dict.values():\n",
    "    df[\"affected_portfolio_str\"] = df.pop(\"affected_portfolio_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort columns of deltas df\n",
    "delta_brs = delta_brs[id_name_issuers_cols + [col for col in delta_brs.columns if (col not in id_name_issuers_cols) and (col not in delta_test_cols)]]\n",
    "delta_clarity = delta_clarity[id_name_issuers_cols + [col for col in delta_clarity.columns if (col not in id_name_issuers_cols) and (col not in delta_test_cols)]]\n",
    "delta_benchmarks = delta_benchmarks[id_name_issuers_cols + [col for col in delta_benchmarks.columns if (col not in id_name_issuers_cols) and (col not in delta_test_cols)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# define a function to save results in an Excel file\n",
    "def save_excel(df_dict: dict, output_dir: Path, file_name: str) -> Path:\n",
    "    \"\"\"\n",
    "    Writes multiple DataFrames to an Excel file with each DataFrame in a separate sheet.\n",
    "\n",
    "    Parameters:\n",
    "    - df_dict (dict): A dictionary where keys are sheet names and values are DataFrames.\n",
    "    - output_dir (Path): The directory where the Excel file will be saved.\n",
    "    - file_name (str): The base name for the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    - Path: The full path to the saved Excel file.\n",
    "    \"\"\"\n",
    "    # Create a date string in \"YYYYMMDD\" format\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    logger.info(\"Creating output directory: %s\", output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Construct the full output file path (e.g., file_name_YYYYMMDD.xlsx)\n",
    "    output_file = output_dir / f\"{date_str}_{file_name}.xlsx\"\n",
    "\n",
    "    # Write each DataFrame to its own sheet with index set to False\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        logger.info(\"Writing DataFrames to Excel file: %s\", output_file)\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            logger.info(\"Writing sheet: %s\", sheet_name)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    logger.info(\"Results saved to Excel file: %s\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aladdin_id</th>\n",
       "      <th>permid</th>\n",
       "      <th>issuer_name</th>\n",
       "      <th>new_exclusion</th>\n",
       "      <th>exclusion_list_brs</th>\n",
       "      <th>new_inclusion</th>\n",
       "      <th>inclusion_list_brs</th>\n",
       "      <th>ovr_list</th>\n",
       "      <th>affected_portfolio_str</th>\n",
       "      <th>affected_benchmark_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E97445</td>\n",
       "      <td>5000757618</td>\n",
       "      <td>ABN Amro Bank NV</td>\n",
       "      <td>True</td>\n",
       "      <td>[str_004_asec]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[FIG00677, str_004_asec, FIG01998, str_sfdr8_a...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>R57925</td>\n",
       "      <td>4295867366</td>\n",
       "      <td>L'Air Liquide Societe Anonyme pour l'Etude et ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[str_005_ec]</td>\n",
       "      <td>False</td>\n",
       "      <td>[str_006_sec, scs_001_sec, scs_002_ec]</td>\n",
       "      <td>{'str_005_ec': 'OK'}</td>\n",
       "      <td>[CPE00264, str_003b_ec, EPV00043, str_001_s, E...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>R63005</td>\n",
       "      <td>4295884955</td>\n",
       "      <td>Airbus SE</td>\n",
       "      <td>True</td>\n",
       "      <td>[str_001_s, str_004_asec]</td>\n",
       "      <td>False</td>\n",
       "      <td>[str_002_ec, str_003_ec, str_003b_ec, str_005_...</td>\n",
       "      <td>{'str_001_s': 'OK', 'str_004_asec': 'OK', 'cs_...</td>\n",
       "      <td>[ADM0PSA1, scs_002_ec, ADM0PSA1, scs_002_ec, A...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>B95488</td>\n",
       "      <td>5051775617</td>\n",
       "      <td>Anheuser-Busch Inbev SA (Pre-Reincorporation)</td>\n",
       "      <td>True</td>\n",
       "      <td>[str_005_ec]</td>\n",
       "      <td>False</td>\n",
       "      <td>[str_002_ec]</td>\n",
       "      <td>{'str_005_ec': 'OK'}</td>\n",
       "      <td>[ADM0PSA1, scs_002_ec, EPV00043, str_001_s, EP...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>007699</td>\n",
       "      <td>8589934205</td>\n",
       "      <td>Banco Santander SA</td>\n",
       "      <td>True</td>\n",
       "      <td>[str_001_s, str_002_ec, str_003_ec, str_003b_e...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'str_001_s': 'OK', 'str_002_ec': 'OK', 'str_0...</td>\n",
       "      <td>[ADM0PSA1, scs_002_ec, ADM0PSA2, scs_002_ec, C...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aladdin_id      permid                                        issuer_name  \\\n",
       "4      E97445  5000757618                                   ABN Amro Bank NV   \n",
       "12     R57925  4295867366  L'Air Liquide Societe Anonyme pour l'Etude et ...   \n",
       "13     R63005  4295884955                                          Airbus SE   \n",
       "21     B95488  5051775617      Anheuser-Busch Inbev SA (Pre-Reincorporation)   \n",
       "27     007699  8589934205                                 Banco Santander SA   \n",
       "\n",
       "    new_exclusion                                 exclusion_list_brs  \\\n",
       "4            True                                     [str_004_asec]   \n",
       "12           True                                       [str_005_ec]   \n",
       "13           True                          [str_001_s, str_004_asec]   \n",
       "21           True                                       [str_005_ec]   \n",
       "27           True  [str_001_s, str_002_ec, str_003_ec, str_003b_e...   \n",
       "\n",
       "    new_inclusion                                 inclusion_list_brs  \\\n",
       "4           False                                                 []   \n",
       "12          False             [str_006_sec, scs_001_sec, scs_002_ec]   \n",
       "13          False  [str_002_ec, str_003_ec, str_003b_ec, str_005_...   \n",
       "21          False                                       [str_002_ec]   \n",
       "27          False                                                 []   \n",
       "\n",
       "                                             ovr_list  \\\n",
       "4                                                 NaN   \n",
       "12                               {'str_005_ec': 'OK'}   \n",
       "13  {'str_001_s': 'OK', 'str_004_asec': 'OK', 'cs_...   \n",
       "21                               {'str_005_ec': 'OK'}   \n",
       "27  {'str_001_s': 'OK', 'str_002_ec': 'OK', 'str_0...   \n",
       "\n",
       "                               affected_portfolio_str affected_benchmark_str  \n",
       "4   [FIG00677, str_004_asec, FIG01998, str_sfdr8_a...                     []  \n",
       "12  [CPE00264, str_003b_ec, EPV00043, str_001_s, E...                     []  \n",
       "13  [ADM0PSA1, scs_002_ec, ADM0PSA1, scs_002_ec, A...                     []  \n",
       "21  [ADM0PSA1, scs_002_ec, EPV00043, str_001_s, EP...                     []  \n",
       "27  [ADM0PSA1, scs_002_ec, ADM0PSA2, scs_002_ec, C...                     []  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_brs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE ISSUER HAS ALL THE PORTFOLIOS IN THEIR LIST NOT JUST THE LAST ONE\n",
    "\n",
    "# create dict of df and df name\n",
    "dfs_dict = {\n",
    "    \"zombie_analysis\": zombie_df,\n",
    "    \"delta_carteras\": delta_brs,\n",
    "    \"delta_benchmarks\": delta_benchmarks,\n",
    "    \"delta_clarity\": delta_clarity,\n",
    "    \"new_issuers_clarity\": new_issuers_clarity,\n",
    "    \"out_issuer_clarity\": out_issuer_clarity,\n",
    "}\n",
    "\n",
    "# add to dfs_dict the str_dfs_dict\n",
    "dfs_dict.update(str_dfs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:58:12,180 - pre-ovr-analysis - INFO - Creating output directory: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\n",
      "2025-03-28 16:58:12,187 - pre-ovr-analysis - INFO - Writing DataFrames to Excel file: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\\20250328_pre_ovr_analysis.xlsx\n",
      "2025-03-28 16:58:12,192 - pre-ovr-analysis - INFO - Writing sheet: zombie_analysis\n",
      "2025-03-28 16:58:12,212 - pre-ovr-analysis - INFO - Writing sheet: delta_carteras\n",
      "2025-03-28 16:58:12,231 - pre-ovr-analysis - INFO - Writing sheet: delta_benchmarks\n",
      "2025-03-28 16:58:12,247 - pre-ovr-analysis - INFO - Writing sheet: delta_clarity\n",
      "2025-03-28 16:58:12,817 - pre-ovr-analysis - INFO - Writing sheet: new_issuers_clarity\n",
      "2025-03-28 16:58:12,970 - pre-ovr-analysis - INFO - Writing sheet: out_issuer_clarity\n",
      "2025-03-28 16:58:12,979 - pre-ovr-analysis - INFO - Writing sheet: str_001_s\n",
      "2025-03-28 16:58:12,983 - pre-ovr-analysis - INFO - Writing sheet: str_002_ec\n",
      "2025-03-28 16:58:12,990 - pre-ovr-analysis - INFO - Writing sheet: str_003_ec\n",
      "2025-03-28 16:58:12,997 - pre-ovr-analysis - INFO - Writing sheet: str_003b_ec\n",
      "2025-03-28 16:58:13,004 - pre-ovr-analysis - INFO - Writing sheet: str_004_asec\n",
      "2025-03-28 16:58:13,012 - pre-ovr-analysis - INFO - Writing sheet: str_005_ec\n",
      "2025-03-28 16:58:13,017 - pre-ovr-analysis - INFO - Writing sheet: str_006_sec\n",
      "2025-03-28 16:58:13,024 - pre-ovr-analysis - INFO - Writing sheet: str_sfdr8_aec\n",
      "2025-03-28 16:58:13,028 - pre-ovr-analysis - INFO - Writing sheet: scs_001_sec\n",
      "2025-03-28 16:58:13,033 - pre-ovr-analysis - INFO - Writing sheet: scs_002_ec\n",
      "2025-03-28 16:58:13,397 - pre-ovr-analysis - INFO - Results saved to Excel file: C:\\Users\\n740789\\Documents\\clarity_data_quality_controls\\excel_books\\sri_data\\pre-ovr-analysis\\20250328_pre_ovr_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "# save to excel\n",
    "save_excel(dfs_dict, OUTPUT_DIR, file_name=\"pre_ovr_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

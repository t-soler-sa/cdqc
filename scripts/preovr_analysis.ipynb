{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.dataloaders import (\n",
    "    load_clarity_data,\n",
    "    load_aladdin_data,\n",
    "    load_crossreference,\n",
    "    load_portfolios,\n",
    "    load_overrides,\n",
    "    save_excel\n",
    ")\n",
    "from utils.zombie_killer import main as zombie_killer\n",
    "\n",
    "# Import the centralized configuration\n",
    "from config import get_config\n",
    "\n",
    "# Get the common configuration for the Pre-OVR-Analysis script.\n",
    "config = get_config(\"pre-ovr-analysis\", interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config[\"logger\"]\n",
    "DATE = config[\"DATE\"]\n",
    "YEAR = config[\"YEAR\"]\n",
    "DATE_PREV = config[\"DATE_PREV\"]\n",
    "REPO_DIR = config[\"REPO_DIR\"]\n",
    "DATAFEED_DIR = config[\"DATAFEED_DIR\"]\n",
    "SRI_DATA_DIR = config[\"SRI_DATA_DIR\"]\n",
    "paths = config[\"paths\"]\n",
    "\n",
    "# Use the paths from config\n",
    "df_1_path = paths[\"PRE_DF_WOVR_PATH\"]\n",
    "df_2_path = paths[\"CURRENT_DF_WOUTOVR_PATH\"]\n",
    "CROSSREFERENCE_PATH = paths[\"CROSSREFERENCE_PATH\"]\n",
    "BMK_PORTF_STR_PATH = paths[\"BMK_PORTF_STR_PATH\"]\n",
    "OVR_PATH = paths[\"OVR_PATH\"]\n",
    "COMMITTEE_PATH = paths[\"COMMITTEE_PATH\"]\n",
    "\n",
    "# Define the output directory and file based on the configuration.\n",
    "OUTPUT_DIR = config[\"OUTPUT_DIR\"]\n",
    "OUTPUT_FILE = OUTPUT_DIR / f\"{DATE}_pre_ovr_analysis.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore workbook warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the date constants are set correctly\n",
    "print(f\"{DATE} and {YEAR} and {DATE_PREV}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TEST COLUMNS\n",
    "# let's define necessary column lists\n",
    "\n",
    "id_name_cols = [\"permid\", \"isin\", \"issuer_name\"]\n",
    "id_name_issuers_cols = [\"aladdin_id\", \"permid\", \"issuer_name\"]\n",
    "clarity_test_col = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"art_8_basicos\",\n",
    "    \"str_006_sec\",\n",
    "    \"cs_001_sec\",\n",
    "    \"cs_002_ec\",\n",
    "]\n",
    "columns_to_read = id_name_cols + clarity_test_col\n",
    "delta_test_cols = [\n",
    "    \"str_001_s\",\n",
    "    \"str_002_ec\",\n",
    "    \"str_003_ec\",\n",
    "    \"str_003b_ec\",\n",
    "    \"str_004_asec\",\n",
    "    \"str_005_ec\",\n",
    "    \"str_006_sec\",\n",
    "    \"str_sfdr8_aec\",\n",
    "    \"scs_001_sec\",\n",
    "    \"scs_002_ec\",\n",
    "]\n",
    "\n",
    "brs_test_cols = [\"aladdin_id\"] + delta_test_cols\n",
    "rename_dict = {\n",
    "    \"cs_001_sec\": \"scs_001_sec\",\n",
    "    \"cs_002_ec\": \"scs_002_ec\",\n",
    "    \"art_8_basicos\": \"str_sfdr8_aec\",\n",
    "}\n",
    "\n",
    "dfs_name_str_dict = {\n",
    "    \"str_001_s\" : \"str_001_s\",\n",
    "    \"str_002_ec\" : \"str_002_ec\",\n",
    "    \"str_003_ec\" : \"str_003_ec\",\n",
    "    \"str_003b_ec\" : \"str_003b_ec\",\n",
    "    \"str_004_asec\" : \"str_004_asec\",\n",
    "    \"str_005_ec\" : \"str_005_ec\",\n",
    "    \"str_006_sec\" : \"str_006_sec\",\n",
    "    \"str_sfdr8_aec\" : \"str_sfdr8_aec\",\n",
    "    \"scs_001_sec\" : \"scs_001_sec\",\n",
    "    \"scs_002_ec\" : \"scs_002_ec\",\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframes(\n",
    "    base_df: pd.DataFrame, new_df: pd.DataFrame, target_index:str = \"permid\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare DataFrames by setting the index and filtering for common indexes.\n",
    "    Logs info about common, new, and missing indexes.\n",
    "    \"\"\"\n",
    "    # Set index to 'permid' if it exists, otherwise assume it's already the index.\n",
    "    logger.info(f\"Setting index to {target_index}.\")\n",
    "    if target_index in base_df.columns:\n",
    "        base_df = base_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df1 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    if target_index in new_df.columns:\n",
    "        new_df = new_df.set_index(target_index)\n",
    "    else:\n",
    "        logger.warning(\"df2 does not contain a 'permid' column. Using current index.\")\n",
    "\n",
    "    common_indexes = base_df.index.intersection(new_df.index)\n",
    "    new_indexes = new_df.index.difference(base_df.index)\n",
    "    missing_indexes = base_df.index.difference(new_df.index)\n",
    "\n",
    "    logger.info(f\"Number of common indexes: {len(common_indexes)}\")\n",
    "\n",
    "    return (\n",
    "        base_df.loc[common_indexes],\n",
    "        new_df.loc[common_indexes],\n",
    "        new_df.loc[new_indexes],\n",
    "        base_df.loc[missing_indexes],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(\n",
    "    df1: pd.DataFrame, df2: pd.DataFrame, test_col: List[str] = delta_test_cols\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare DataFrames and create a delta DataFrame.\"\"\"\n",
    "    delta = df2.copy()\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Comparing column: {col}\")\n",
    "            # Create a mask for differences between the two DataFrames\n",
    "            diff_mask = df1[col] != df2[col]\n",
    "            # Update the delta DataFrame with the differences\n",
    "            delta.loc[~diff_mask, col] = np.nan\n",
    "    return delta\n",
    "\n",
    "\n",
    "def get_exclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed to EXCLUDED.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] == \"EXCLUDED\" and df1.loc[row.name, col] != \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_inclusion_list(\n",
    "    row: pd.Series,\n",
    "    df1: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of columns that changed from EXCLUDED to any other value.\"\"\"\n",
    "    return [\n",
    "        col\n",
    "        for col in test_col\n",
    "        if row[col] != \"EXCLUDED\" and df1.loc[row.name, col] == \"EXCLUDED\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def check_new_exclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new exclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_exclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new exclusions in column: {col}\")\n",
    "            mask = (df1[col] != \"EXCLUDED\") & (df2[col] == \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_exclusion\"] = True\n",
    "            logger.info(f\"Number of new exclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"exclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_exclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "\n",
    "def check_new_inclusions(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    suffix_level: str = \"\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Check for new inclusions and update the delta DataFrame.\"\"\"\n",
    "    delta[\"new_inclusion\"] = False\n",
    "    for col in test_col:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            logger.info(f\"Checking for new inclusions in column: {col}\")\n",
    "            mask = (df1[col] == \"EXCLUDED\") & (df2[col] != \"EXCLUDED\")\n",
    "            delta.loc[mask, \"new_inclusion\"] = True\n",
    "            logger.info(f\"Number of new inclusions in {col}: {mask.sum()}\")\n",
    "    delta[f\"inclusion_list{suffix_level}\"] = delta.apply(\n",
    "        lambda row: get_inclusion_list(row, df1, test_col), axis=1\n",
    "    )\n",
    "    return delta\n",
    "\n",
    "\n",
    "def finalize_delta(\n",
    "    delta: pd.DataFrame,\n",
    "    test_col: List[str] = delta_test_cols,\n",
    "    target_index: str = \"permid\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Finalize the delta DataFrame by removing unchanged rows and resetting the index.\"\"\"\n",
    "    delta = delta.dropna(subset=test_col, how=\"all\")\n",
    "    delta.reset_index(inplace=True)\n",
    "    delta[target_index] = delta[target_index].astype(str)\n",
    "    logger.info(f\"Final delta shape: {delta.shape}\")\n",
    "    return delta\n",
    "\n",
    "def override_dict(\n",
    "        df:pd.DataFrame=None,\n",
    "        id_col:str=\"aladdin_id\",\n",
    "        str_col:str=\"ovr_target\",\n",
    "        ovr_col:str=\"ovr_value\",\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Converts the overrides DataFrame to a dictionary.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the overrides.\n",
    "        id_col (str): Column name for the identifier.\n",
    "        str_col (str): Column name for the strategy.\n",
    "        ovr_col (str): Column name for the override value.\n",
    "    Returns:\n",
    "        dict: Dictionary of overrides.\n",
    "    \"\"\"\n",
    "    # 1. Groupd the df by issuer_id\n",
    "    grouped = df.groupby(id_col)\n",
    "\n",
    "    # 2. Initialise the dictionary\n",
    "    ovr_dict = {}\n",
    "\n",
    "    # 3. Iterate over each group (issuer id and its corresponding rows)\n",
    "    for id, group_data in grouped:\n",
    "        # 3.1. for each issuer id create a dict pairing the strategy and the override value\n",
    "        ovr_result = dict(zip(group_data[str_col], group_data[ovr_col]))\n",
    "        # 3.2. add the dict to the main dict\n",
    "        ovr_dict[id] = ovr_result\n",
    "    \n",
    "    return ovr_dict\n",
    "\n",
    "# define a function to add portfolio OR benchmark info to the delta_df\n",
    "def add_portfolio_benchmark_info_to_df(\n",
    "    portfolio_dict, delta_df, column_name=\"affected_portfolio_str\"\n",
    "):\n",
    "\n",
    "    # Initialize a defaultdict to accumulate (portfolio_id, strategy_name) pairs\n",
    "    aladdin_to_info = defaultdict(list)\n",
    "\n",
    "    for portfolio_id, data in portfolio_dict.items():\n",
    "        strategy = data.get(\"strategy_name\")\n",
    "        for a_id in data.get(\"aladdin_id\", []):\n",
    "            aladdin_to_info[a_id].append((portfolio_id, strategy))\n",
    "\n",
    "    # Map each aladdin_id in delta_df to a list of accumulated portfolio info\n",
    "    delta_df[column_name] = delta_df[\"aladdin_id\"].apply(\n",
    "        lambda x: list(chain.from_iterable(aladdin_to_info.get(x, [])))\n",
    "    )\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "\n",
    "def get_issuer_level_df(df: pd.DataFrame, idx_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicates based on idx_name, and drops rows where idx_name column contains\n",
    "    NaN, None, or strings like \"nan\", \"NaN\", \"none\", or empty strings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        idx_name (str): Column name used for duplicate removal and NaN filtering.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Drop duplicates\n",
    "    df_cleaned = df.drop_duplicates(subset=[idx_name])\n",
    "\n",
    "    # Drop rows where idx_name is NaN/None or has invalid strings\n",
    "    valid_rows = df_cleaned[idx_name].notnull() & (\n",
    "        ~df_cleaned[idx_name]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .isin([\"nan\", \"none\", \"\"])\n",
    "    )\n",
    "\n",
    "    return df_cleaned[valid_rows]\n",
    "\n",
    "def filter_non_empty_lists(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame filtered so that rows where the specified column contains\n",
    "    an empty list are removed. Keeps rows where the column has a list with at least one element.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame\n",
    "    - column (str): The name of the column to check\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    return df[df[column].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "def filter_rows_with_common_elements(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Return rows of df where the lists in col1 and col2 have at least one common element.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        col1 (str): The name of the first column containing lists.\n",
    "        col2 (str): The name of the second column containing lists.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame filtered to include only rows where col1 and col2 have a common element.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering rows with common elements in columns: {col1} and {col2}\")\n",
    "    mask = df.apply(lambda row: bool(set(row[col1]).intersection(row[col2])), axis=1)\n",
    "    return df[mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "# clarity data\n",
    "df_1 = load_clarity_data(df_1_path, columns_to_read)\n",
    "df_2 = load_clarity_data(df_2_path, columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns in df_1 and df_2 using the rename_dict\n",
    "df_1.rename(columns=rename_dict, inplace=True)\n",
    "df_2.rename(columns=rename_dict, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aladdin /brs data / perimetros\n",
    "brs_carteras = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_carteras\")    \n",
    "brs_benchmarks = load_aladdin_data(BMK_PORTF_STR_PATH, \"portfolio_benchmarks\")\n",
    "crosreference = load_crossreference(CROSSREFERENCE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add aladdin_id to df_1 and df_2\n",
    "logger.info(\"Adding aladdin_id to clarity dfs\")\n",
    "df_1 = df_1.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")\n",
    "df_2 = df_2.merge(crosreference[[\"permid\", \"aladdin_id\"]], on=\"permid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_carteras_issuerlevel = get_issuer_level_df(brs_carteras, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BRS data at issuer level for becnhmarks without empty aladdin_id\n",
    "brs_benchmarks_issuerlevel = get_issuer_level_df(brs_benchmarks, \"aladdin_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sri/ESG Team data\n",
    "overrides = load_overrides(OVR_PATH)\n",
    "# rename column brs_id to aladdin_id\n",
    "overrides.rename(columns={\"brs_id\": \"aladdin_id\"}, inplace=True)\n",
    "# rename value column \"ovr_target\" using rename_dict if value is string\n",
    "overrides[\"ovr_target\"] = overrides[\"ovr_target\"].apply(\n",
    "    lambda x: pd.NA if isinstance(x, str) and x.strip().lower() in [\"na\", \"nan\"]\n",
    "    else rename_dict[x] if isinstance(x, str) and x in rename_dict\n",
    "    else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_dict = override_dict(overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load portfolios & benchmarks dicts\n",
    "(\n",
    "    portfolio_dict,\n",
    "    benchmark_dict,\n",
    ") = load_portfolios(path_pb=BMK_PORTF_STR_PATH, path_committe=COMMITTEE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k,v) in enumerate(portfolio_dict.items()):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = set(df_1.columns) & set(df_2.columns) & set(brs_carteras_issuerlevel.columns)\n",
    "common_cols = sorted(list(common_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START PRE-OVR ANALISIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the values of of the columns delta_test_cols are strings and all uppercase and strip\n",
    "for col in delta_test_cols:\n",
    "    df_1[col] = df_1[col].str.upper().str.strip()\n",
    "    df_2[col] = df_2[col].str.upper().str.strip()\n",
    "    brs_carteras_issuerlevel[col] = brs_carteras_issuerlevel[col].str.upper().str.strip()\n",
    "    brs_benchmarks_issuerlevel[col] = brs_benchmarks_issuerlevel[col].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA CLARITY LEVEL\n",
    "(\n",
    "    df_1, \n",
    "    df_2,\n",
    "    new_issuers_clarity,\n",
    "    out_issuer_clarity,\n",
    ") = prepare_dataframes(df_1, df_2)\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number of new issuers: {new_issuers_clarity.shape[0]}\")\n",
    "logger.info(f\"Number of missing issuers: {out_issuer_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index for new_issuers_clarity and out_issuer_clarity\n",
    "new_issuers_clarity.reset_index(inplace=True)\n",
    "out_issuer_clarity.reset_index(inplace=True)\n",
    "\n",
    "# drop isin from out_issuer_clarity and new_issuers_clarity\n",
    "out_issuer_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "new_issuers_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "\n",
    "# remember to remove empyt empyt aladin id\n",
    "new_issuers_clarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA CARTERAS BRS LEVEL\n",
    "(\n",
    "    brs_df, \n",
    "    clarity_df,\n",
    "    in_clarity_but_not_in_brs,\n",
    "    in_brs_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_carteras_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not Aladdin: {in_clarity_but_not_in_brs.shape[0]}\")\n",
    "logger.info(f\"Number issuers in Aladdin but not Clarity: {in_brs_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA BENCHMARK BRS LEVEL\n",
    "(\n",
    "    brs_df_benchmarks, \n",
    "    clarity_df_benchmarks,\n",
    "    in_clarity_but_not_in_brs_benchmarks,\n",
    "    in_brs_benchmark_but_not_in_clarity,\n",
    ") = prepare_dataframes(brs_benchmarks_issuerlevel, df_2, target_index=\"aladdin_id\")\n",
    "\n",
    "# log size of new and missing issuers\n",
    "logger.info(f\"Number issuers in clarity but not benchmarks: {in_clarity_but_not_in_brs_benchmarks.shape[0]}\")\n",
    "logger.info(f\"Number issuers in benchmarks but not Clarity: {in_brs_benchmark_but_not_in_clarity.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARE DATA\n",
    "logger.info(\"comparing clarity dataframes\")\n",
    "delta_clarity = compare_dataframes(df_1, df_2)\n",
    "delta_clarity = check_new_exclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = check_new_inclusions(df_1, df_2, delta_clarity)\n",
    "delta_clarity = finalize_delta(delta_clarity)\n",
    "logger.info(\"checking impact compared to BRS portfolio data\")\n",
    "delta_brs = compare_dataframes(brs_df, clarity_df)\n",
    "delta_brs = check_new_exclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = check_new_inclusions(brs_df, clarity_df, delta_brs, suffix_level=\"_brs\")\n",
    "delta_brs = finalize_delta(delta_brs, target_index=\"aladdin_id\")\n",
    "logger.info(\"checking impact compared to BRS benchmarks data\")\n",
    "delta_benchmarks = compare_dataframes(brs_df_benchmarks, clarity_df_benchmarks)\n",
    "delta_benchmarks = check_new_exclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = check_new_inclusions(brs_df_benchmarks, clarity_df_benchmarks, delta_benchmarks, suffix_level=\"_brs\")\n",
    "delta_benchmarks = finalize_delta(delta_benchmarks, target_index=\"aladdin_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.zombie_killer import main as zombie_killer\n",
    "logger.info(\"Getting zombie analysis df\")\n",
    "zombie_df = zombie_killer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREP DELTAS BEFORE SAVING\n",
    "logger.info(\"Preparing deltas before saving\")\n",
    "# use crossreference to add permid to delta_brs\n",
    "delta_brs = delta_brs.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "delta_benchmarks = delta_benchmarks.merge(crosreference[[\"aladdin_id\", \"permid\"]], on=\"aladdin_id\", how=\"left\")\n",
    "# drop isin from deltas\n",
    "delta_clarity.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_brs.drop(columns=[\"isin\"], inplace=True)\n",
    "delta_benchmarks.drop(columns=[\"isin\"], inplace=True)\n",
    "# add new column to delta_brs with ovr_dict value using aladdin_id\n",
    "delta_brs[\"ovr_list\"] = delta_brs[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_clarity[\"ovr_list\"] = delta_clarity[\"aladdin_id\"].map(ovr_dict)\n",
    "delta_benchmarks[\"ovr_list\"] = delta_benchmarks[\"aladdin_id\"].map(ovr_dict)\n",
    "# let's add portfolio info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_clarity)\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_brs)\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(portfolio_dict, delta_benchmarks)\n",
    "# let's add benchmark info to the delta_df\n",
    "delta_clarity = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_clarity, \"affected_benchmark_str\")\n",
    "delta_brs = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_brs, \"affected_benchmark_str\")\n",
    "delta_benchmarks = add_portfolio_benchmark_info_to_df(benchmark_dict, delta_benchmarks, \"affected_benchmark_str\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_brs = filter_non_empty_lists(delta_brs, \"affected_portfolio_str\")\n",
    "# let's use filter_non_empty_lists to remove rows with empty lists in affected_portfolio_str\n",
    "delta_benchmarks = filter_non_empty_lists(delta_benchmarks, \"affected_portfolio_str\")\n",
    "# pass filter_rows_with_common_elements for columns exclusion_list_brs and affected_portfolio_str\n",
    "delta_brs = filter_rows_with_common_elements(delta_brs, \"exclusion_list_brs\", \"affected_portfolio_str\")\n",
    "delta_benchmarks = filter_rows_with_common_elements(delta_benchmarks, \"exclusion_list_brs\", \"affected_portfolio_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows from delta_brs where len of the list of in the column affected_benchmark_str is bigger than zero\n",
    "#delta_brs[delta_brs[\"affected_benchmark_str\"].apply(lambda x: len(x) > 0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reset df1 index to permid\n",
    "df_1.reset_index(inplace=True)\n",
    "df_1[\"permid\"] = df_1[\"permid\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dfs for the sheets of the strategies\n",
    "str_dfs_dict = {}\n",
    "# iterate through delta_test_cols\n",
    "for strategy in delta_test_cols:\n",
    "    df_name = strategy\n",
    "    # empty df\n",
    "    df_strategy = pd.DataFrame()\n",
    "    # iterate over the delta_df rows\n",
    "    for i, row in delta_brs.iterrows():\n",
    "        # if strategy is in the list inside exclusion_list_brs\n",
    "        if strategy in row[\"exclusion_list_brs\"]:\n",
    "            # get the aladdin_id\n",
    "            aladdin_id = row[\"aladdin_id\"]\n",
    "            # get the permid\n",
    "            permid = row[\"permid\"]\n",
    "            # get the issuer_name\n",
    "            issuer_name = row[\"issuer_name\"]\n",
    "            # get the strategy_value\n",
    "            strategy_value = row[strategy]\n",
    "            # get the affected_portfolio_str\n",
    "            affected_portfolio_str = row[\"affected_portfolio_str\"]\n",
    "            # get the exclusion_list_brs\n",
    "            exclusion_list_brs = row[\"exclusion_list_brs\"]\n",
    "            # get the new_exclusion\n",
    "            new_exclusion = row[\"new_exclusion\"]\n",
    "            # create new df with the columns\n",
    "            df = pd.DataFrame({\n",
    "                \"aladdin_id\": [aladdin_id],\n",
    "                \"permid\": [permid],\n",
    "                \"issuer_name\": [issuer_name],\n",
    "                \"affected_portfolio_str\": [affected_portfolio_str],\n",
    "                f\"{strategy}\": [strategy_value],\n",
    "            })\n",
    "            # add the df to the df_strategy\n",
    "            df_strategy = pd.concat([df_strategy, df])\n",
    "    # add the df_strategy to the dict\n",
    "    str_dfs_dict[df_name] = df_strategy\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "for strategy_name, df in str_dfs_dict.items():\n",
    "    #iterate over the rows of df\n",
    "    for i, row in df.iterrows():\n",
    "        # if permid in df_1\n",
    "        if row[\"permid\"] in df_1[\"permid\"].values:\n",
    "            # add the value column strategy_name in df_1 to the df\n",
    "            df.loc[i, f\"{strategy_name}_old\"] = df_1[df_1[\"permid\"] == row[\"permid\"]][strategy_name].values[0]\n",
    "        # if aladdin_id in brs_carteras_issuerlevel\n",
    "        if row[\"aladdin_id\"] in brs_carteras_issuerlevel[\"aladdin_id\"].values:\n",
    "            # add the value column strategy_name in brs_carteras_issuerlevel to the df\n",
    "            df.loc[i, f\"{strategy_name}_brs\"] = brs_carteras_issuerlevel[brs_carteras_issuerlevel[\"aladdin_id\"] == row[\"aladdin_id\"]][strategy_name].values[0]\n",
    "        # if permid in overrides & strategy_name in ovr_target\n",
    "        if (row[\"permid\"] in overrides[\"permid\"].values) and (strategy_name in overrides[\"ovr_target\"].values):\n",
    "            # Add the value column strategy_name in overrides to the df\n",
    "            match = overrides[(overrides[\"permid\"] == row[\"permid\"]) & (overrides[\"ovr_target\"] == strategy_name)]\n",
    "            if not match.empty:\n",
    "                df.loc[i, f\"{strategy_name}_ovr\"] = match[\"ovr_value\"].values[0]\n",
    "\n",
    "# for df in str_dfs_dict.values() move column named \"affected_portfolio_str\" to the end\n",
    "for df in str_dfs_dict.values():\n",
    "    df[\"affected_portfolio_str\"] = df.pop(\"affected_portfolio_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort columns of deltas df\n",
    "delta_brs = delta_brs[id_name_issuers_cols + [col for col in delta_brs.columns if (col not in id_name_issuers_cols) and (col not in delta_test_cols)]]\n",
    "delta_clarity = delta_clarity[id_name_issuers_cols + [col for col in delta_clarity.columns if (col not in id_name_issuers_cols) and (col not in delta_test_cols)]]\n",
    "delta_benchmarks = delta_benchmarks[id_name_issuers_cols + [col for col in delta_benchmarks.columns if (col not in id_name_issuers_cols) and (col not in delta_test_cols)]]\n",
    "new_issuers_clarity = new_issuers_clarity[id_name_issuers_cols + [col for col in new_issuers_clarity.columns if col not in id_name_cols]]\n",
    "out_issuer_clarity = out_issuer_clarity[id_name_issuers_cols + [col for col in out_issuer_clarity.columns if col not in id_name_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# define a function to save results in an Excel file\n",
    "def save_excel(df_dict: dict, output_dir: Path, file_name: str) -> Path:\n",
    "    \"\"\"\n",
    "    Writes multiple DataFrames to an Excel file with each DataFrame in a separate sheet.\n",
    "\n",
    "    Parameters:\n",
    "    - df_dict (dict): A dictionary where keys are sheet names and values are DataFrames.\n",
    "    - output_dir (Path): The directory where the Excel file will be saved.\n",
    "    - file_name (str): The base name for the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    - Path: The full path to the saved Excel file.\n",
    "    \"\"\"\n",
    "    # Create a date string in \"YYYYMMDD\" format\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    logger.info(\"Creating output directory: %s\", output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Construct the full output file path (e.g., file_name_YYYYMMDD.xlsx)\n",
    "    output_file = output_dir / f\"{date_str}_{file_name}.xlsx\"\n",
    "\n",
    "    # Write each DataFrame to its own sheet with index set to False\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        logger.info(\"Writing DataFrames to Excel file: %s\", output_file)\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            logger.info(\"Writing sheet: %s\", sheet_name)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    logger.info(\"Results saved to Excel file: %s\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_brs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE ISSUER HAS ALL THE PORTFOLIOS IN THEIR LIST NOT JUST THE LAST ONE\n",
    "\n",
    "# create dict of df and df name\n",
    "dfs_dict = {\n",
    "    \"zombie_analysis\": zombie_df,\n",
    "    \"delta_carteras\": delta_brs,\n",
    "    \"delta_benchmarks\": delta_benchmarks,\n",
    "    \"delta_clarity\": delta_clarity,\n",
    "    \"new_issuers_clarity\": new_issuers_clarity,\n",
    "    \"out_issuer_clarity\": out_issuer_clarity,\n",
    "}\n",
    "\n",
    "# add to dfs_dict the str_dfs_dict\n",
    "dfs_dict.update(str_dfs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel\n",
    "save_excel(dfs_dict, OUTPUT_DIR, file_name=\"pre_ovr_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
